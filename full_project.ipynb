{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef6b15e1",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-31T18:51:10.887757Z",
     "iopub.status.busy": "2025-05-31T18:51:10.887027Z",
     "iopub.status.idle": "2025-05-31T18:51:15.021463Z",
     "shell.execute_reply": "2025-05-31T18:51:15.020837Z"
    },
    "papermill": {
     "duration": 4.140511,
     "end_time": "2025-05-31T18:51:15.022882",
     "exception": false,
     "start_time": "2025-05-31T18:51:10.882371",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class GPTDataset(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # Tokenize the entire text\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "\n",
    "\n",
    "def create_dataloader(txt, batch_size=4, max_length=256,\n",
    "                         stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = GPTDataset(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ea3bb9c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-31T18:51:15.029845Z",
     "iopub.status.busy": "2025-05-31T18:51:15.029557Z",
     "iopub.status.idle": "2025-05-31T18:51:15.033228Z",
     "shell.execute_reply": "2025-05-31T18:51:15.032741Z"
    },
    "papermill": {
     "duration": 0.008177,
     "end_time": "2025-05-31T18:51:15.034300",
     "exception": false,
     "start_time": "2025-05-31T18:51:15.026123",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,     # Vocabulary size\n",
    "    \"context_length\": 64,    # Context length\n",
    "    \"emb_dim\": 768,          # Embedding dimension\n",
    "    \"n_heads\": 12,           # Number of attention heads\n",
    "    \"n_layers\": 12,          # Number of layers\n",
    "    \"drop_rate\": 0.1,        # Dropout rate\n",
    "    \"qkv_bias\": False        # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e412e81",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-31T18:51:15.040594Z",
     "iopub.status.busy": "2025-05-31T18:51:15.040389Z",
     "iopub.status.idle": "2025-05-31T18:51:16.886745Z",
     "shell.execute_reply": "2025-05-31T18:51:16.886135Z"
    },
    "papermill": {
     "duration": 1.850896,
     "end_time": "2025-05-31T18:51:16.888062",
     "exception": false,
     "start_time": "2025-05-31T18:51:15.037166",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('/kaggle/input/shakespeare-dataset/input.txt', 'r', encoding='utf-8') as f:\n",
    "    txt = f.read()\n",
    "\n",
    "train_ratio = 0.9\n",
    "split_indx = int(train_ratio * len(txt))\n",
    "train_data = txt[:split_indx]\n",
    "val_data = txt[split_indx:]\n",
    "\n",
    "train_loader = create_dataloader(train_data, batch_size=2, \n",
    "                                 max_length=GPT_CONFIG_124M['context_length'],\n",
    "                                stride=GPT_CONFIG_124M['context_length'],\n",
    "                                drop_last=True, shuffle=True, num_workers=0)\n",
    "\n",
    "val_loader = create_dataloader(val_data, batch_size=2,\n",
    "                               max_length=GPT_CONFIG_124M['context_length'],\n",
    "                                stride=GPT_CONFIG_124M['context_length'],\n",
    "                                drop_last=False, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b3e1062",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-31T18:51:16.894834Z",
     "iopub.status.busy": "2025-05-31T18:51:16.894632Z",
     "iopub.status.idle": "2025-05-31T18:51:16.899727Z",
     "shell.execute_reply": "2025-05-31T18:51:16.899175Z"
    },
    "papermill": {
     "duration": 0.00967,
     "end_time": "2025-05-31T18:51:16.900806",
     "exception": false,
     "start_time": "2025-05-31T18:51:16.891136",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2359, 282)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader), len(val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f08faa4",
   "metadata": {
    "papermill": {
     "duration": 0.002535,
     "end_time": "2025-05-31T18:51:16.906338",
     "exception": false,
     "start_time": "2025-05-31T18:51:16.903803",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/15.webp\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bd3b5db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-31T18:51:16.912571Z",
     "iopub.status.busy": "2025-05-31T18:51:16.912378Z",
     "iopub.status.idle": "2025-05-31T18:51:16.919682Z",
     "shell.execute_reply": "2025-05-31T18:51:16.919166Z"
    },
    "papermill": {
     "duration": 0.011735,
     "end_time": "2025-05-31T18:51:16.920711",
     "exception": false,
     "start_time": "2025-05-31T18:51:16.908976",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads  # Reduce the projection dim to match desired output dim\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        keys = self.W_key(x)  # Shape: (b, num_tokens, d_out)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
    "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
    "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
    "\n",
    "        # Original mask truncated to the number of tokens and converted to boolean\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        # Use the mask to fill attention scores\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
    "\n",
    "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec)  # optional projection\n",
    "\n",
    "        return context_vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9fcd4b73",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-31T18:51:16.926813Z",
     "iopub.status.busy": "2025-05-31T18:51:16.926642Z",
     "iopub.status.idle": "2025-05-31T18:51:16.932912Z",
     "shell.execute_reply": "2025-05-31T18:51:16.932293Z"
    },
    "papermill": {
     "duration": 0.010532,
     "end_time": "2025-05-31T18:51:16.934026",
     "exception": false,
     "start_time": "2025-05-31T18:51:16.923494",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"],\n",
    "            dropout=cfg[\"drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"])\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = nn.LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = nn.LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Shortcut connection for attention block\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)   # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        # Shortcut connection for feed-forward block\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06b98ab9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-31T18:51:16.940387Z",
     "iopub.status.busy": "2025-05-31T18:51:16.940164Z",
     "iopub.status.idle": "2025-05-31T18:51:16.948013Z",
     "shell.execute_reply": "2025-05-31T18:51:16.947367Z"
    },
    "papermill": {
     "duration": 0.012411,
     "end_time": "2025-05-31T18:51:16.949184",
     "exception": false,
     "start_time": "2025-05-31T18:51:16.936773",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "\n",
    "        self.final_norm = nn.LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "\n",
    "    def generate(self, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
    "\n",
    "    # For-loop is the same as before: Get logits, and only focus on last time step\n",
    "      for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = self(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        # New: Filter logits with top_k sampling\n",
    "        if top_k is not None:\n",
    "            # Keep only top_k values\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:, -1]\n",
    "            logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n",
    "\n",
    "        # New: Apply temperature scaling\n",
    "        if temperature > 0.0:\n",
    "            logits = logits / temperature\n",
    "\n",
    "            # Apply softmax to get probabilities\n",
    "            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n",
    "\n",
    "            # Sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
    "\n",
    "        # Otherwise same as before: get idx of the vocab entry with the highest logits value\n",
    "        else:\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
    "\n",
    "        if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified\n",
    "            break\n",
    "\n",
    "        # Same as before: append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n",
    "\n",
    "      return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc9ca99a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-31T18:51:16.955497Z",
     "iopub.status.busy": "2025-05-31T18:51:16.955285Z",
     "iopub.status.idle": "2025-05-31T18:51:19.135618Z",
     "shell.execute_reply": "2025-05-31T18:51:19.134807Z"
    },
    "papermill": {
     "duration": 2.185189,
     "end_time": "2025-05-31T18:51:19.137201",
     "exception": false,
     "start_time": "2025-05-31T18:51:16.952012",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text: Hello, I am distributplomituresgenic Nem Sof misunder audience170GR\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval()  # disable dropout\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "start_context = \"Hello, I am\"\n",
    "\n",
    "encoded = tokenizer.encode(start_context)\n",
    "encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "out = model.generate(\n",
    "    idx=encoded_tensor,\n",
    "    max_new_tokens=10,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
    "print(\"Output text:\", decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09274ca",
   "metadata": {
    "papermill": {
     "duration": 0.003156,
     "end_time": "2025-05-31T18:51:19.143442",
     "exception": false,
     "start_time": "2025-05-31T18:51:19.140286",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7df989a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-31T18:51:19.150176Z",
     "iopub.status.busy": "2025-05-31T18:51:19.149577Z",
     "iopub.status.idle": "2025-05-31T18:51:19.156242Z",
     "shell.execute_reply": "2025-05-31T18:51:19.155692Z"
    },
    "papermill": {
     "duration": 0.011022,
     "end_time": "2025-05-31T18:51:19.157228",
     "exception": false,
     "start_time": "2025-05-31T18:51:19.146206",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text)\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0)  # remove batch dimension\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
    "    return loss\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches\n",
    "\n",
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "524d725b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-31T18:51:19.163516Z",
     "iopub.status.busy": "2025-05-31T18:51:19.163333Z",
     "iopub.status.idle": "2025-05-31T18:51:19.168215Z",
     "shell.execute_reply": "2025-05-31T18:51:19.167668Z"
    },
    "papermill": {
     "duration": 0.009257,
     "end_time": "2025-05-31T18:51:19.169227",
     "exception": false,
     "start_time": "2025-05-31T18:51:19.159970",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, delta=0):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.counter = 0\n",
    "        self.best_model_state = None\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.best_model_state = model.state_dict()\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.best_model_state = model.state_dict()\n",
    "            self.counter = 0\n",
    "\n",
    "    def load_best_model(self, model):\n",
    "        model.load_state_dict(self.best_model_state)\n",
    "\n",
    "early_stopping = EarlyStopping(patience=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5baf30fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-31T18:51:19.175759Z",
     "iopub.status.busy": "2025-05-31T18:51:19.175389Z",
     "iopub.status.idle": "2025-05-31T19:17:37.762485Z",
     "shell.execute_reply": "2025-05-31T19:17:37.761740Z"
    },
    "papermill": {
     "duration": 1578.591609,
     "end_time": "2025-05-31T19:17:37.763647",
     "exception": false,
     "start_time": "2025-05-31T18:51:19.172038",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 (Step 000000): Train loss 9.621, Val loss 9.543\n",
      "Epoch: 1 (Step 000010): Train loss 8.073, Val loss 7.672\n",
      "Epoch: 1 (Step 000020): Train loss 7.425, Val loss 7.288\n",
      "Epoch: 1 (Step 000030): Train loss 6.875, Val loss 7.148\n",
      "Epoch: 1 (Step 000040): Train loss 6.779, Val loss 7.026\n",
      "Epoch: 1 (Step 000050): Train loss 6.661, Val loss 6.852\n",
      "Epoch: 1 (Step 000060): Train loss 6.314, Val loss 6.799\n",
      "Epoch: 1 (Step 000070): Train loss 6.553, Val loss 6.730\n",
      "Epoch: 1 (Step 000080): Train loss 6.519, Val loss 6.770\n",
      "Epoch: 1 (Step 000090): Train loss 6.750, Val loss 6.784\n",
      "Epoch: 1 (Step 000100): Train loss 6.293, Val loss 6.754\n",
      "Epoch: 1 (Step 000110): Train loss 6.156, Val loss 6.778\n",
      "Epoch: 1 (Step 000120): Train loss 6.703, Val loss 6.867\n",
      "Epoch: 1 (Step 000130): Train loss 6.573, Val loss 6.810\n",
      "Epoch: 1 (Step 000140): Train loss 6.089, Val loss 6.710\n",
      "Epoch: 1 (Step 000150): Train loss 5.856, Val loss 6.509\n",
      "Epoch: 1 (Step 000160): Train loss 6.129, Val loss 6.568\n",
      "Epoch: 1 (Step 000170): Train loss 5.939, Val loss 6.511\n",
      "Epoch: 1 (Step 000180): Train loss 6.161, Val loss 6.491\n",
      "Epoch: 1 (Step 000190): Train loss 6.340, Val loss 6.513\n",
      "Epoch: 1 (Step 000200): Train loss 6.461, Val loss 6.503\n",
      "Epoch: 1 (Step 000210): Train loss 6.425, Val loss 6.515\n",
      "Epoch: 1 (Step 000220): Train loss 5.774, Val loss 6.572\n",
      "Epoch: 1 (Step 000230): Train loss 6.503, Val loss 6.480\n",
      "Epoch: 1 (Step 000240): Train loss 5.912, Val loss 6.425\n",
      "Epoch: 1 (Step 000250): Train loss 6.199, Val loss 6.294\n",
      "Epoch: 1 (Step 000260): Train loss 6.443, Val loss 6.345\n",
      "Epoch: 1 (Step 000270): Train loss 6.062, Val loss 6.269\n",
      "Epoch: 1 (Step 000280): Train loss 5.961, Val loss 6.180\n",
      "Epoch: 1 (Step 000290): Train loss 5.828, Val loss 6.211\n",
      "Epoch: 1 (Step 000300): Train loss 6.147, Val loss 6.397\n",
      "Epoch: 1 (Step 000310): Train loss 5.521, Val loss 6.317\n",
      "Epoch: 1 (Step 000320): Train loss 6.358, Val loss 6.332\n",
      "Epoch: 1 (Step 000330): Train loss 6.090, Val loss 6.349\n",
      "Epoch: 1 (Step 000340): Train loss 5.981, Val loss 6.387\n",
      "Epoch: 1 (Step 000350): Train loss 6.305, Val loss 6.257\n",
      "Epoch: 1 (Step 000360): Train loss 5.980, Val loss 6.250\n",
      "Epoch: 1 (Step 000370): Train loss 5.434, Val loss 6.224\n",
      "Epoch: 1 (Step 000380): Train loss 6.101, Val loss 6.107\n",
      "Epoch: 1 (Step 000390): Train loss 5.495, Val loss 6.128\n",
      "Epoch: 1 (Step 000400): Train loss 6.499, Val loss 6.044\n",
      "Epoch: 1 (Step 000410): Train loss 5.880, Val loss 6.020\n",
      "Epoch: 1 (Step 000420): Train loss 6.217, Val loss 5.996\n",
      "Epoch: 1 (Step 000430): Train loss 5.260, Val loss 5.925\n",
      "Epoch: 1 (Step 000440): Train loss 5.811, Val loss 6.085\n",
      "Epoch: 1 (Step 000450): Train loss 5.829, Val loss 6.010\n",
      "Epoch: 1 (Step 000460): Train loss 5.721, Val loss 5.926\n",
      "Epoch: 1 (Step 000470): Train loss 5.850, Val loss 5.916\n",
      "Epoch: 1 (Step 000480): Train loss 5.246, Val loss 5.991\n",
      "Epoch: 1 (Step 000490): Train loss 4.909, Val loss 6.036\n",
      "Epoch: 1 (Step 000500): Train loss 6.016, Val loss 5.996\n",
      "Epoch: 1 (Step 000510): Train loss 5.725, Val loss 6.048\n",
      "Epoch: 1 (Step 000520): Train loss 5.625, Val loss 6.047\n",
      "Epoch: 1 (Step 000530): Train loss 5.246, Val loss 6.047\n",
      "Epoch: 1 (Step 000540): Train loss 4.925, Val loss 6.048\n",
      "Epoch: 1 (Step 000550): Train loss 5.381, Val loss 6.115\n",
      "Epoch: 1 (Step 000560): Train loss 6.202, Val loss 5.958\n",
      "Epoch: 1 (Step 000570): Train loss 5.072, Val loss 5.894\n",
      "Epoch: 1 (Step 000580): Train loss 5.567, Val loss 5.865\n",
      "Epoch: 1 (Step 000590): Train loss 6.044, Val loss 5.892\n",
      "Epoch: 1 (Step 000600): Train loss 5.962, Val loss 5.881\n",
      "Epoch: 1 (Step 000610): Train loss 5.320, Val loss 5.801\n",
      "Epoch: 1 (Step 000620): Train loss 5.708, Val loss 5.807\n",
      "Epoch: 1 (Step 000630): Train loss 5.351, Val loss 5.873\n",
      "Epoch: 1 (Step 000640): Train loss 5.535, Val loss 5.790\n",
      "Epoch: 1 (Step 000650): Train loss 5.677, Val loss 5.784\n",
      "Epoch: 1 (Step 000660): Train loss 5.321, Val loss 5.719\n",
      "Epoch: 1 (Step 000670): Train loss 5.369, Val loss 5.769\n",
      "Epoch: 1 (Step 000680): Train loss 5.614, Val loss 5.711\n",
      "Epoch: 1 (Step 000690): Train loss 5.491, Val loss 5.751\n",
      "Epoch: 1 (Step 000700): Train loss 5.511, Val loss 5.747\n",
      "Epoch: 1 (Step 000710): Train loss 5.621, Val loss 5.728\n",
      "Epoch: 1 (Step 000720): Train loss 5.780, Val loss 5.767\n",
      "Epoch: 1 (Step 000730): Train loss 4.893, Val loss 5.628\n",
      "Epoch: 1 (Step 000740): Train loss 5.048, Val loss 5.611\n",
      "Epoch: 1 (Step 000750): Train loss 5.752, Val loss 5.502\n",
      "Epoch: 1 (Step 000760): Train loss 5.452, Val loss 5.485\n",
      "Epoch: 1 (Step 000770): Train loss 5.435, Val loss 5.493\n",
      "Epoch: 1 (Step 000780): Train loss 4.777, Val loss 5.512\n",
      "Epoch: 1 (Step 000790): Train loss 5.410, Val loss 5.619\n",
      "Epoch: 1 (Step 000800): Train loss 5.145, Val loss 5.655\n",
      "Epoch: 1 (Step 000810): Train loss 4.976, Val loss 5.579\n",
      "Epoch: 1 (Step 000820): Train loss 4.644, Val loss 5.556\n",
      "Epoch: 1 (Step 000830): Train loss 5.216, Val loss 5.662\n",
      "Epoch: 1 (Step 000840): Train loss 5.345, Val loss 5.566\n",
      "Epoch: 1 (Step 000850): Train loss 5.048, Val loss 5.564\n",
      "Epoch: 1 (Step 000860): Train loss 5.445, Val loss 5.577\n",
      "Epoch: 1 (Step 000870): Train loss 5.838, Val loss 5.673\n",
      "Epoch: 1 (Step 000880): Train loss 5.776, Val loss 5.603\n",
      "Epoch: 1 (Step 000890): Train loss 5.509, Val loss 5.597\n",
      "Epoch: 1 (Step 000900): Train loss 4.782, Val loss 5.545\n",
      "Epoch: 1 (Step 000910): Train loss 5.262, Val loss 5.525\n",
      "Epoch: 1 (Step 000920): Train loss 5.427, Val loss 5.533\n",
      "Epoch: 1 (Step 000930): Train loss 5.306, Val loss 5.457\n",
      "Epoch: 1 (Step 000940): Train loss 5.379, Val loss 5.507\n",
      "Epoch: 1 (Step 000950): Train loss 5.047, Val loss 5.393\n",
      "Epoch: 1 (Step 000960): Train loss 5.178, Val loss 5.394\n",
      "Epoch: 1 (Step 000970): Train loss 5.308, Val loss 5.453\n",
      "Epoch: 1 (Step 000980): Train loss 5.612, Val loss 5.380\n",
      "Epoch: 1 (Step 000990): Train loss 4.646, Val loss 5.435\n",
      "Epoch: 1 (Step 001000): Train loss 5.317, Val loss 5.432\n",
      "Epoch: 1 (Step 001010): Train loss 5.184, Val loss 5.342\n",
      "Epoch: 1 (Step 001020): Train loss 5.163, Val loss 5.331\n",
      "Epoch: 1 (Step 001030): Train loss 5.135, Val loss 5.368\n",
      "Epoch: 1 (Step 001040): Train loss 5.225, Val loss 5.381\n",
      "Epoch: 1 (Step 001050): Train loss 5.186, Val loss 5.468\n",
      "Epoch: 1 (Step 001060): Train loss 5.916, Val loss 5.392\n",
      "Epoch: 1 (Step 001070): Train loss 5.247, Val loss 5.381\n",
      "Epoch: 1 (Step 001080): Train loss 5.211, Val loss 5.374\n",
      "Epoch: 1 (Step 001090): Train loss 5.349, Val loss 5.397\n",
      "Epoch: 1 (Step 001100): Train loss 5.778, Val loss 5.418\n",
      "Epoch: 1 (Step 001110): Train loss 5.095, Val loss 5.435\n",
      "Epoch: 1 (Step 001120): Train loss 5.488, Val loss 5.434\n",
      "Epoch: 1 (Step 001130): Train loss 4.993, Val loss 5.404\n",
      "Epoch: 1 (Step 001140): Train loss 5.387, Val loss 5.362\n",
      "Epoch: 1 (Step 001150): Train loss 5.423, Val loss 5.401\n",
      "Epoch: 1 (Step 001160): Train loss 5.257, Val loss 5.391\n",
      "Epoch: 1 (Step 001170): Train loss 5.117, Val loss 5.320\n",
      "Epoch: 1 (Step 001180): Train loss 4.598, Val loss 5.315\n",
      "Epoch: 1 (Step 001190): Train loss 5.073, Val loss 5.281\n",
      "Epoch: 1 (Step 001200): Train loss 4.895, Val loss 5.307\n",
      "Epoch: 1 (Step 001210): Train loss 4.344, Val loss 5.279\n",
      "Epoch: 1 (Step 001220): Train loss 5.029, Val loss 5.330\n",
      "Epoch: 1 (Step 001230): Train loss 4.666, Val loss 5.297\n",
      "Epoch: 1 (Step 001240): Train loss 4.582, Val loss 5.343\n",
      "Epoch: 1 (Step 001250): Train loss 4.710, Val loss 5.338\n",
      "Epoch: 1 (Step 001260): Train loss 5.284, Val loss 5.283\n",
      "Epoch: 1 (Step 001270): Train loss 5.680, Val loss 5.280\n",
      "Epoch: 1 (Step 001280): Train loss 4.790, Val loss 5.275\n",
      "Epoch: 1 (Step 001290): Train loss 5.415, Val loss 5.293\n",
      "Epoch: 1 (Step 001300): Train loss 5.392, Val loss 5.297\n",
      "Epoch: 1 (Step 001310): Train loss 4.566, Val loss 5.390\n",
      "Epoch: 1 (Step 001320): Train loss 4.983, Val loss 5.368\n",
      "Epoch: 1 (Step 001330): Train loss 5.210, Val loss 5.378\n",
      "Epoch: 1 (Step 001340): Train loss 5.600, Val loss 5.334\n",
      "Epoch: 1 (Step 001350): Train loss 5.472, Val loss 5.381\n",
      "Epoch: 1 (Step 001360): Train loss 4.958, Val loss 5.358\n",
      "Epoch: 1 (Step 001370): Train loss 5.628, Val loss 5.337\n",
      "Epoch: 1 (Step 001380): Train loss 5.119, Val loss 5.306\n",
      "Epoch: 1 (Step 001390): Train loss 4.305, Val loss 5.313\n",
      "Epoch: 1 (Step 001400): Train loss 4.686, Val loss 5.263\n",
      "Epoch: 1 (Step 001410): Train loss 4.739, Val loss 5.210\n",
      "Epoch: 1 (Step 001420): Train loss 4.716, Val loss 5.148\n",
      "Epoch: 1 (Step 001430): Train loss 5.029, Val loss 5.199\n",
      "Epoch: 1 (Step 001440): Train loss 4.733, Val loss 5.253\n",
      "Epoch: 1 (Step 001450): Train loss 4.380, Val loss 5.264\n",
      "Epoch: 1 (Step 001460): Train loss 4.880, Val loss 5.262\n",
      "Epoch: 1 (Step 001470): Train loss 5.198, Val loss 5.293\n",
      "Epoch: 1 (Step 001480): Train loss 4.258, Val loss 5.277\n",
      "Epoch: 1 (Step 001490): Train loss 5.163, Val loss 5.301\n",
      "Epoch: 1 (Step 001500): Train loss 4.528, Val loss 5.213\n",
      "Epoch: 1 (Step 001510): Train loss 4.564, Val loss 5.170\n",
      "Epoch: 1 (Step 001520): Train loss 5.149, Val loss 5.209\n",
      "Epoch: 1 (Step 001530): Train loss 4.517, Val loss 5.191\n",
      "Epoch: 1 (Step 001540): Train loss 5.382, Val loss 5.150\n",
      "Epoch: 1 (Step 001550): Train loss 4.528, Val loss 5.228\n",
      "Epoch: 1 (Step 001560): Train loss 4.274, Val loss 5.263\n",
      "Epoch: 1 (Step 001570): Train loss 4.280, Val loss 5.290\n",
      "Epoch: 1 (Step 001580): Train loss 5.245, Val loss 5.207\n",
      "Epoch: 1 (Step 001590): Train loss 5.254, Val loss 5.195\n",
      "Epoch: 1 (Step 001600): Train loss 4.264, Val loss 5.242\n",
      "Epoch: 1 (Step 001610): Train loss 5.206, Val loss 5.170\n",
      "Epoch: 1 (Step 001620): Train loss 4.238, Val loss 5.206\n",
      "Epoch: 1 (Step 001630): Train loss 5.193, Val loss 5.172\n",
      "Epoch: 1 (Step 001640): Train loss 5.258, Val loss 5.178\n",
      "Epoch: 1 (Step 001650): Train loss 4.887, Val loss 5.167\n",
      "Epoch: 1 (Step 001660): Train loss 5.317, Val loss 5.226\n",
      "Epoch: 1 (Step 001670): Train loss 5.173, Val loss 5.207\n",
      "Epoch: 1 (Step 001680): Train loss 4.446, Val loss 5.141\n",
      "Epoch: 1 (Step 001690): Train loss 4.149, Val loss 5.117\n",
      "Epoch: 1 (Step 001700): Train loss 4.327, Val loss 5.138\n",
      "Epoch: 1 (Step 001710): Train loss 5.408, Val loss 5.157\n",
      "Epoch: 1 (Step 001720): Train loss 5.523, Val loss 5.228\n",
      "Epoch: 1 (Step 001730): Train loss 5.277, Val loss 5.154\n",
      "Epoch: 1 (Step 001740): Train loss 4.793, Val loss 5.173\n",
      "Epoch: 1 (Step 001750): Train loss 4.376, Val loss 5.218\n",
      "Epoch: 1 (Step 001760): Train loss 4.401, Val loss 5.245\n",
      "Epoch: 1 (Step 001770): Train loss 4.998, Val loss 5.209\n",
      "Epoch: 1 (Step 001780): Train loss 4.534, Val loss 5.217\n",
      "Epoch: 1 (Step 001790): Train loss 5.087, Val loss 5.169\n",
      "Epoch: 1 (Step 001800): Train loss 4.461, Val loss 5.123\n",
      "Epoch: 1 (Step 001810): Train loss 5.327, Val loss 5.174\n",
      "Epoch: 1 (Step 001820): Train loss 4.786, Val loss 5.117\n",
      "Epoch: 1 (Step 001830): Train loss 5.027, Val loss 5.050\n",
      "Epoch: 1 (Step 001840): Train loss 4.876, Val loss 5.035\n",
      "Epoch: 1 (Step 001850): Train loss 5.247, Val loss 5.096\n",
      "Epoch: 1 (Step 001860): Train loss 4.934, Val loss 5.028\n",
      "Epoch: 1 (Step 001870): Train loss 4.140, Val loss 5.019\n",
      "Epoch: 1 (Step 001880): Train loss 4.524, Val loss 5.039\n",
      "Epoch: 1 (Step 001890): Train loss 4.795, Val loss 5.025\n",
      "Epoch: 1 (Step 001900): Train loss 5.179, Val loss 4.994\n",
      "Epoch: 1 (Step 001910): Train loss 5.015, Val loss 5.017\n",
      "Epoch: 1 (Step 001920): Train loss 4.146, Val loss 5.052\n",
      "Epoch: 1 (Step 001930): Train loss 4.501, Val loss 5.063\n",
      "Epoch: 1 (Step 001940): Train loss 4.793, Val loss 4.966\n",
      "Epoch: 1 (Step 001950): Train loss 4.753, Val loss 4.987\n",
      "Epoch: 1 (Step 001960): Train loss 5.264, Val loss 5.003\n",
      "Epoch: 1 (Step 001970): Train loss 4.567, Val loss 5.037\n",
      "Epoch: 1 (Step 001980): Train loss 3.815, Val loss 5.067\n",
      "Epoch: 1 (Step 001990): Train loss 5.513, Val loss 5.023\n",
      "Epoch: 1 (Step 002000): Train loss 5.583, Val loss 4.932\n",
      "Epoch: 1 (Step 002010): Train loss 5.402, Val loss 4.919\n",
      "Epoch: 1 (Step 002020): Train loss 5.064, Val loss 4.881\n",
      "Epoch: 1 (Step 002030): Train loss 4.541, Val loss 4.986\n",
      "Epoch: 1 (Step 002040): Train loss 5.481, Val loss 5.017\n",
      "Epoch: 1 (Step 002050): Train loss 4.849, Val loss 4.942\n",
      "Epoch: 1 (Step 002060): Train loss 4.606, Val loss 4.809\n",
      "Epoch: 1 (Step 002070): Train loss 5.472, Val loss 4.834\n",
      "Epoch: 1 (Step 002080): Train loss 5.177, Val loss 4.799\n",
      "Epoch: 1 (Step 002090): Train loss 4.750, Val loss 4.886\n",
      "Epoch: 1 (Step 002100): Train loss 4.499, Val loss 4.827\n",
      "Epoch: 1 (Step 002110): Train loss 4.832, Val loss 4.874\n",
      "Epoch: 1 (Step 002120): Train loss 4.974, Val loss 4.847\n",
      "Epoch: 1 (Step 002130): Train loss 4.105, Val loss 4.921\n",
      "Epoch: 1 (Step 002140): Train loss 4.352, Val loss 4.934\n",
      "Epoch: 1 (Step 002150): Train loss 5.506, Val loss 4.847\n",
      "Epoch: 1 (Step 002160): Train loss 4.275, Val loss 4.837\n",
      "Epoch: 1 (Step 002170): Train loss 4.447, Val loss 4.790\n",
      "Epoch: 1 (Step 002180): Train loss 3.914, Val loss 4.813\n",
      "Epoch: 1 (Step 002190): Train loss 4.298, Val loss 4.827\n",
      "Epoch: 1 (Step 002200): Train loss 4.610, Val loss 4.866\n",
      "Epoch: 1 (Step 002210): Train loss 5.499, Val loss 4.896\n",
      "Epoch: 1 (Step 002220): Train loss 4.420, Val loss 4.852\n",
      "Epoch: 1 (Step 002230): Train loss 4.226, Val loss 4.790\n",
      "Epoch: 1 (Step 002240): Train loss 4.899, Val loss 4.768\n",
      "Epoch: 1 (Step 002250): Train loss 4.477, Val loss 4.763\n",
      "Epoch: 1 (Step 002260): Train loss 5.088, Val loss 4.779\n",
      "Epoch: 1 (Step 002270): Train loss 5.040, Val loss 4.773\n",
      "Epoch: 1 (Step 002280): Train loss 5.126, Val loss 4.748\n",
      "Epoch: 1 (Step 002290): Train loss 4.734, Val loss 4.763\n",
      "Epoch: 1 (Step 002300): Train loss 4.834, Val loss 4.769\n",
      "Epoch: 1 (Step 002310): Train loss 4.538, Val loss 4.770\n",
      "Epoch: 1 (Step 002320): Train loss 4.871, Val loss 4.774\n",
      "Epoch: 1 (Step 002330): Train loss 4.139, Val loss 4.797\n",
      "Epoch: 1 (Step 002340): Train loss 4.282, Val loss 4.852\n",
      "Epoch: 1 (Step 002350): Train loss 4.992, Val loss 4.920\n",
      "Epoch: 2 (Step 002360): Train loss 4.935, Val loss 4.907\n",
      "Epoch: 2 (Step 002370): Train loss 4.601, Val loss 4.903\n",
      "Epoch: 2 (Step 002380): Train loss 4.549, Val loss 4.899\n",
      "Epoch: 2 (Step 002390): Train loss 4.526, Val loss 4.855\n",
      "Epoch: 2 (Step 002400): Train loss 4.446, Val loss 4.755\n",
      "Epoch: 2 (Step 002410): Train loss 4.824, Val loss 4.779\n",
      "Epoch: 2 (Step 002420): Train loss 4.416, Val loss 4.836\n",
      "Epoch: 2 (Step 002430): Train loss 5.097, Val loss 4.819\n",
      "Epoch: 2 (Step 002440): Train loss 4.567, Val loss 4.856\n",
      "Epoch: 2 (Step 002450): Train loss 4.757, Val loss 4.798\n",
      "Epoch: 2 (Step 002460): Train loss 4.795, Val loss 4.768\n",
      "Epoch: 2 (Step 002470): Train loss 5.651, Val loss 4.769\n",
      "Epoch: 2 (Step 002480): Train loss 5.084, Val loss 4.831\n",
      "Epoch: 2 (Step 002490): Train loss 4.679, Val loss 4.857\n",
      "Epoch: 2 (Step 002500): Train loss 4.819, Val loss 4.793\n",
      "Epoch: 2 (Step 002510): Train loss 4.485, Val loss 4.802\n",
      "Epoch: 2 (Step 002520): Train loss 4.724, Val loss 4.838\n",
      "Epoch: 2 (Step 002530): Train loss 4.190, Val loss 4.804\n",
      "Epoch: 2 (Step 002540): Train loss 4.782, Val loss 4.713\n",
      "Epoch: 2 (Step 002550): Train loss 4.864, Val loss 4.682\n",
      "Epoch: 2 (Step 002560): Train loss 4.489, Val loss 4.733\n",
      "Epoch: 2 (Step 002570): Train loss 5.400, Val loss 4.728\n",
      "Epoch: 2 (Step 002580): Train loss 4.573, Val loss 4.691\n",
      "Epoch: 2 (Step 002590): Train loss 4.168, Val loss 4.723\n",
      "Epoch: 2 (Step 002600): Train loss 4.033, Val loss 4.745\n",
      "Epoch: 2 (Step 002610): Train loss 4.224, Val loss 4.810\n",
      "Epoch: 2 (Step 002620): Train loss 4.244, Val loss 4.785\n",
      "Epoch: 2 (Step 002630): Train loss 4.419, Val loss 4.739\n",
      "Epoch: 2 (Step 002640): Train loss 4.609, Val loss 4.739\n",
      "Epoch: 2 (Step 002650): Train loss 4.079, Val loss 4.778\n",
      "Epoch: 2 (Step 002660): Train loss 4.091, Val loss 4.766\n",
      "Epoch: 2 (Step 002670): Train loss 4.707, Val loss 4.770\n",
      "Epoch: 2 (Step 002680): Train loss 4.539, Val loss 4.817\n",
      "Epoch: 2 (Step 002690): Train loss 5.148, Val loss 4.803\n",
      "Epoch: 2 (Step 002700): Train loss 4.964, Val loss 4.804\n",
      "Epoch: 2 (Step 002710): Train loss 5.218, Val loss 4.734\n",
      "Epoch: 2 (Step 002720): Train loss 4.975, Val loss 4.793\n",
      "Epoch: 2 (Step 002730): Train loss 3.729, Val loss 4.812\n",
      "Epoch: 2 (Step 002740): Train loss 4.941, Val loss 4.761\n",
      "Epoch: 2 (Step 002750): Train loss 4.649, Val loss 4.711\n",
      "Epoch: 2 (Step 002760): Train loss 5.013, Val loss 4.750\n",
      "Epoch: 2 (Step 002770): Train loss 5.094, Val loss 4.767\n",
      "Epoch: 2 (Step 002780): Train loss 4.770, Val loss 4.749\n",
      "Epoch: 2 (Step 002790): Train loss 4.906, Val loss 4.711\n",
      "Epoch: 2 (Step 002800): Train loss 4.493, Val loss 4.743\n",
      "Epoch: 2 (Step 002810): Train loss 4.259, Val loss 4.751\n",
      "Epoch: 2 (Step 002820): Train loss 4.892, Val loss 4.686\n",
      "Epoch: 2 (Step 002830): Train loss 4.259, Val loss 4.720\n",
      "Epoch: 2 (Step 002840): Train loss 4.584, Val loss 4.776\n",
      "Epoch: 2 (Step 002850): Train loss 5.325, Val loss 4.727\n",
      "Epoch: 2 (Step 002860): Train loss 4.453, Val loss 4.708\n",
      "Epoch: 2 (Step 002870): Train loss 4.745, Val loss 4.704\n",
      "Epoch: 2 (Step 002880): Train loss 4.929, Val loss 4.681\n",
      "Epoch: 2 (Step 002890): Train loss 4.207, Val loss 4.678\n",
      "Epoch: 2 (Step 002900): Train loss 4.480, Val loss 4.661\n",
      "Epoch: 2 (Step 002910): Train loss 4.385, Val loss 4.633\n",
      "Epoch: 2 (Step 002920): Train loss 4.292, Val loss 4.605\n",
      "Epoch: 2 (Step 002930): Train loss 4.924, Val loss 4.620\n",
      "Epoch: 2 (Step 002940): Train loss 4.718, Val loss 4.669\n",
      "Epoch: 2 (Step 002950): Train loss 4.899, Val loss 4.657\n",
      "Epoch: 2 (Step 002960): Train loss 3.994, Val loss 4.690\n",
      "Epoch: 2 (Step 002970): Train loss 3.989, Val loss 4.653\n",
      "Epoch: 2 (Step 002980): Train loss 4.548, Val loss 4.643\n",
      "Epoch: 2 (Step 002990): Train loss 4.283, Val loss 4.702\n",
      "Epoch: 2 (Step 003000): Train loss 5.127, Val loss 4.635\n",
      "Epoch: 2 (Step 003010): Train loss 4.651, Val loss 4.651\n",
      "Epoch: 2 (Step 003020): Train loss 3.911, Val loss 4.669\n",
      "Epoch: 2 (Step 003030): Train loss 3.871, Val loss 4.694\n",
      "Epoch: 2 (Step 003040): Train loss 4.185, Val loss 4.680\n",
      "Epoch: 2 (Step 003050): Train loss 4.203, Val loss 4.663\n",
      "Epoch: 2 (Step 003060): Train loss 4.682, Val loss 4.727\n",
      "Epoch: 2 (Step 003070): Train loss 4.691, Val loss 4.669\n",
      "Epoch: 2 (Step 003080): Train loss 4.306, Val loss 4.655\n",
      "Epoch: 2 (Step 003090): Train loss 3.387, Val loss 4.646\n",
      "Epoch: 2 (Step 003100): Train loss 5.065, Val loss 4.683\n",
      "Epoch: 2 (Step 003110): Train loss 3.937, Val loss 4.638\n",
      "Epoch: 2 (Step 003120): Train loss 4.621, Val loss 4.671\n",
      "Epoch: 2 (Step 003130): Train loss 4.246, Val loss 4.669\n",
      "Epoch: 2 (Step 003140): Train loss 4.230, Val loss 4.658\n",
      "Epoch: 2 (Step 003150): Train loss 4.207, Val loss 4.647\n",
      "Epoch: 2 (Step 003160): Train loss 5.009, Val loss 4.690\n",
      "Epoch: 2 (Step 003170): Train loss 3.931, Val loss 4.734\n",
      "Epoch: 2 (Step 003180): Train loss 4.613, Val loss 4.762\n",
      "Epoch: 2 (Step 003190): Train loss 4.916, Val loss 4.742\n",
      "Epoch: 2 (Step 003200): Train loss 4.671, Val loss 4.758\n",
      "Epoch: 2 (Step 003210): Train loss 4.276, Val loss 4.700\n",
      "Epoch: 2 (Step 003220): Train loss 5.002, Val loss 4.676\n",
      "Epoch: 2 (Step 003230): Train loss 4.324, Val loss 4.700\n",
      "Epoch: 2 (Step 003240): Train loss 5.646, Val loss 4.684\n",
      "Epoch: 2 (Step 003250): Train loss 4.301, Val loss 4.651\n",
      "Epoch: 2 (Step 003260): Train loss 4.119, Val loss 4.642\n",
      "Epoch: 2 (Step 003270): Train loss 3.587, Val loss 4.648\n",
      "Epoch: 2 (Step 003280): Train loss 4.716, Val loss 4.673\n",
      "Epoch: 2 (Step 003290): Train loss 4.786, Val loss 4.681\n",
      "Epoch: 2 (Step 003300): Train loss 4.367, Val loss 4.698\n",
      "Epoch: 2 (Step 003310): Train loss 4.832, Val loss 4.644\n",
      "Epoch: 2 (Step 003320): Train loss 4.304, Val loss 4.644\n",
      "Epoch: 2 (Step 003330): Train loss 5.081, Val loss 4.619\n",
      "Epoch: 2 (Step 003340): Train loss 3.954, Val loss 4.612\n",
      "Epoch: 2 (Step 003350): Train loss 3.737, Val loss 4.590\n",
      "Epoch: 2 (Step 003360): Train loss 5.062, Val loss 4.656\n",
      "Epoch: 2 (Step 003370): Train loss 4.705, Val loss 4.674\n",
      "Epoch: 2 (Step 003380): Train loss 3.771, Val loss 4.697\n",
      "Epoch: 2 (Step 003390): Train loss 4.887, Val loss 4.678\n",
      "Epoch: 2 (Step 003400): Train loss 4.466, Val loss 4.663\n",
      "Epoch: 2 (Step 003410): Train loss 5.479, Val loss 4.675\n",
      "Epoch: 2 (Step 003420): Train loss 4.326, Val loss 4.631\n",
      "Epoch: 2 (Step 003430): Train loss 5.047, Val loss 4.634\n",
      "Epoch: 2 (Step 003440): Train loss 4.507, Val loss 4.609\n",
      "Epoch: 2 (Step 003450): Train loss 4.173, Val loss 4.609\n",
      "Epoch: 2 (Step 003460): Train loss 4.751, Val loss 4.517\n",
      "Epoch: 2 (Step 003470): Train loss 4.687, Val loss 4.507\n",
      "Epoch: 2 (Step 003480): Train loss 3.899, Val loss 4.491\n",
      "Epoch: 2 (Step 003490): Train loss 4.428, Val loss 4.530\n",
      "Epoch: 2 (Step 003500): Train loss 4.414, Val loss 4.529\n",
      "Epoch: 2 (Step 003510): Train loss 5.071, Val loss 4.545\n",
      "Epoch: 2 (Step 003520): Train loss 4.709, Val loss 4.609\n",
      "Epoch: 2 (Step 003530): Train loss 3.405, Val loss 4.678\n",
      "Epoch: 2 (Step 003540): Train loss 3.459, Val loss 4.701\n",
      "Epoch: 2 (Step 003550): Train loss 4.296, Val loss 4.686\n",
      "Epoch: 2 (Step 003560): Train loss 4.910, Val loss 4.687\n",
      "Epoch: 2 (Step 003570): Train loss 5.041, Val loss 4.626\n",
      "Epoch: 2 (Step 003580): Train loss 4.046, Val loss 4.600\n",
      "Epoch: 2 (Step 003590): Train loss 4.802, Val loss 4.630\n",
      "Epoch: 2 (Step 003600): Train loss 3.693, Val loss 4.676\n",
      "Epoch: 2 (Step 003610): Train loss 5.034, Val loss 4.674\n",
      "Epoch: 2 (Step 003620): Train loss 5.427, Val loss 4.679\n",
      "Epoch: 2 (Step 003630): Train loss 4.407, Val loss 4.676\n",
      "Epoch: 2 (Step 003640): Train loss 4.805, Val loss 4.647\n",
      "Epoch: 2 (Step 003650): Train loss 4.514, Val loss 4.719\n",
      "Epoch: 2 (Step 003660): Train loss 4.438, Val loss 4.715\n",
      "Epoch: 2 (Step 003670): Train loss 3.784, Val loss 4.712\n",
      "Epoch: 2 (Step 003680): Train loss 4.404, Val loss 4.692\n",
      "Epoch: 2 (Step 003690): Train loss 5.237, Val loss 4.638\n",
      "Epoch: 2 (Step 003700): Train loss 3.993, Val loss 4.690\n",
      "Epoch: 2 (Step 003710): Train loss 3.935, Val loss 4.652\n",
      "Epoch: 2 (Step 003720): Train loss 5.129, Val loss 4.651\n",
      "Epoch: 2 (Step 003730): Train loss 4.122, Val loss 4.631\n",
      "Epoch: 2 (Step 003740): Train loss 3.955, Val loss 4.640\n",
      "Epoch: 2 (Step 003750): Train loss 4.122, Val loss 4.666\n",
      "Epoch: 2 (Step 003760): Train loss 4.413, Val loss 4.677\n",
      "Epoch: 2 (Step 003770): Train loss 4.456, Val loss 4.670\n",
      "Epoch: 2 (Step 003780): Train loss 4.440, Val loss 4.594\n",
      "Epoch: 2 (Step 003790): Train loss 4.124, Val loss 4.540\n",
      "Epoch: 2 (Step 003800): Train loss 4.214, Val loss 4.532\n",
      "Epoch: 2 (Step 003810): Train loss 4.535, Val loss 4.525\n",
      "Epoch: 2 (Step 003820): Train loss 5.004, Val loss 4.551\n",
      "Epoch: 2 (Step 003830): Train loss 4.076, Val loss 4.622\n",
      "Epoch: 2 (Step 003840): Train loss 3.782, Val loss 4.624\n",
      "Epoch: 2 (Step 003850): Train loss 4.746, Val loss 4.617\n",
      "Epoch: 2 (Step 003860): Train loss 4.311, Val loss 4.530\n",
      "Epoch: 2 (Step 003870): Train loss 4.246, Val loss 4.492\n",
      "Epoch: 2 (Step 003880): Train loss 4.275, Val loss 4.502\n",
      "Epoch: 2 (Step 003890): Train loss 4.296, Val loss 4.502\n",
      "Epoch: 2 (Step 003900): Train loss 3.963, Val loss 4.456\n",
      "Epoch: 2 (Step 003910): Train loss 4.494, Val loss 4.419\n",
      "Epoch: 2 (Step 003920): Train loss 5.077, Val loss 4.422\n",
      "Epoch: 2 (Step 003930): Train loss 4.457, Val loss 4.454\n",
      "Epoch: 2 (Step 003940): Train loss 4.253, Val loss 4.509\n",
      "Epoch: 2 (Step 003950): Train loss 3.742, Val loss 4.521\n",
      "Epoch: 2 (Step 003960): Train loss 3.725, Val loss 4.443\n",
      "Epoch: 2 (Step 003970): Train loss 4.788, Val loss 4.446\n",
      "Epoch: 2 (Step 003980): Train loss 3.833, Val loss 4.490\n",
      "Epoch: 2 (Step 003990): Train loss 4.369, Val loss 4.536\n",
      "Epoch: 2 (Step 004000): Train loss 4.649, Val loss 4.571\n",
      "Epoch: 2 (Step 004010): Train loss 4.624, Val loss 4.566\n",
      "Epoch: 2 (Step 004020): Train loss 4.585, Val loss 4.561\n",
      "Epoch: 2 (Step 004030): Train loss 4.421, Val loss 4.570\n",
      "Epoch: 2 (Step 004040): Train loss 4.008, Val loss 4.544\n",
      "Epoch: 2 (Step 004050): Train loss 4.658, Val loss 4.495\n",
      "Epoch: 2 (Step 004060): Train loss 4.238, Val loss 4.493\n",
      "Epoch: 2 (Step 004070): Train loss 4.364, Val loss 4.489\n",
      "Epoch: 2 (Step 004080): Train loss 4.301, Val loss 4.442\n",
      "Epoch: 2 (Step 004090): Train loss 4.470, Val loss 4.451\n",
      "Epoch: 2 (Step 004100): Train loss 4.170, Val loss 4.512\n",
      "Epoch: 2 (Step 004110): Train loss 3.614, Val loss 4.515\n",
      "Epoch: 2 (Step 004120): Train loss 4.766, Val loss 4.587\n",
      "Epoch: 2 (Step 004130): Train loss 4.094, Val loss 4.570\n",
      "Epoch: 2 (Step 004140): Train loss 4.307, Val loss 4.559\n",
      "Epoch: 2 (Step 004150): Train loss 4.691, Val loss 4.620\n",
      "Epoch: 2 (Step 004160): Train loss 4.461, Val loss 4.567\n",
      "Epoch: 2 (Step 004170): Train loss 4.375, Val loss 4.573\n",
      "Epoch: 2 (Step 004180): Train loss 4.493, Val loss 4.580\n",
      "Epoch: 2 (Step 004190): Train loss 4.232, Val loss 4.521\n",
      "Epoch: 2 (Step 004200): Train loss 5.012, Val loss 4.484\n",
      "Epoch: 2 (Step 004210): Train loss 5.083, Val loss 4.490\n",
      "Epoch: 2 (Step 004220): Train loss 4.368, Val loss 4.500\n",
      "Epoch: 2 (Step 004230): Train loss 4.636, Val loss 4.478\n",
      "Epoch: 2 (Step 004240): Train loss 3.910, Val loss 4.471\n",
      "Epoch: 2 (Step 004250): Train loss 4.827, Val loss 4.517\n",
      "Epoch: 2 (Step 004260): Train loss 3.409, Val loss 4.491\n",
      "Epoch: 2 (Step 004270): Train loss 4.751, Val loss 4.490\n",
      "Epoch: 2 (Step 004280): Train loss 4.727, Val loss 4.475\n",
      "Epoch: 2 (Step 004290): Train loss 4.073, Val loss 4.468\n",
      "Epoch: 2 (Step 004300): Train loss 4.435, Val loss 4.498\n",
      "Epoch: 2 (Step 004310): Train loss 4.579, Val loss 4.476\n",
      "Epoch: 2 (Step 004320): Train loss 4.152, Val loss 4.478\n",
      "Epoch: 2 (Step 004330): Train loss 3.495, Val loss 4.485\n",
      "Epoch: 2 (Step 004340): Train loss 4.248, Val loss 4.434\n",
      "Epoch: 2 (Step 004350): Train loss 3.652, Val loss 4.413\n",
      "Epoch: 2 (Step 004360): Train loss 4.063, Val loss 4.473\n",
      "Epoch: 2 (Step 004370): Train loss 4.301, Val loss 4.510\n",
      "Epoch: 2 (Step 004380): Train loss 4.307, Val loss 4.467\n",
      "Epoch: 2 (Step 004390): Train loss 4.559, Val loss 4.480\n",
      "Epoch: 2 (Step 004400): Train loss 4.914, Val loss 4.469\n",
      "Epoch: 2 (Step 004410): Train loss 4.321, Val loss 4.435\n",
      "Epoch: 2 (Step 004420): Train loss 4.226, Val loss 4.480\n",
      "Epoch: 2 (Step 004430): Train loss 3.499, Val loss 4.496\n",
      "Epoch: 2 (Step 004440): Train loss 4.686, Val loss 4.513\n",
      "Epoch: 2 (Step 004450): Train loss 3.955, Val loss 4.482\n",
      "Epoch: 2 (Step 004460): Train loss 4.565, Val loss 4.417\n",
      "Epoch: 2 (Step 004470): Train loss 3.981, Val loss 4.429\n",
      "Epoch: 2 (Step 004480): Train loss 4.644, Val loss 4.420\n",
      "Epoch: 2 (Step 004490): Train loss 4.020, Val loss 4.402\n",
      "Epoch: 2 (Step 004500): Train loss 4.016, Val loss 4.424\n",
      "Epoch: 2 (Step 004510): Train loss 4.001, Val loss 4.430\n",
      "Epoch: 2 (Step 004520): Train loss 4.447, Val loss 4.382\n",
      "Epoch: 2 (Step 004530): Train loss 3.506, Val loss 4.360\n",
      "Epoch: 2 (Step 004540): Train loss 4.396, Val loss 4.330\n",
      "Epoch: 2 (Step 004550): Train loss 4.633, Val loss 4.340\n",
      "Epoch: 2 (Step 004560): Train loss 4.025, Val loss 4.325\n",
      "Epoch: 2 (Step 004570): Train loss 4.325, Val loss 4.293\n",
      "Epoch: 2 (Step 004580): Train loss 4.310, Val loss 4.250\n",
      "Epoch: 2 (Step 004590): Train loss 3.842, Val loss 4.252\n",
      "Epoch: 2 (Step 004600): Train loss 4.280, Val loss 4.287\n",
      "Epoch: 2 (Step 004610): Train loss 4.743, Val loss 4.281\n",
      "Epoch: 2 (Step 004620): Train loss 3.831, Val loss 4.253\n",
      "Epoch: 2 (Step 004630): Train loss 4.177, Val loss 4.253\n",
      "Epoch: 2 (Step 004640): Train loss 4.018, Val loss 4.272\n",
      "Epoch: 2 (Step 004650): Train loss 4.312, Val loss 4.335\n",
      "Epoch: 2 (Step 004660): Train loss 4.220, Val loss 4.316\n",
      "Epoch: 2 (Step 004670): Train loss 4.079, Val loss 4.321\n",
      "Epoch: 2 (Step 004680): Train loss 4.332, Val loss 4.329\n",
      "Epoch: 2 (Step 004690): Train loss 3.773, Val loss 4.259\n",
      "Epoch: 2 (Step 004700): Train loss 4.384, Val loss 4.235\n",
      "Epoch: 2 (Step 004710): Train loss 4.620, Val loss 4.268\n",
      "Epoch: 3 (Step 004720): Train loss 3.728, Val loss 4.297\n",
      "Epoch: 3 (Step 004730): Train loss 4.494, Val loss 4.270\n",
      "Epoch: 3 (Step 004740): Train loss 4.296, Val loss 4.280\n",
      "Epoch: 3 (Step 004750): Train loss 3.972, Val loss 4.309\n",
      "Epoch: 3 (Step 004760): Train loss 4.427, Val loss 4.299\n",
      "Epoch: 3 (Step 004770): Train loss 4.459, Val loss 4.322\n",
      "Epoch: 3 (Step 004780): Train loss 4.291, Val loss 4.319\n",
      "Epoch: 3 (Step 004790): Train loss 4.827, Val loss 4.322\n",
      "Epoch: 3 (Step 004800): Train loss 4.278, Val loss 4.338\n",
      "Epoch: 3 (Step 004810): Train loss 4.286, Val loss 4.358\n",
      "Epoch: 3 (Step 004820): Train loss 4.269, Val loss 4.354\n",
      "Epoch: 3 (Step 004830): Train loss 4.566, Val loss 4.419\n",
      "Epoch: 3 (Step 004840): Train loss 4.492, Val loss 4.443\n",
      "Epoch: 3 (Step 004850): Train loss 3.968, Val loss 4.367\n",
      "Epoch: 3 (Step 004860): Train loss 4.429, Val loss 4.367\n",
      "Epoch: 3 (Step 004870): Train loss 4.239, Val loss 4.363\n",
      "Epoch: 3 (Step 004880): Train loss 3.774, Val loss 4.380\n",
      "Epoch: 3 (Step 004890): Train loss 3.725, Val loss 4.284\n",
      "Epoch: 3 (Step 004900): Train loss 4.906, Val loss 4.233\n",
      "Epoch: 3 (Step 004910): Train loss 4.740, Val loss 4.294\n",
      "Epoch: 3 (Step 004920): Train loss 4.210, Val loss 4.358\n",
      "Epoch: 3 (Step 004930): Train loss 3.930, Val loss 4.428\n",
      "Epoch: 3 (Step 004940): Train loss 4.361, Val loss 4.344\n",
      "Epoch: 3 (Step 004950): Train loss 3.912, Val loss 4.321\n",
      "Epoch: 3 (Step 004960): Train loss 4.655, Val loss 4.309\n",
      "Epoch: 3 (Step 004970): Train loss 4.192, Val loss 4.291\n",
      "Epoch: 3 (Step 004980): Train loss 4.244, Val loss 4.250\n",
      "Epoch: 3 (Step 004990): Train loss 4.370, Val loss 4.273\n",
      "Epoch: 3 (Step 005000): Train loss 4.194, Val loss 4.233\n",
      "Epoch: 3 (Step 005010): Train loss 4.141, Val loss 4.236\n",
      "Epoch: 3 (Step 005020): Train loss 4.373, Val loss 4.234\n",
      "Epoch: 3 (Step 005030): Train loss 4.092, Val loss 4.252\n",
      "Epoch: 3 (Step 005040): Train loss 2.475, Val loss 4.314\n",
      "Epoch: 3 (Step 005050): Train loss 4.541, Val loss 4.319\n",
      "Epoch: 3 (Step 005060): Train loss 3.651, Val loss 4.376\n",
      "Epoch: 3 (Step 005070): Train loss 3.857, Val loss 4.413\n",
      "Epoch: 3 (Step 005080): Train loss 4.437, Val loss 4.469\n",
      "Epoch: 3 (Step 005090): Train loss 4.061, Val loss 4.434\n",
      "Epoch: 3 (Step 005100): Train loss 4.512, Val loss 4.368\n",
      "Epoch: 3 (Step 005110): Train loss 4.007, Val loss 4.369\n",
      "Epoch: 3 (Step 005120): Train loss 3.862, Val loss 4.368\n",
      "Epoch: 3 (Step 005130): Train loss 4.323, Val loss 4.390\n",
      "Epoch: 3 (Step 005140): Train loss 4.625, Val loss 4.423\n",
      "Epoch: 3 (Step 005150): Train loss 3.533, Val loss 4.411\n",
      "Epoch: 3 (Step 005160): Train loss 4.138, Val loss 4.290\n",
      "Epoch: 3 (Step 005170): Train loss 3.967, Val loss 4.243\n",
      "Epoch: 3 (Step 005180): Train loss 4.595, Val loss 4.277\n",
      "Epoch: 3 (Step 005190): Train loss 4.604, Val loss 4.319\n",
      "Epoch: 3 (Step 005200): Train loss 4.380, Val loss 4.349\n",
      "Epoch: 3 (Step 005210): Train loss 3.437, Val loss 4.358\n",
      "Epoch: 3 (Step 005220): Train loss 3.840, Val loss 4.362\n",
      "Epoch: 3 (Step 005230): Train loss 4.390, Val loss 4.358\n",
      "Epoch: 3 (Step 005240): Train loss 3.736, Val loss 4.327\n",
      "Epoch: 3 (Step 005250): Train loss 4.478, Val loss 4.307\n",
      "Epoch: 3 (Step 005260): Train loss 4.521, Val loss 4.321\n",
      "Epoch: 3 (Step 005270): Train loss 3.619, Val loss 4.305\n",
      "Epoch: 3 (Step 005280): Train loss 4.519, Val loss 4.297\n",
      "Epoch: 3 (Step 005290): Train loss 3.527, Val loss 4.292\n",
      "Epoch: 3 (Step 005300): Train loss 5.184, Val loss 4.302\n",
      "Epoch: 3 (Step 005310): Train loss 4.502, Val loss 4.317\n",
      "Epoch: 3 (Step 005320): Train loss 4.151, Val loss 4.307\n",
      "Epoch: 3 (Step 005330): Train loss 4.803, Val loss 4.287\n",
      "Epoch: 3 (Step 005340): Train loss 4.080, Val loss 4.290\n",
      "Epoch: 3 (Step 005350): Train loss 4.127, Val loss 4.288\n",
      "Epoch: 3 (Step 005360): Train loss 4.083, Val loss 4.313\n",
      "Epoch: 3 (Step 005370): Train loss 3.969, Val loss 4.331\n",
      "Epoch: 3 (Step 005380): Train loss 4.632, Val loss 4.335\n",
      "Epoch: 3 (Step 005390): Train loss 4.496, Val loss 4.338\n",
      "Epoch: 3 (Step 005400): Train loss 4.344, Val loss 4.367\n",
      "Epoch: 3 (Step 005410): Train loss 3.862, Val loss 4.427\n",
      "Epoch: 3 (Step 005420): Train loss 4.620, Val loss 4.403\n",
      "Epoch: 3 (Step 005430): Train loss 4.048, Val loss 4.338\n",
      "Epoch: 3 (Step 005440): Train loss 4.668, Val loss 4.338\n",
      "Epoch: 3 (Step 005450): Train loss 4.383, Val loss 4.357\n",
      "Epoch: 3 (Step 005460): Train loss 4.288, Val loss 4.373\n",
      "Epoch: 3 (Step 005470): Train loss 4.590, Val loss 4.427\n",
      "Epoch: 3 (Step 005480): Train loss 4.729, Val loss 4.418\n",
      "Epoch: 3 (Step 005490): Train loss 4.646, Val loss 4.408\n",
      "Epoch: 3 (Step 005500): Train loss 3.990, Val loss 4.398\n",
      "Epoch: 3 (Step 005510): Train loss 4.270, Val loss 4.435\n",
      "Epoch: 3 (Step 005520): Train loss 4.177, Val loss 4.399\n",
      "Epoch: 3 (Step 005530): Train loss 4.370, Val loss 4.403\n",
      "Epoch: 3 (Step 005540): Train loss 4.853, Val loss 4.404\n",
      "Epoch: 3 (Step 005550): Train loss 4.500, Val loss 4.401\n",
      "Epoch: 3 (Step 005560): Train loss 4.969, Val loss 4.400\n",
      "Epoch: 3 (Step 005570): Train loss 4.112, Val loss 4.406\n",
      "Epoch: 3 (Step 005580): Train loss 3.843, Val loss 4.355\n",
      "Epoch: 3 (Step 005590): Train loss 4.314, Val loss 4.349\n",
      "Epoch: 3 (Step 005600): Train loss 4.088, Val loss 4.324\n",
      "Epoch: 3 (Step 005610): Train loss 3.932, Val loss 4.304\n",
      "Epoch: 3 (Step 005620): Train loss 4.524, Val loss 4.316\n",
      "Epoch: 3 (Step 005630): Train loss 4.095, Val loss 4.226\n",
      "Epoch: 3 (Step 005640): Train loss 4.034, Val loss 4.197\n",
      "Epoch: 3 (Step 005650): Train loss 4.536, Val loss 4.195\n",
      "Epoch: 3 (Step 005660): Train loss 4.239, Val loss 4.218\n",
      "Epoch: 3 (Step 005670): Train loss 4.172, Val loss 4.232\n",
      "Epoch: 3 (Step 005680): Train loss 4.769, Val loss 4.178\n",
      "Epoch: 3 (Step 005690): Train loss 3.819, Val loss 4.217\n",
      "Epoch: 3 (Step 005700): Train loss 5.108, Val loss 4.268\n",
      "Epoch: 3 (Step 005710): Train loss 4.521, Val loss 4.252\n",
      "Epoch: 3 (Step 005720): Train loss 3.849, Val loss 4.231\n",
      "Epoch: 3 (Step 005730): Train loss 4.342, Val loss 4.171\n",
      "Epoch: 3 (Step 005740): Train loss 4.606, Val loss 4.213\n",
      "Epoch: 3 (Step 005750): Train loss 3.740, Val loss 4.235\n",
      "Epoch: 3 (Step 005760): Train loss 4.504, Val loss 4.231\n",
      "Epoch: 3 (Step 005770): Train loss 4.130, Val loss 4.160\n",
      "Epoch: 3 (Step 005780): Train loss 3.632, Val loss 4.166\n",
      "Epoch: 3 (Step 005790): Train loss 3.462, Val loss 4.184\n",
      "Epoch: 3 (Step 005800): Train loss 4.407, Val loss 4.202\n",
      "Epoch: 3 (Step 005810): Train loss 4.011, Val loss 4.220\n",
      "Epoch: 3 (Step 005820): Train loss 5.103, Val loss 4.233\n",
      "Epoch: 3 (Step 005830): Train loss 3.736, Val loss 4.204\n",
      "Epoch: 3 (Step 005840): Train loss 3.801, Val loss 4.219\n",
      "Epoch: 3 (Step 005850): Train loss 3.920, Val loss 4.206\n",
      "Epoch: 3 (Step 005860): Train loss 3.672, Val loss 4.216\n",
      "Epoch: 3 (Step 005870): Train loss 3.902, Val loss 4.214\n",
      "Epoch: 3 (Step 005880): Train loss 3.851, Val loss 4.219\n",
      "Epoch: 3 (Step 005890): Train loss 3.301, Val loss 4.143\n",
      "Epoch: 3 (Step 005900): Train loss 4.363, Val loss 4.155\n",
      "Epoch: 3 (Step 005910): Train loss 3.846, Val loss 4.201\n",
      "Epoch: 3 (Step 005920): Train loss 4.891, Val loss 4.215\n",
      "Epoch: 3 (Step 005930): Train loss 4.755, Val loss 4.167\n",
      "Epoch: 3 (Step 005940): Train loss 4.214, Val loss 4.133\n",
      "Epoch: 3 (Step 005950): Train loss 4.515, Val loss 4.183\n",
      "Epoch: 3 (Step 005960): Train loss 4.080, Val loss 4.258\n",
      "Epoch: 3 (Step 005970): Train loss 4.220, Val loss 4.240\n",
      "Epoch: 3 (Step 005980): Train loss 4.158, Val loss 4.304\n",
      "Epoch: 3 (Step 005990): Train loss 4.466, Val loss 4.307\n",
      "Epoch: 3 (Step 006000): Train loss 3.922, Val loss 4.276\n",
      "Epoch: 3 (Step 006010): Train loss 3.873, Val loss 4.313\n",
      "Epoch: 3 (Step 006020): Train loss 4.046, Val loss 4.268\n",
      "Epoch: 3 (Step 006030): Train loss 3.927, Val loss 4.181\n",
      "Epoch: 3 (Step 006040): Train loss 5.191, Val loss 4.181\n",
      "Epoch: 3 (Step 006050): Train loss 3.847, Val loss 4.210\n",
      "Epoch: 3 (Step 006060): Train loss 3.745, Val loss 4.207\n",
      "Epoch: 3 (Step 006070): Train loss 4.666, Val loss 4.196\n",
      "Epoch: 3 (Step 006080): Train loss 4.207, Val loss 4.187\n",
      "Epoch: 3 (Step 006090): Train loss 3.585, Val loss 4.188\n",
      "Epoch: 3 (Step 006100): Train loss 4.772, Val loss 4.260\n",
      "Epoch: 3 (Step 006110): Train loss 4.307, Val loss 4.299\n",
      "Epoch: 3 (Step 006120): Train loss 4.181, Val loss 4.243\n",
      "Epoch: 3 (Step 006130): Train loss 4.487, Val loss 4.229\n",
      "Epoch: 3 (Step 006140): Train loss 4.407, Val loss 4.205\n",
      "Epoch: 3 (Step 006150): Train loss 4.522, Val loss 4.191\n",
      "Epoch: 3 (Step 006160): Train loss 4.244, Val loss 4.214\n",
      "Epoch: 3 (Step 006170): Train loss 4.534, Val loss 4.192\n",
      "Epoch: 3 (Step 006180): Train loss 4.386, Val loss 4.199\n",
      "Epoch: 3 (Step 006190): Train loss 3.428, Val loss 4.186\n",
      "Epoch: 3 (Step 006200): Train loss 4.489, Val loss 4.152\n",
      "Epoch: 3 (Step 006210): Train loss 4.054, Val loss 4.147\n",
      "Epoch: 3 (Step 006220): Train loss 3.889, Val loss 4.150\n",
      "Epoch: 3 (Step 006230): Train loss 3.991, Val loss 4.112\n",
      "Epoch: 3 (Step 006240): Train loss 4.197, Val loss 4.169\n",
      "Epoch: 3 (Step 006250): Train loss 4.245, Val loss 4.213\n",
      "Epoch: 3 (Step 006260): Train loss 4.164, Val loss 4.214\n",
      "Epoch: 3 (Step 006270): Train loss 4.224, Val loss 4.230\n",
      "Epoch: 3 (Step 006280): Train loss 4.414, Val loss 4.279\n",
      "Epoch: 3 (Step 006290): Train loss 3.296, Val loss 4.223\n",
      "Epoch: 3 (Step 006300): Train loss 4.309, Val loss 4.152\n",
      "Epoch: 3 (Step 006310): Train loss 4.210, Val loss 4.177\n",
      "Epoch: 3 (Step 006320): Train loss 4.002, Val loss 4.211\n",
      "Epoch: 3 (Step 006330): Train loss 4.299, Val loss 4.240\n",
      "Epoch: 3 (Step 006340): Train loss 4.266, Val loss 4.236\n",
      "Epoch: 3 (Step 006350): Train loss 3.462, Val loss 4.230\n",
      "Epoch: 3 (Step 006360): Train loss 3.752, Val loss 4.223\n",
      "Epoch: 3 (Step 006370): Train loss 4.043, Val loss 4.209\n",
      "Epoch: 3 (Step 006380): Train loss 4.324, Val loss 4.184\n",
      "Epoch: 3 (Step 006390): Train loss 3.999, Val loss 4.181\n",
      "Epoch: 3 (Step 006400): Train loss 3.990, Val loss 4.153\n",
      "Epoch: 3 (Step 006410): Train loss 4.641, Val loss 4.107\n",
      "Epoch: 3 (Step 006420): Train loss 4.115, Val loss 4.118\n",
      "Epoch: 3 (Step 006430): Train loss 3.969, Val loss 4.158\n",
      "Epoch: 3 (Step 006440): Train loss 4.001, Val loss 4.166\n",
      "Epoch: 3 (Step 006450): Train loss 3.914, Val loss 4.160\n",
      "Epoch: 3 (Step 006460): Train loss 3.999, Val loss 4.169\n",
      "Epoch: 3 (Step 006470): Train loss 4.457, Val loss 4.202\n",
      "Epoch: 3 (Step 006480): Train loss 4.013, Val loss 4.195\n",
      "Epoch: 3 (Step 006490): Train loss 4.079, Val loss 4.229\n",
      "Epoch: 3 (Step 006500): Train loss 4.392, Val loss 4.234\n",
      "Epoch: 3 (Step 006510): Train loss 4.244, Val loss 4.214\n",
      "Epoch: 3 (Step 006520): Train loss 3.879, Val loss 4.201\n",
      "Epoch: 3 (Step 006530): Train loss 4.045, Val loss 4.199\n",
      "Epoch: 3 (Step 006540): Train loss 3.921, Val loss 4.224\n",
      "Epoch: 3 (Step 006550): Train loss 4.339, Val loss 4.201\n",
      "Epoch: 3 (Step 006560): Train loss 3.965, Val loss 4.225\n",
      "Epoch: 3 (Step 006570): Train loss 4.915, Val loss 4.266\n",
      "Epoch: 3 (Step 006580): Train loss 3.558, Val loss 4.258\n",
      "Epoch: 3 (Step 006590): Train loss 4.340, Val loss 4.156\n",
      "Epoch: 3 (Step 006600): Train loss 4.950, Val loss 4.118\n",
      "Epoch: 3 (Step 006610): Train loss 4.022, Val loss 4.125\n",
      "Epoch: 3 (Step 006620): Train loss 4.054, Val loss 4.138\n",
      "Epoch: 3 (Step 006630): Train loss 4.010, Val loss 4.142\n",
      "Epoch: 3 (Step 006640): Train loss 2.853, Val loss 4.146\n",
      "Epoch: 3 (Step 006650): Train loss 4.714, Val loss 4.126\n",
      "Epoch: 3 (Step 006660): Train loss 3.540, Val loss 4.094\n",
      "Epoch: 3 (Step 006670): Train loss 4.027, Val loss 4.030\n",
      "Epoch: 3 (Step 006680): Train loss 4.016, Val loss 4.057\n",
      "Epoch: 3 (Step 006690): Train loss 3.489, Val loss 4.048\n",
      "Epoch: 3 (Step 006700): Train loss 4.039, Val loss 4.032\n",
      "Epoch: 3 (Step 006710): Train loss 4.711, Val loss 4.042\n",
      "Epoch: 3 (Step 006720): Train loss 4.581, Val loss 4.009\n",
      "Epoch: 3 (Step 006730): Train loss 4.213, Val loss 4.008\n",
      "Epoch: 3 (Step 006740): Train loss 4.554, Val loss 4.030\n",
      "Epoch: 3 (Step 006750): Train loss 3.789, Val loss 4.078\n",
      "Epoch: 3 (Step 006760): Train loss 3.998, Val loss 4.161\n",
      "Epoch: 3 (Step 006770): Train loss 4.560, Val loss 4.185\n",
      "Epoch: 3 (Step 006780): Train loss 4.047, Val loss 4.133\n",
      "Epoch: 3 (Step 006790): Train loss 4.076, Val loss 4.079\n",
      "Epoch: 3 (Step 006800): Train loss 4.085, Val loss 4.072\n",
      "Epoch: 3 (Step 006810): Train loss 4.551, Val loss 4.037\n",
      "Epoch: 3 (Step 006820): Train loss 3.808, Val loss 4.039\n",
      "Epoch: 3 (Step 006830): Train loss 4.071, Val loss 4.099\n",
      "Epoch: 3 (Step 006840): Train loss 4.304, Val loss 4.094\n",
      "Epoch: 3 (Step 006850): Train loss 4.082, Val loss 4.112\n",
      "Epoch: 3 (Step 006860): Train loss 4.603, Val loss 4.106\n",
      "Epoch: 3 (Step 006870): Train loss 4.323, Val loss 4.110\n",
      "Epoch: 3 (Step 006880): Train loss 4.543, Val loss 4.136\n",
      "Epoch: 3 (Step 006890): Train loss 4.344, Val loss 4.094\n",
      "Epoch: 3 (Step 006900): Train loss 4.313, Val loss 4.051\n",
      "Epoch: 3 (Step 006910): Train loss 4.084, Val loss 4.040\n",
      "Epoch: 3 (Step 006920): Train loss 3.647, Val loss 4.041\n",
      "Epoch: 3 (Step 006930): Train loss 4.272, Val loss 4.067\n",
      "Epoch: 3 (Step 006940): Train loss 4.282, Val loss 4.059\n",
      "Epoch: 3 (Step 006950): Train loss 3.867, Val loss 4.090\n",
      "Epoch: 3 (Step 006960): Train loss 3.708, Val loss 4.100\n",
      "Epoch: 3 (Step 006970): Train loss 4.391, Val loss 4.120\n",
      "Epoch: 3 (Step 006980): Train loss 3.737, Val loss 4.131\n",
      "Epoch: 3 (Step 006990): Train loss 4.093, Val loss 4.126\n",
      "Epoch: 3 (Step 007000): Train loss 3.964, Val loss 4.135\n",
      "Epoch: 3 (Step 007010): Train loss 3.935, Val loss 4.167\n",
      "Epoch: 3 (Step 007020): Train loss 3.720, Val loss 4.124\n",
      "Epoch: 3 (Step 007030): Train loss 3.791, Val loss 4.104\n",
      "Epoch: 3 (Step 007040): Train loss 4.189, Val loss 4.090\n",
      "Epoch: 3 (Step 007050): Train loss 4.002, Val loss 4.074\n",
      "Epoch: 3 (Step 007060): Train loss 4.300, Val loss 4.094\n",
      "Epoch: 3 (Step 007070): Train loss 3.473, Val loss 4.093\n",
      "Epoch: 4 (Step 007080): Train loss 3.982, Val loss 4.111\n",
      "Epoch: 4 (Step 007090): Train loss 3.565, Val loss 4.104\n",
      "Epoch: 4 (Step 007100): Train loss 3.879, Val loss 4.071\n",
      "Epoch: 4 (Step 007110): Train loss 4.433, Val loss 4.046\n",
      "Epoch: 4 (Step 007120): Train loss 3.475, Val loss 4.075\n",
      "Epoch: 4 (Step 007130): Train loss 4.009, Val loss 4.125\n",
      "Epoch: 4 (Step 007140): Train loss 3.855, Val loss 4.157\n",
      "Epoch: 4 (Step 007150): Train loss 3.635, Val loss 4.097\n",
      "Epoch: 4 (Step 007160): Train loss 4.009, Val loss 4.111\n",
      "Epoch: 4 (Step 007170): Train loss 3.960, Val loss 4.097\n",
      "Epoch: 4 (Step 007180): Train loss 3.626, Val loss 4.129\n",
      "Epoch: 4 (Step 007190): Train loss 3.782, Val loss 4.187\n",
      "Epoch: 4 (Step 007200): Train loss 3.680, Val loss 4.181\n",
      "Epoch: 4 (Step 007210): Train loss 4.115, Val loss 4.229\n",
      "Epoch: 4 (Step 007220): Train loss 3.673, Val loss 4.175\n",
      "Epoch: 4 (Step 007230): Train loss 4.198, Val loss 4.176\n",
      "Epoch: 4 (Step 007240): Train loss 4.573, Val loss 4.164\n",
      "Epoch: 4 (Step 007250): Train loss 4.717, Val loss 4.200\n",
      "Epoch: 4 (Step 007260): Train loss 4.485, Val loss 4.165\n",
      "Epoch: 4 (Step 007270): Train loss 3.593, Val loss 4.229\n",
      "Epoch: 4 (Step 007280): Train loss 4.112, Val loss 4.274\n",
      "Epoch: 4 (Step 007290): Train loss 4.346, Val loss 4.236\n",
      "Epoch: 4 (Step 007300): Train loss 3.595, Val loss 4.216\n",
      "Epoch: 4 (Step 007310): Train loss 3.643, Val loss 4.221\n",
      "Epoch: 4 (Step 007320): Train loss 3.872, Val loss 4.177\n",
      "Epoch: 4 (Step 007330): Train loss 4.173, Val loss 4.191\n",
      "Epoch: 4 (Step 007340): Train loss 4.602, Val loss 4.215\n",
      "Epoch: 4 (Step 007350): Train loss 3.996, Val loss 4.165\n",
      "Epoch: 4 (Step 007360): Train loss 3.421, Val loss 4.154\n",
      "Epoch: 4 (Step 007370): Train loss 3.611, Val loss 4.189\n",
      "Epoch: 4 (Step 007380): Train loss 3.795, Val loss 4.196\n",
      "Epoch: 4 (Step 007390): Train loss 3.503, Val loss 4.198\n",
      "Epoch: 4 (Step 007400): Train loss 4.072, Val loss 4.187\n",
      "Epoch: 4 (Step 007410): Train loss 3.952, Val loss 4.235\n",
      "Epoch: 4 (Step 007420): Train loss 4.408, Val loss 4.290\n",
      "Epoch: 4 (Step 007430): Train loss 4.135, Val loss 4.286\n",
      "Epoch: 4 (Step 007440): Train loss 4.686, Val loss 4.244\n",
      "Epoch: 4 (Step 007450): Train loss 3.965, Val loss 4.158\n",
      "Epoch: 4 (Step 007460): Train loss 4.313, Val loss 4.148\n",
      "Epoch: 4 (Step 007470): Train loss 3.868, Val loss 4.147\n",
      "Epoch: 4 (Step 007480): Train loss 3.578, Val loss 4.119\n",
      "Epoch: 4 (Step 007490): Train loss 4.100, Val loss 4.174\n",
      "Epoch: 4 (Step 007500): Train loss 4.163, Val loss 4.136\n",
      "Epoch: 4 (Step 007510): Train loss 3.877, Val loss 4.166\n",
      "Epoch: 4 (Step 007520): Train loss 4.180, Val loss 4.214\n",
      "Epoch: 4 (Step 007530): Train loss 3.463, Val loss 4.230\n",
      "Epoch: 4 (Step 007540): Train loss 3.766, Val loss 4.246\n",
      "Epoch: 4 (Step 007550): Train loss 4.219, Val loss 4.250\n",
      "Epoch: 4 (Step 007560): Train loss 4.012, Val loss 4.202\n",
      "Epoch: 4 (Step 007570): Train loss 4.025, Val loss 4.133\n",
      "Epoch: 4 (Step 007580): Train loss 4.107, Val loss 4.138\n",
      "Epoch: 4 (Step 007590): Train loss 3.709, Val loss 4.149\n",
      "Epoch: 4 (Step 007600): Train loss 4.345, Val loss 4.128\n",
      "Epoch: 4 (Step 007610): Train loss 3.496, Val loss 4.117\n",
      "Epoch: 4 (Step 007620): Train loss 4.175, Val loss 4.076\n",
      "Epoch: 4 (Step 007630): Train loss 3.909, Val loss 4.119\n",
      "Epoch: 4 (Step 007640): Train loss 4.118, Val loss 4.132\n",
      "Epoch: 4 (Step 007650): Train loss 3.949, Val loss 4.182\n",
      "Epoch: 4 (Step 007660): Train loss 4.321, Val loss 4.218\n",
      "Epoch: 4 (Step 007670): Train loss 4.219, Val loss 4.209\n",
      "Epoch: 4 (Step 007680): Train loss 4.167, Val loss 4.205\n",
      "Epoch: 4 (Step 007690): Train loss 4.268, Val loss 4.220\n",
      "Epoch: 4 (Step 007700): Train loss 3.572, Val loss 4.269\n",
      "Epoch: 4 (Step 007710): Train loss 4.636, Val loss 4.239\n",
      "Epoch: 4 (Step 007720): Train loss 3.945, Val loss 4.263\n",
      "Epoch: 4 (Step 007730): Train loss 3.824, Val loss 4.205\n",
      "Epoch: 4 (Step 007740): Train loss 4.426, Val loss 4.233\n",
      "Epoch: 4 (Step 007750): Train loss 3.551, Val loss 4.295\n",
      "Epoch: 4 (Step 007760): Train loss 4.319, Val loss 4.252\n",
      "Epoch: 4 (Step 007770): Train loss 3.859, Val loss 4.232\n",
      "Epoch: 4 (Step 007780): Train loss 4.138, Val loss 4.214\n",
      "Epoch: 4 (Step 007790): Train loss 4.153, Val loss 4.254\n",
      "Epoch: 4 (Step 007800): Train loss 3.835, Val loss 4.260\n",
      "Epoch: 4 (Step 007810): Train loss 3.940, Val loss 4.224\n",
      "Epoch: 4 (Step 007820): Train loss 3.045, Val loss 4.202\n",
      "Epoch: 4 (Step 007830): Train loss 3.935, Val loss 4.212\n",
      "Epoch: 4 (Step 007840): Train loss 4.111, Val loss 4.197\n",
      "Epoch: 4 (Step 007850): Train loss 3.591, Val loss 4.189\n",
      "Epoch: 4 (Step 007860): Train loss 3.653, Val loss 4.230\n",
      "Epoch: 4 (Step 007870): Train loss 4.432, Val loss 4.286\n",
      "Epoch: 4 (Step 007880): Train loss 3.994, Val loss 4.246\n",
      "Epoch: 4 (Step 007890): Train loss 4.175, Val loss 4.256\n",
      "Epoch: 4 (Step 007900): Train loss 4.389, Val loss 4.245\n",
      "Epoch: 4 (Step 007910): Train loss 4.123, Val loss 4.226\n",
      "Epoch: 4 (Step 007920): Train loss 3.591, Val loss 4.203\n",
      "Epoch: 4 (Step 007930): Train loss 4.083, Val loss 4.187\n",
      "Epoch: 4 (Step 007940): Train loss 3.956, Val loss 4.174\n",
      "Epoch: 4 (Step 007950): Train loss 3.720, Val loss 4.164\n",
      "Epoch: 4 (Step 007960): Train loss 4.177, Val loss 4.172\n",
      "Epoch: 4 (Step 007970): Train loss 4.059, Val loss 4.148\n",
      "Epoch: 4 (Step 007980): Train loss 4.622, Val loss 4.152\n",
      "Epoch: 4 (Step 007990): Train loss 4.157, Val loss 4.146\n",
      "Epoch: 4 (Step 008000): Train loss 3.832, Val loss 4.196\n",
      "Epoch: 4 (Step 008010): Train loss 3.725, Val loss 4.161\n",
      "Epoch: 4 (Step 008020): Train loss 3.461, Val loss 4.164\n",
      "Epoch: 4 (Step 008030): Train loss 3.589, Val loss 4.161\n",
      "Epoch: 4 (Step 008040): Train loss 3.846, Val loss 4.187\n",
      "Epoch: 4 (Step 008050): Train loss 4.899, Val loss 4.177\n",
      "Epoch: 4 (Step 008060): Train loss 4.479, Val loss 4.146\n",
      "Epoch: 4 (Step 008070): Train loss 3.851, Val loss 4.105\n",
      "Epoch: 4 (Step 008080): Train loss 3.960, Val loss 4.119\n",
      "Epoch: 4 (Step 008090): Train loss 3.298, Val loss 4.112\n",
      "Epoch: 4 (Step 008100): Train loss 3.616, Val loss 4.140\n",
      "Epoch: 4 (Step 008110): Train loss 4.000, Val loss 4.123\n",
      "Epoch: 4 (Step 008120): Train loss 4.098, Val loss 4.121\n",
      "Epoch: 4 (Step 008130): Train loss 4.561, Val loss 4.119\n",
      "Epoch: 4 (Step 008140): Train loss 3.761, Val loss 4.141\n",
      "Epoch: 4 (Step 008150): Train loss 4.190, Val loss 4.110\n",
      "Epoch: 4 (Step 008160): Train loss 3.724, Val loss 4.146\n",
      "Epoch: 4 (Step 008170): Train loss 3.710, Val loss 4.130\n",
      "Epoch: 4 (Step 008180): Train loss 4.622, Val loss 4.152\n",
      "Epoch: 4 (Step 008190): Train loss 4.234, Val loss 4.120\n",
      "Epoch: 4 (Step 008200): Train loss 3.288, Val loss 4.114\n",
      "Epoch: 4 (Step 008210): Train loss 3.916, Val loss 4.111\n",
      "Epoch: 4 (Step 008220): Train loss 3.990, Val loss 4.130\n",
      "Epoch: 4 (Step 008230): Train loss 4.215, Val loss 4.085\n",
      "Epoch: 4 (Step 008240): Train loss 3.835, Val loss 4.093\n",
      "Epoch: 4 (Step 008250): Train loss 3.961, Val loss 4.103\n",
      "Epoch: 4 (Step 008260): Train loss 3.774, Val loss 4.127\n",
      "Epoch: 4 (Step 008270): Train loss 4.520, Val loss 4.129\n",
      "Epoch: 4 (Step 008280): Train loss 4.297, Val loss 4.057\n",
      "Epoch: 4 (Step 008290): Train loss 3.301, Val loss 4.121\n",
      "Epoch: 4 (Step 008300): Train loss 3.424, Val loss 4.113\n",
      "Epoch: 4 (Step 008310): Train loss 4.311, Val loss 4.088\n",
      "Epoch: 4 (Step 008320): Train loss 3.683, Val loss 4.096\n",
      "Epoch: 4 (Step 008330): Train loss 3.753, Val loss 4.086\n",
      "Epoch: 4 (Step 008340): Train loss 4.046, Val loss 4.069\n",
      "Epoch: 4 (Step 008350): Train loss 3.648, Val loss 4.054\n",
      "Epoch: 4 (Step 008360): Train loss 4.204, Val loss 4.059\n",
      "Epoch: 4 (Step 008370): Train loss 3.724, Val loss 4.077\n",
      "Epoch: 4 (Step 008380): Train loss 4.535, Val loss 4.110\n",
      "Epoch: 4 (Step 008390): Train loss 3.840, Val loss 4.133\n",
      "Epoch: 4 (Step 008400): Train loss 3.820, Val loss 4.165\n",
      "Epoch: 4 (Step 008410): Train loss 3.612, Val loss 4.169\n",
      "Epoch: 4 (Step 008420): Train loss 4.174, Val loss 4.145\n",
      "Epoch: 4 (Step 008430): Train loss 3.783, Val loss 4.108\n",
      "Epoch: 4 (Step 008440): Train loss 3.048, Val loss 4.085\n",
      "Epoch: 4 (Step 008450): Train loss 3.940, Val loss 4.117\n",
      "Epoch: 4 (Step 008460): Train loss 4.578, Val loss 4.080\n",
      "Epoch: 4 (Step 008470): Train loss 4.006, Val loss 4.018\n",
      "Epoch: 4 (Step 008480): Train loss 4.546, Val loss 4.005\n",
      "Epoch: 4 (Step 008490): Train loss 3.415, Val loss 4.033\n",
      "Epoch: 4 (Step 008500): Train loss 3.470, Val loss 4.048\n",
      "Epoch: 4 (Step 008510): Train loss 4.388, Val loss 4.053\n",
      "Epoch: 4 (Step 008520): Train loss 4.185, Val loss 4.113\n",
      "Epoch: 4 (Step 008530): Train loss 3.979, Val loss 4.120\n",
      "Epoch: 4 (Step 008540): Train loss 3.765, Val loss 4.090\n",
      "Epoch: 4 (Step 008550): Train loss 3.410, Val loss 4.079\n",
      "Epoch: 4 (Step 008560): Train loss 3.273, Val loss 4.082\n",
      "Epoch: 4 (Step 008570): Train loss 3.908, Val loss 4.084\n",
      "Epoch: 4 (Step 008580): Train loss 4.499, Val loss 4.124\n",
      "Epoch: 4 (Step 008590): Train loss 3.952, Val loss 4.087\n",
      "Epoch: 4 (Step 008600): Train loss 3.853, Val loss 4.110\n",
      "Epoch: 4 (Step 008610): Train loss 3.785, Val loss 4.181\n",
      "Epoch: 4 (Step 008620): Train loss 4.170, Val loss 4.156\n",
      "Epoch: 4 (Step 008630): Train loss 4.179, Val loss 4.129\n",
      "Epoch: 4 (Step 008640): Train loss 4.004, Val loss 4.107\n",
      "Epoch: 4 (Step 008650): Train loss 4.171, Val loss 4.149\n",
      "Epoch: 4 (Step 008660): Train loss 4.085, Val loss 4.114\n",
      "Epoch: 4 (Step 008670): Train loss 3.618, Val loss 4.113\n",
      "Epoch: 4 (Step 008680): Train loss 3.960, Val loss 4.086\n",
      "Epoch: 4 (Step 008690): Train loss 3.881, Val loss 4.076\n",
      "Epoch: 4 (Step 008700): Train loss 3.778, Val loss 4.069\n",
      "Epoch: 4 (Step 008710): Train loss 4.367, Val loss 4.053\n",
      "Epoch: 4 (Step 008720): Train loss 3.936, Val loss 4.095\n",
      "Epoch: 4 (Step 008730): Train loss 4.014, Val loss 4.015\n",
      "Epoch: 4 (Step 008740): Train loss 3.316, Val loss 3.972\n",
      "Epoch: 4 (Step 008750): Train loss 3.525, Val loss 3.992\n",
      "Epoch: 4 (Step 008760): Train loss 4.154, Val loss 4.063\n",
      "Epoch: 4 (Step 008770): Train loss 3.429, Val loss 4.045\n",
      "Epoch: 4 (Step 008780): Train loss 3.366, Val loss 4.033\n",
      "Epoch: 4 (Step 008790): Train loss 3.923, Val loss 4.077\n",
      "Epoch: 4 (Step 008800): Train loss 4.385, Val loss 4.075\n",
      "Epoch: 4 (Step 008810): Train loss 4.013, Val loss 4.023\n",
      "Epoch: 4 (Step 008820): Train loss 3.806, Val loss 4.023\n",
      "Epoch: 4 (Step 008830): Train loss 4.135, Val loss 4.052\n",
      "Epoch: 4 (Step 008840): Train loss 4.493, Val loss 4.093\n",
      "Epoch: 4 (Step 008850): Train loss 4.288, Val loss 4.046\n",
      "Epoch: 4 (Step 008860): Train loss 4.057, Val loss 3.959\n",
      "Epoch: 4 (Step 008870): Train loss 3.668, Val loss 4.018\n",
      "Epoch: 4 (Step 008880): Train loss 3.731, Val loss 4.004\n",
      "Epoch: 4 (Step 008890): Train loss 3.961, Val loss 3.966\n",
      "Epoch: 4 (Step 008900): Train loss 4.064, Val loss 3.968\n",
      "Epoch: 4 (Step 008910): Train loss 3.429, Val loss 4.035\n",
      "Epoch: 4 (Step 008920): Train loss 4.006, Val loss 4.021\n",
      "Epoch: 4 (Step 008930): Train loss 3.898, Val loss 3.935\n",
      "Epoch: 4 (Step 008940): Train loss 4.111, Val loss 3.928\n",
      "Epoch: 4 (Step 008950): Train loss 4.115, Val loss 3.956\n",
      "Epoch: 4 (Step 008960): Train loss 3.750, Val loss 3.966\n",
      "Epoch: 4 (Step 008970): Train loss 3.866, Val loss 3.946\n",
      "Epoch: 4 (Step 008980): Train loss 3.947, Val loss 3.938\n",
      "Epoch: 4 (Step 008990): Train loss 4.367, Val loss 3.985\n",
      "Epoch: 4 (Step 009000): Train loss 3.797, Val loss 3.975\n",
      "Epoch: 4 (Step 009010): Train loss 4.076, Val loss 3.987\n",
      "Epoch: 4 (Step 009020): Train loss 4.289, Val loss 3.997\n",
      "Epoch: 4 (Step 009030): Train loss 4.122, Val loss 3.979\n",
      "Epoch: 4 (Step 009040): Train loss 3.883, Val loss 4.000\n",
      "Epoch: 4 (Step 009050): Train loss 4.230, Val loss 4.001\n",
      "Epoch: 4 (Step 009060): Train loss 4.129, Val loss 3.988\n",
      "Epoch: 4 (Step 009070): Train loss 4.036, Val loss 3.989\n",
      "Epoch: 4 (Step 009080): Train loss 3.881, Val loss 3.962\n",
      "Epoch: 4 (Step 009090): Train loss 3.986, Val loss 3.980\n",
      "Epoch: 4 (Step 009100): Train loss 3.425, Val loss 4.025\n",
      "Epoch: 4 (Step 009110): Train loss 3.857, Val loss 4.011\n",
      "Epoch: 4 (Step 009120): Train loss 3.656, Val loss 3.997\n",
      "Epoch: 4 (Step 009130): Train loss 3.497, Val loss 4.017\n",
      "Epoch: 4 (Step 009140): Train loss 3.746, Val loss 4.034\n",
      "Epoch: 4 (Step 009150): Train loss 3.760, Val loss 3.991\n",
      "Epoch: 4 (Step 009160): Train loss 4.009, Val loss 3.948\n",
      "Epoch: 4 (Step 009170): Train loss 4.010, Val loss 3.957\n",
      "Epoch: 4 (Step 009180): Train loss 3.979, Val loss 3.937\n",
      "Epoch: 4 (Step 009190): Train loss 3.339, Val loss 3.986\n",
      "Epoch: 4 (Step 009200): Train loss 4.208, Val loss 4.021\n",
      "Epoch: 4 (Step 009210): Train loss 3.356, Val loss 3.996\n",
      "Epoch: 4 (Step 009220): Train loss 3.757, Val loss 3.955\n",
      "Epoch: 4 (Step 009230): Train loss 3.603, Val loss 3.926\n",
      "Epoch: 4 (Step 009240): Train loss 3.650, Val loss 3.934\n",
      "Epoch: 4 (Step 009250): Train loss 3.756, Val loss 3.940\n",
      "Epoch: 4 (Step 009260): Train loss 3.921, Val loss 3.972\n",
      "Epoch: 4 (Step 009270): Train loss 4.084, Val loss 3.999\n",
      "Epoch: 4 (Step 009280): Train loss 3.676, Val loss 4.033\n",
      "Epoch: 4 (Step 009290): Train loss 3.917, Val loss 4.026\n",
      "Epoch: 4 (Step 009300): Train loss 4.108, Val loss 4.035\n",
      "Epoch: 4 (Step 009310): Train loss 4.498, Val loss 3.987\n",
      "Epoch: 4 (Step 009320): Train loss 4.087, Val loss 3.917\n",
      "Epoch: 4 (Step 009330): Train loss 3.672, Val loss 3.948\n",
      "Epoch: 4 (Step 009340): Train loss 3.947, Val loss 3.984\n",
      "Epoch: 4 (Step 009350): Train loss 4.322, Val loss 3.974\n",
      "Epoch: 4 (Step 009360): Train loss 3.867, Val loss 3.990\n",
      "Epoch: 4 (Step 009370): Train loss 3.569, Val loss 4.028\n",
      "Epoch: 4 (Step 009380): Train loss 3.325, Val loss 4.019\n",
      "Epoch: 4 (Step 009390): Train loss 3.966, Val loss 3.989\n",
      "Epoch: 4 (Step 009400): Train loss 3.723, Val loss 3.995\n",
      "Epoch: 4 (Step 009410): Train loss 3.968, Val loss 4.005\n",
      "Epoch: 4 (Step 009420): Train loss 4.499, Val loss 4.022\n",
      "Epoch: 4 (Step 009430): Train loss 3.630, Val loss 3.985\n",
      "Epoch: 5 (Step 009440): Train loss 4.693, Val loss 4.002\n",
      "Epoch: 5 (Step 009450): Train loss 3.753, Val loss 4.005\n",
      "Epoch: 5 (Step 009460): Train loss 3.662, Val loss 4.021\n",
      "Epoch: 5 (Step 009470): Train loss 4.116, Val loss 4.011\n",
      "Epoch: 5 (Step 009480): Train loss 3.750, Val loss 3.990\n",
      "Epoch: 5 (Step 009490): Train loss 4.324, Val loss 4.047\n",
      "Epoch: 5 (Step 009500): Train loss 3.417, Val loss 4.075\n",
      "Epoch: 5 (Step 009510): Train loss 3.736, Val loss 4.067\n",
      "Epoch: 5 (Step 009520): Train loss 3.699, Val loss 4.025\n",
      "Epoch: 5 (Step 009530): Train loss 4.335, Val loss 4.000\n",
      "Epoch: 5 (Step 009540): Train loss 3.747, Val loss 4.024\n",
      "Epoch: 5 (Step 009550): Train loss 3.755, Val loss 4.065\n",
      "Epoch: 5 (Step 009560): Train loss 3.563, Val loss 4.022\n",
      "Epoch: 5 (Step 009570): Train loss 3.681, Val loss 4.026\n",
      "Epoch: 5 (Step 009580): Train loss 3.894, Val loss 4.077\n",
      "Epoch: 5 (Step 009590): Train loss 4.237, Val loss 4.139\n",
      "Epoch: 5 (Step 009600): Train loss 3.965, Val loss 4.125\n",
      "Epoch: 5 (Step 009610): Train loss 3.824, Val loss 4.099\n",
      "Epoch: 5 (Step 009620): Train loss 3.730, Val loss 4.109\n",
      "Epoch: 5 (Step 009630): Train loss 3.409, Val loss 4.108\n",
      "Epoch: 5 (Step 009640): Train loss 4.285, Val loss 4.079\n",
      "Epoch: 5 (Step 009650): Train loss 4.171, Val loss 4.067\n",
      "Epoch: 5 (Step 009660): Train loss 4.065, Val loss 4.043\n",
      "Epoch: 5 (Step 009670): Train loss 3.680, Val loss 3.995\n",
      "Epoch: 5 (Step 009680): Train loss 4.000, Val loss 4.030\n",
      "Epoch: 5 (Step 009690): Train loss 3.724, Val loss 4.060\n",
      "Epoch: 5 (Step 009700): Train loss 3.353, Val loss 4.095\n",
      "Epoch: 5 (Step 009710): Train loss 3.436, Val loss 4.057\n",
      "Epoch: 5 (Step 009720): Train loss 4.065, Val loss 4.071\n",
      "Epoch: 5 (Step 009730): Train loss 4.389, Val loss 4.055\n",
      "Epoch: 5 (Step 009740): Train loss 4.149, Val loss 4.081\n",
      "Epoch: 5 (Step 009750): Train loss 4.346, Val loss 4.091\n",
      "Epoch: 5 (Step 009760): Train loss 3.199, Val loss 4.105\n",
      "Epoch: 5 (Step 009770): Train loss 3.982, Val loss 4.097\n",
      "Epoch: 5 (Step 009780): Train loss 4.118, Val loss 4.147\n",
      "Epoch: 5 (Step 009790): Train loss 3.534, Val loss 4.180\n",
      "Epoch: 5 (Step 009800): Train loss 4.178, Val loss 4.146\n",
      "Epoch: 5 (Step 009810): Train loss 3.400, Val loss 4.137\n",
      "Epoch: 5 (Step 009820): Train loss 3.796, Val loss 4.184\n",
      "Epoch: 5 (Step 009830): Train loss 3.392, Val loss 4.177\n",
      "Epoch: 5 (Step 009840): Train loss 4.218, Val loss 4.136\n",
      "Epoch: 5 (Step 009850): Train loss 3.348, Val loss 4.121\n",
      "Epoch: 5 (Step 009860): Train loss 3.882, Val loss 4.178\n",
      "Epoch: 5 (Step 009870): Train loss 3.957, Val loss 4.196\n",
      "Epoch: 5 (Step 009880): Train loss 3.309, Val loss 4.138\n",
      "Epoch: 5 (Step 009890): Train loss 3.731, Val loss 4.170\n",
      "Epoch: 5 (Step 009900): Train loss 3.107, Val loss 4.187\n",
      "Epoch: 5 (Step 009910): Train loss 4.309, Val loss 4.199\n",
      "Epoch: 5 (Step 009920): Train loss 3.923, Val loss 4.211\n",
      "Epoch: 5 (Step 009930): Train loss 3.802, Val loss 4.169\n",
      "Epoch: 5 (Step 009940): Train loss 3.471, Val loss 4.223\n",
      "Epoch: 5 (Step 009950): Train loss 3.434, Val loss 4.274\n",
      "Epoch: 5 (Step 009960): Train loss 4.169, Val loss 4.224\n",
      "Epoch: 5 (Step 009970): Train loss 3.841, Val loss 4.204\n",
      "Epoch: 5 (Step 009980): Train loss 4.216, Val loss 4.194\n",
      "Epoch: 5 (Step 009990): Train loss 3.475, Val loss 4.179\n",
      "Epoch: 5 (Step 010000): Train loss 4.271, Val loss 4.205\n",
      "Epoch: 5 (Step 010010): Train loss 3.442, Val loss 4.175\n",
      "Epoch: 5 (Step 010020): Train loss 3.023, Val loss 4.203\n",
      "Epoch: 5 (Step 010030): Train loss 4.068, Val loss 4.201\n",
      "Epoch: 5 (Step 010040): Train loss 3.997, Val loss 4.203\n",
      "Epoch: 5 (Step 010050): Train loss 4.068, Val loss 4.145\n",
      "Epoch: 5 (Step 010060): Train loss 4.213, Val loss 4.091\n",
      "Epoch: 5 (Step 010070): Train loss 3.406, Val loss 4.074\n",
      "Epoch: 5 (Step 010080): Train loss 3.094, Val loss 4.124\n",
      "Epoch: 5 (Step 010090): Train loss 3.857, Val loss 4.125\n",
      "Epoch: 5 (Step 010100): Train loss 3.256, Val loss 4.053\n",
      "Epoch: 5 (Step 010110): Train loss 4.207, Val loss 4.033\n",
      "Epoch: 5 (Step 010120): Train loss 3.595, Val loss 4.076\n",
      "Epoch: 5 (Step 010130): Train loss 4.850, Val loss 4.085\n",
      "Epoch: 5 (Step 010140): Train loss 2.953, Val loss 4.098\n",
      "Epoch: 5 (Step 010150): Train loss 3.628, Val loss 4.067\n",
      "Epoch: 5 (Step 010160): Train loss 3.625, Val loss 4.118\n",
      "Epoch: 5 (Step 010170): Train loss 3.430, Val loss 4.094\n",
      "Epoch: 5 (Step 010180): Train loss 3.322, Val loss 4.085\n",
      "Epoch: 5 (Step 010190): Train loss 3.487, Val loss 4.109\n",
      "Epoch: 5 (Step 010200): Train loss 3.246, Val loss 4.102\n",
      "Epoch: 5 (Step 010210): Train loss 3.729, Val loss 4.128\n",
      "Epoch: 5 (Step 010220): Train loss 4.286, Val loss 4.118\n",
      "Epoch: 5 (Step 010230): Train loss 3.861, Val loss 4.127\n",
      "Epoch: 5 (Step 010240): Train loss 3.553, Val loss 4.121\n",
      "Epoch: 5 (Step 010250): Train loss 2.702, Val loss 4.101\n",
      "Epoch: 5 (Step 010260): Train loss 3.501, Val loss 4.099\n",
      "Epoch: 5 (Step 010270): Train loss 2.946, Val loss 4.120\n",
      "Epoch: 5 (Step 010280): Train loss 3.019, Val loss 4.130\n",
      "Epoch: 5 (Step 010290): Train loss 4.257, Val loss 4.150\n",
      "Epoch: 5 (Step 010300): Train loss 4.151, Val loss 4.180\n",
      "Epoch: 5 (Step 010310): Train loss 3.636, Val loss 4.180\n",
      "Epoch: 5 (Step 010320): Train loss 4.154, Val loss 4.095\n",
      "Epoch: 5 (Step 010330): Train loss 3.503, Val loss 4.094\n",
      "Epoch: 5 (Step 010340): Train loss 3.002, Val loss 4.103\n",
      "Epoch: 5 (Step 010350): Train loss 3.340, Val loss 4.088\n",
      "Epoch: 5 (Step 010360): Train loss 3.840, Val loss 4.066\n",
      "Epoch: 5 (Step 010370): Train loss 3.118, Val loss 4.039\n",
      "Epoch: 5 (Step 010380): Train loss 3.954, Val loss 4.043\n",
      "Epoch: 5 (Step 010390): Train loss 3.293, Val loss 4.039\n",
      "Epoch: 5 (Step 010400): Train loss 3.202, Val loss 4.054\n",
      "Epoch: 5 (Step 010410): Train loss 3.807, Val loss 4.117\n",
      "Epoch: 5 (Step 010420): Train loss 3.645, Val loss 4.084\n",
      "Epoch: 5 (Step 010430): Train loss 3.957, Val loss 4.078\n",
      "Epoch: 5 (Step 010440): Train loss 4.398, Val loss 4.033\n",
      "Epoch: 5 (Step 010450): Train loss 3.392, Val loss 4.078\n",
      "Epoch: 5 (Step 010460): Train loss 3.105, Val loss 4.098\n",
      "Epoch: 5 (Step 010470): Train loss 3.286, Val loss 4.117\n",
      "Epoch: 5 (Step 010480): Train loss 3.322, Val loss 4.151\n",
      "Epoch: 5 (Step 010490): Train loss 3.355, Val loss 4.170\n",
      "Epoch: 5 (Step 010500): Train loss 3.461, Val loss 4.142\n",
      "Epoch: 5 (Step 010510): Train loss 3.691, Val loss 4.134\n",
      "Epoch: 5 (Step 010520): Train loss 2.914, Val loss 4.097\n",
      "Epoch: 5 (Step 010530): Train loss 3.999, Val loss 4.057\n",
      "Epoch: 5 (Step 010540): Train loss 3.787, Val loss 4.022\n",
      "Epoch: 5 (Step 010550): Train loss 3.999, Val loss 3.953\n",
      "Epoch: 5 (Step 010560): Train loss 3.708, Val loss 3.967\n",
      "Epoch: 5 (Step 010570): Train loss 3.954, Val loss 4.006\n",
      "Epoch: 5 (Step 010580): Train loss 3.516, Val loss 3.979\n",
      "Epoch: 5 (Step 010590): Train loss 3.843, Val loss 3.960\n",
      "Epoch: 5 (Step 010600): Train loss 3.712, Val loss 3.978\n",
      "Epoch: 5 (Step 010610): Train loss 3.881, Val loss 3.958\n",
      "Epoch: 5 (Step 010620): Train loss 2.956, Val loss 3.951\n",
      "Epoch: 5 (Step 010630): Train loss 4.108, Val loss 3.948\n",
      "Epoch: 5 (Step 010640): Train loss 3.211, Val loss 3.981\n",
      "Epoch: 5 (Step 010650): Train loss 3.847, Val loss 3.986\n",
      "Epoch: 5 (Step 010660): Train loss 3.634, Val loss 3.986\n",
      "Epoch: 5 (Step 010670): Train loss 3.947, Val loss 4.031\n",
      "Epoch: 5 (Step 010680): Train loss 3.720, Val loss 4.031\n",
      "Epoch: 5 (Step 010690): Train loss 3.425, Val loss 4.007\n",
      "Epoch: 5 (Step 010700): Train loss 3.903, Val loss 3.988\n",
      "Epoch: 5 (Step 010710): Train loss 3.824, Val loss 3.986\n",
      "Epoch: 5 (Step 010720): Train loss 3.866, Val loss 3.980\n",
      "Epoch: 5 (Step 010730): Train loss 3.708, Val loss 3.987\n",
      "Epoch: 5 (Step 010740): Train loss 3.814, Val loss 4.007\n",
      "Epoch: 5 (Step 010750): Train loss 3.351, Val loss 3.965\n",
      "Epoch: 5 (Step 010760): Train loss 4.167, Val loss 3.956\n",
      "Epoch: 5 (Step 010770): Train loss 3.645, Val loss 3.949\n",
      "Epoch: 5 (Step 010780): Train loss 3.738, Val loss 3.935\n",
      "Epoch: 5 (Step 010790): Train loss 3.590, Val loss 3.940\n",
      "Epoch: 5 (Step 010800): Train loss 3.454, Val loss 3.944\n",
      "Epoch: 5 (Step 010810): Train loss 3.480, Val loss 3.909\n",
      "Epoch: 5 (Step 010820): Train loss 3.737, Val loss 3.946\n",
      "Epoch: 5 (Step 010830): Train loss 3.750, Val loss 3.949\n",
      "Epoch: 5 (Step 010840): Train loss 3.563, Val loss 3.918\n",
      "Epoch: 5 (Step 010850): Train loss 3.737, Val loss 3.951\n",
      "Epoch: 5 (Step 010860): Train loss 3.694, Val loss 3.959\n",
      "Epoch: 5 (Step 010870): Train loss 3.009, Val loss 3.950\n",
      "Epoch: 5 (Step 010880): Train loss 4.268, Val loss 3.914\n",
      "Epoch: 5 (Step 010890): Train loss 3.398, Val loss 3.931\n",
      "Epoch: 5 (Step 010900): Train loss 3.309, Val loss 3.971\n",
      "Epoch: 5 (Step 010910): Train loss 3.611, Val loss 3.960\n",
      "Epoch: 5 (Step 010920): Train loss 3.636, Val loss 3.963\n",
      "Epoch: 5 (Step 010930): Train loss 4.016, Val loss 3.991\n",
      "Epoch: 5 (Step 010940): Train loss 3.652, Val loss 4.017\n",
      "Epoch: 5 (Step 010950): Train loss 4.132, Val loss 4.007\n",
      "Epoch: 5 (Step 010960): Train loss 3.161, Val loss 3.992\n",
      "Epoch: 5 (Step 010970): Train loss 3.568, Val loss 4.036\n",
      "Epoch: 5 (Step 010980): Train loss 3.809, Val loss 4.022\n",
      "Epoch: 5 (Step 010990): Train loss 3.214, Val loss 4.035\n",
      "Epoch: 5 (Step 011000): Train loss 3.472, Val loss 4.041\n",
      "Epoch: 5 (Step 011010): Train loss 3.784, Val loss 3.992\n",
      "Epoch: 5 (Step 011020): Train loss 3.426, Val loss 3.982\n",
      "Epoch: 5 (Step 011030): Train loss 3.147, Val loss 3.946\n",
      "Epoch: 5 (Step 011040): Train loss 3.878, Val loss 3.925\n",
      "Epoch: 5 (Step 011050): Train loss 3.945, Val loss 3.954\n",
      "Epoch: 5 (Step 011060): Train loss 3.680, Val loss 3.945\n",
      "Epoch: 5 (Step 011070): Train loss 3.217, Val loss 3.899\n",
      "Epoch: 5 (Step 011080): Train loss 3.530, Val loss 3.893\n",
      "Epoch: 5 (Step 011090): Train loss 3.204, Val loss 3.936\n",
      "Epoch: 5 (Step 011100): Train loss 3.295, Val loss 3.903\n",
      "Epoch: 5 (Step 011110): Train loss 3.468, Val loss 3.918\n",
      "Epoch: 5 (Step 011120): Train loss 3.880, Val loss 3.917\n",
      "Epoch: 5 (Step 011130): Train loss 3.693, Val loss 3.934\n",
      "Epoch: 5 (Step 011140): Train loss 3.897, Val loss 3.965\n",
      "Epoch: 5 (Step 011150): Train loss 3.893, Val loss 3.958\n",
      "Epoch: 5 (Step 011160): Train loss 3.190, Val loss 3.936\n",
      "Epoch: 5 (Step 011170): Train loss 3.259, Val loss 3.922\n",
      "Epoch: 5 (Step 011180): Train loss 3.145, Val loss 3.928\n",
      "Epoch: 5 (Step 011190): Train loss 3.709, Val loss 3.866\n",
      "Epoch: 5 (Step 011200): Train loss 3.375, Val loss 3.836\n",
      "Epoch: 5 (Step 011210): Train loss 4.032, Val loss 3.873\n",
      "Epoch: 5 (Step 011220): Train loss 3.071, Val loss 3.809\n",
      "Epoch: 5 (Step 011230): Train loss 3.434, Val loss 3.780\n",
      "Epoch: 5 (Step 011240): Train loss 3.293, Val loss 3.830\n",
      "Epoch: 5 (Step 011250): Train loss 3.791, Val loss 3.850\n",
      "Epoch: 5 (Step 011260): Train loss 3.278, Val loss 3.902\n",
      "Epoch: 5 (Step 011270): Train loss 3.992, Val loss 3.940\n",
      "Epoch: 5 (Step 011280): Train loss 3.692, Val loss 3.899\n",
      "Epoch: 5 (Step 011290): Train loss 3.774, Val loss 3.905\n",
      "Epoch: 5 (Step 011300): Train loss 3.820, Val loss 3.942\n",
      "Epoch: 5 (Step 011310): Train loss 3.777, Val loss 3.971\n",
      "Epoch: 5 (Step 011320): Train loss 3.405, Val loss 3.942\n",
      "Epoch: 5 (Step 011330): Train loss 3.706, Val loss 3.925\n",
      "Epoch: 5 (Step 011340): Train loss 3.858, Val loss 3.914\n",
      "Epoch: 5 (Step 011350): Train loss 3.281, Val loss 3.887\n",
      "Epoch: 5 (Step 011360): Train loss 3.731, Val loss 3.891\n",
      "Epoch: 5 (Step 011370): Train loss 3.774, Val loss 3.905\n",
      "Epoch: 5 (Step 011380): Train loss 3.676, Val loss 3.879\n",
      "Epoch: 5 (Step 011390): Train loss 3.425, Val loss 3.884\n",
      "Epoch: 5 (Step 011400): Train loss 2.760, Val loss 3.903\n",
      "Epoch: 5 (Step 011410): Train loss 3.304, Val loss 3.879\n",
      "Epoch: 5 (Step 011420): Train loss 3.749, Val loss 3.897\n",
      "Epoch: 5 (Step 011430): Train loss 3.778, Val loss 3.891\n",
      "Epoch: 5 (Step 011440): Train loss 4.163, Val loss 3.907\n",
      "Epoch: 5 (Step 011450): Train loss 3.868, Val loss 3.947\n",
      "Epoch: 5 (Step 011460): Train loss 3.917, Val loss 3.938\n",
      "Epoch: 5 (Step 011470): Train loss 3.627, Val loss 3.965\n",
      "Epoch: 5 (Step 011480): Train loss 3.809, Val loss 3.972\n",
      "Epoch: 5 (Step 011490): Train loss 3.202, Val loss 3.944\n",
      "Epoch: 5 (Step 011500): Train loss 3.645, Val loss 3.938\n",
      "Epoch: 5 (Step 011510): Train loss 3.574, Val loss 3.905\n",
      "Epoch: 5 (Step 011520): Train loss 4.282, Val loss 3.955\n",
      "Epoch: 5 (Step 011530): Train loss 4.492, Val loss 3.991\n",
      "Epoch: 5 (Step 011540): Train loss 4.487, Val loss 3.970\n",
      "Epoch: 5 (Step 011550): Train loss 3.003, Val loss 3.930\n",
      "Epoch: 5 (Step 011560): Train loss 3.517, Val loss 3.955\n",
      "Epoch: 5 (Step 011570): Train loss 3.539, Val loss 3.951\n",
      "Epoch: 5 (Step 011580): Train loss 3.856, Val loss 3.960\n",
      "Epoch: 5 (Step 011590): Train loss 3.675, Val loss 3.978\n",
      "Epoch: 5 (Step 011600): Train loss 3.992, Val loss 3.996\n",
      "Epoch: 5 (Step 011610): Train loss 3.006, Val loss 3.979\n",
      "Epoch: 5 (Step 011620): Train loss 3.609, Val loss 4.000\n",
      "Epoch: 5 (Step 011630): Train loss 3.699, Val loss 4.014\n",
      "Epoch: 5 (Step 011640): Train loss 3.193, Val loss 3.964\n",
      "Epoch: 5 (Step 011650): Train loss 3.054, Val loss 3.973\n",
      "Epoch: 5 (Step 011660): Train loss 3.458, Val loss 3.953\n",
      "Epoch: 5 (Step 011670): Train loss 3.443, Val loss 3.977\n",
      "Epoch: 5 (Step 011680): Train loss 3.661, Val loss 3.996\n",
      "Epoch: 5 (Step 011690): Train loss 4.063, Val loss 4.020\n",
      "Epoch: 5 (Step 011700): Train loss 3.741, Val loss 4.021\n",
      "Epoch: 5 (Step 011710): Train loss 3.719, Val loss 4.021\n",
      "Epoch: 5 (Step 011720): Train loss 2.968, Val loss 4.017\n",
      "Epoch: 5 (Step 011730): Train loss 3.456, Val loss 3.982\n",
      "Epoch: 5 (Step 011740): Train loss 2.969, Val loss 3.944\n",
      "Epoch: 5 (Step 011750): Train loss 3.380, Val loss 3.923\n",
      "Epoch: 5 (Step 011760): Train loss 3.980, Val loss 3.939\n",
      "Epoch: 5 (Step 011770): Train loss 3.937, Val loss 3.912\n",
      "Epoch: 5 (Step 011780): Train loss 3.989, Val loss 3.946\n",
      "Epoch: 5 (Step 011790): Train loss 3.702, Val loss 3.939\n",
      "Epoch: 6 (Step 011800): Train loss 3.310, Val loss 3.940\n",
      "Epoch: 6 (Step 011810): Train loss 3.363, Val loss 3.911\n",
      "Epoch: 6 (Step 011820): Train loss 3.571, Val loss 3.900\n",
      "Epoch: 6 (Step 011830): Train loss 3.102, Val loss 3.931\n",
      "Epoch: 6 (Step 011840): Train loss 3.136, Val loss 3.956\n",
      "Epoch: 6 (Step 011850): Train loss 3.214, Val loss 3.931\n",
      "Epoch: 6 (Step 011860): Train loss 3.038, Val loss 3.932\n",
      "Epoch: 6 (Step 011870): Train loss 3.471, Val loss 3.973\n",
      "Epoch: 6 (Step 011880): Train loss 3.465, Val loss 3.990\n",
      "Epoch: 6 (Step 011890): Train loss 3.583, Val loss 3.999\n",
      "Epoch: 6 (Step 011900): Train loss 3.419, Val loss 3.970\n",
      "Epoch: 6 (Step 011910): Train loss 3.566, Val loss 3.980\n",
      "Epoch: 6 (Step 011920): Train loss 3.567, Val loss 3.996\n",
      "Epoch: 6 (Step 011930): Train loss 4.437, Val loss 3.999\n",
      "Epoch: 6 (Step 011940): Train loss 4.084, Val loss 3.993\n",
      "Epoch: 6 (Step 011950): Train loss 3.460, Val loss 3.948\n",
      "Epoch: 6 (Step 011960): Train loss 3.057, Val loss 3.973\n",
      "Epoch: 6 (Step 011970): Train loss 2.904, Val loss 3.958\n",
      "Epoch: 6 (Step 011980): Train loss 3.044, Val loss 3.946\n",
      "Epoch: 6 (Step 011990): Train loss 3.771, Val loss 3.941\n",
      "Epoch: 6 (Step 012000): Train loss 3.734, Val loss 3.986\n",
      "Epoch: 6 (Step 012010): Train loss 3.474, Val loss 3.982\n",
      "Epoch: 6 (Step 012020): Train loss 4.265, Val loss 3.941\n",
      "Epoch: 6 (Step 012030): Train loss 3.050, Val loss 3.952\n",
      "Epoch: 6 (Step 012040): Train loss 2.853, Val loss 3.983\n",
      "Epoch: 6 (Step 012050): Train loss 3.680, Val loss 4.027\n",
      "Epoch: 6 (Step 012060): Train loss 3.254, Val loss 3.994\n",
      "Epoch: 6 (Step 012070): Train loss 3.634, Val loss 3.953\n",
      "Epoch: 6 (Step 012080): Train loss 3.681, Val loss 4.021\n",
      "Epoch: 6 (Step 012090): Train loss 3.131, Val loss 4.020\n",
      "Epoch: 6 (Step 012100): Train loss 3.712, Val loss 4.044\n",
      "Epoch: 6 (Step 012110): Train loss 3.444, Val loss 4.128\n",
      "Epoch: 6 (Step 012120): Train loss 4.436, Val loss 4.124\n",
      "Epoch: 6 (Step 012130): Train loss 3.257, Val loss 4.107\n",
      "Epoch: 6 (Step 012140): Train loss 3.324, Val loss 4.103\n",
      "Epoch: 6 (Step 012150): Train loss 3.811, Val loss 4.099\n",
      "Epoch: 6 (Step 012160): Train loss 3.150, Val loss 4.052\n",
      "Epoch: 6 (Step 012170): Train loss 3.835, Val loss 4.018\n",
      "Epoch: 6 (Step 012180): Train loss 3.813, Val loss 3.931\n",
      "Epoch: 6 (Step 012190): Train loss 3.645, Val loss 3.896\n",
      "Epoch: 6 (Step 012200): Train loss 3.272, Val loss 3.942\n",
      "Epoch: 6 (Step 012210): Train loss 3.146, Val loss 3.927\n",
      "Epoch: 6 (Step 012220): Train loss 4.296, Val loss 3.880\n",
      "Epoch: 6 (Step 012230): Train loss 3.905, Val loss 3.889\n",
      "Epoch: 6 (Step 012240): Train loss 3.876, Val loss 3.917\n",
      "Epoch: 6 (Step 012250): Train loss 2.813, Val loss 3.930\n",
      "Epoch: 6 (Step 012260): Train loss 3.475, Val loss 3.912\n",
      "Epoch: 6 (Step 012270): Train loss 3.363, Val loss 3.914\n",
      "Epoch: 6 (Step 012280): Train loss 3.933, Val loss 3.940\n",
      "Epoch: 6 (Step 012290): Train loss 4.150, Val loss 3.958\n",
      "Epoch: 6 (Step 012300): Train loss 3.900, Val loss 3.950\n",
      "Epoch: 6 (Step 012310): Train loss 3.104, Val loss 3.972\n",
      "Epoch: 6 (Step 012320): Train loss 3.449, Val loss 3.986\n",
      "Epoch: 6 (Step 012330): Train loss 3.922, Val loss 3.997\n",
      "Epoch: 6 (Step 012340): Train loss 3.405, Val loss 3.962\n",
      "Epoch: 6 (Step 012350): Train loss 3.431, Val loss 3.855\n",
      "Epoch: 6 (Step 012360): Train loss 2.681, Val loss 3.849\n",
      "Epoch: 6 (Step 012370): Train loss 3.277, Val loss 3.919\n",
      "Epoch: 6 (Step 012380): Train loss 3.746, Val loss 3.936\n",
      "Epoch: 6 (Step 012390): Train loss 3.970, Val loss 3.947\n",
      "Epoch: 6 (Step 012400): Train loss 3.115, Val loss 3.976\n",
      "Epoch: 6 (Step 012410): Train loss 3.320, Val loss 3.941\n",
      "Epoch: 6 (Step 012420): Train loss 3.476, Val loss 3.924\n",
      "Epoch: 6 (Step 012430): Train loss 3.593, Val loss 3.959\n",
      "Epoch: 6 (Step 012440): Train loss 3.501, Val loss 3.976\n",
      "Epoch: 6 (Step 012450): Train loss 3.201, Val loss 3.970\n",
      "Epoch: 6 (Step 012460): Train loss 3.585, Val loss 3.960\n",
      "Epoch: 6 (Step 012470): Train loss 3.423, Val loss 3.984\n",
      "Epoch: 6 (Step 012480): Train loss 3.450, Val loss 4.019\n",
      "Epoch: 6 (Step 012490): Train loss 3.269, Val loss 4.017\n",
      "Epoch: 6 (Step 012500): Train loss 3.277, Val loss 4.021\n",
      "Epoch: 6 (Step 012510): Train loss 3.741, Val loss 4.024\n",
      "Epoch: 6 (Step 012520): Train loss 4.070, Val loss 3.975\n",
      "Epoch: 6 (Step 012530): Train loss 3.930, Val loss 3.968\n",
      "Epoch: 6 (Step 012540): Train loss 3.508, Val loss 3.991\n",
      "Epoch: 6 (Step 012550): Train loss 3.257, Val loss 4.008\n",
      "Epoch: 6 (Step 012560): Train loss 3.783, Val loss 4.009\n",
      "Epoch: 6 (Step 012570): Train loss 3.757, Val loss 4.024\n",
      "Epoch: 6 (Step 012580): Train loss 3.672, Val loss 4.014\n",
      "Epoch: 6 (Step 012590): Train loss 3.458, Val loss 3.964\n",
      "Epoch: 6 (Step 012600): Train loss 3.239, Val loss 3.993\n",
      "Epoch: 6 (Step 012610): Train loss 4.033, Val loss 3.964\n",
      "Epoch: 6 (Step 012620): Train loss 3.091, Val loss 3.966\n",
      "Epoch: 6 (Step 012630): Train loss 3.816, Val loss 3.985\n",
      "Epoch: 6 (Step 012640): Train loss 3.145, Val loss 4.018\n",
      "Epoch: 6 (Step 012650): Train loss 3.641, Val loss 4.079\n",
      "Epoch: 6 (Step 012660): Train loss 3.660, Val loss 4.083\n",
      "Epoch: 6 (Step 012670): Train loss 3.555, Val loss 4.011\n",
      "Epoch: 6 (Step 012680): Train loss 3.610, Val loss 3.967\n",
      "Epoch: 6 (Step 012690): Train loss 3.056, Val loss 3.987\n",
      "Epoch: 6 (Step 012700): Train loss 3.243, Val loss 4.005\n",
      "Epoch: 6 (Step 012710): Train loss 3.475, Val loss 3.982\n",
      "Epoch: 6 (Step 012720): Train loss 3.407, Val loss 3.952\n",
      "Epoch: 6 (Step 012730): Train loss 3.816, Val loss 3.983\n",
      "Epoch: 6 (Step 012740): Train loss 3.850, Val loss 3.957\n",
      "Epoch: 6 (Step 012750): Train loss 2.983, Val loss 3.986\n",
      "Epoch: 6 (Step 012760): Train loss 2.942, Val loss 3.955\n",
      "Epoch: 6 (Step 012770): Train loss 4.285, Val loss 3.916\n",
      "Epoch: 6 (Step 012780): Train loss 3.946, Val loss 3.964\n",
      "Epoch: 6 (Step 012790): Train loss 3.050, Val loss 3.978\n",
      "Epoch: 6 (Step 012800): Train loss 3.245, Val loss 4.011\n",
      "Epoch: 6 (Step 012810): Train loss 3.756, Val loss 4.032\n",
      "Epoch: 6 (Step 012820): Train loss 3.430, Val loss 4.039\n",
      "Epoch: 6 (Step 012830): Train loss 3.817, Val loss 4.010\n",
      "Epoch: 6 (Step 012840): Train loss 4.288, Val loss 3.985\n",
      "Epoch: 6 (Step 012850): Train loss 3.733, Val loss 3.993\n",
      "Epoch: 6 (Step 012860): Train loss 2.989, Val loss 4.035\n",
      "Epoch: 6 (Step 012870): Train loss 3.290, Val loss 4.075\n",
      "Epoch: 6 (Step 012880): Train loss 3.713, Val loss 4.052\n",
      "Epoch: 6 (Step 012890): Train loss 3.438, Val loss 4.011\n",
      "Epoch: 6 (Step 012900): Train loss 3.585, Val loss 3.950\n",
      "Epoch: 6 (Step 012910): Train loss 3.616, Val loss 3.969\n",
      "Epoch: 6 (Step 012920): Train loss 3.335, Val loss 3.992\n",
      "Epoch: 6 (Step 012930): Train loss 4.185, Val loss 4.005\n",
      "Epoch: 6 (Step 012940): Train loss 3.115, Val loss 3.989\n",
      "Epoch: 6 (Step 012950): Train loss 2.789, Val loss 3.996\n",
      "Epoch: 6 (Step 012960): Train loss 3.695, Val loss 4.011\n",
      "Epoch: 6 (Step 012970): Train loss 3.786, Val loss 4.028\n",
      "Epoch: 6 (Step 012980): Train loss 3.663, Val loss 4.057\n",
      "Epoch: 6 (Step 012990): Train loss 2.984, Val loss 4.071\n",
      "Epoch: 6 (Step 013000): Train loss 3.540, Val loss 4.017\n",
      "Epoch: 6 (Step 013010): Train loss 3.083, Val loss 4.044\n",
      "Epoch: 6 (Step 013020): Train loss 3.315, Val loss 4.040\n",
      "Epoch: 6 (Step 013030): Train loss 3.468, Val loss 4.031\n",
      "Epoch: 6 (Step 013040): Train loss 3.541, Val loss 4.080\n",
      "Epoch: 6 (Step 013050): Train loss 3.259, Val loss 4.095\n",
      "Epoch: 6 (Step 013060): Train loss 3.382, Val loss 4.083\n",
      "Epoch: 6 (Step 013070): Train loss 2.740, Val loss 4.058\n",
      "Epoch: 6 (Step 013080): Train loss 3.721, Val loss 4.048\n",
      "Epoch: 6 (Step 013090): Train loss 3.072, Val loss 4.068\n",
      "Epoch: 6 (Step 013100): Train loss 3.213, Val loss 4.110\n",
      "Epoch: 6 (Step 013110): Train loss 3.734, Val loss 4.103\n",
      "Epoch: 6 (Step 013120): Train loss 3.212, Val loss 4.024\n",
      "Epoch: 6 (Step 013130): Train loss 3.654, Val loss 4.012\n",
      "Epoch: 6 (Step 013140): Train loss 3.549, Val loss 4.029\n",
      "Epoch: 6 (Step 013150): Train loss 3.740, Val loss 4.062\n",
      "Epoch: 6 (Step 013160): Train loss 3.692, Val loss 4.111\n",
      "Epoch: 6 (Step 013170): Train loss 3.586, Val loss 4.186\n",
      "Epoch: 6 (Step 013180): Train loss 3.296, Val loss 4.187\n",
      "Epoch: 6 (Step 013190): Train loss 3.823, Val loss 4.139\n",
      "Epoch: 6 (Step 013200): Train loss 3.207, Val loss 4.152\n",
      "Epoch: 6 (Step 013210): Train loss 2.878, Val loss 4.160\n",
      "Epoch: 6 (Step 013220): Train loss 3.095, Val loss 4.110\n",
      "Epoch: 6 (Step 013230): Train loss 3.867, Val loss 4.128\n",
      "Epoch: 6 (Step 013240): Train loss 3.391, Val loss 4.124\n",
      "Epoch: 6 (Step 013250): Train loss 3.461, Val loss 4.107\n",
      "Epoch: 6 (Step 013260): Train loss 3.747, Val loss 4.084\n",
      "Epoch: 6 (Step 013270): Train loss 3.496, Val loss 4.065\n",
      "Epoch: 6 (Step 013280): Train loss 3.463, Val loss 4.101\n",
      "Epoch: 6 (Step 013290): Train loss 3.160, Val loss 4.074\n",
      "Epoch: 6 (Step 013300): Train loss 3.752, Val loss 4.071\n",
      "Epoch: 6 (Step 013310): Train loss 3.308, Val loss 4.060\n",
      "Epoch: 6 (Step 013320): Train loss 3.225, Val loss 4.049\n",
      "Epoch: 6 (Step 013330): Train loss 3.167, Val loss 4.060\n",
      "Epoch: 6 (Step 013340): Train loss 3.006, Val loss 4.058\n",
      "Epoch: 6 (Step 013350): Train loss 3.710, Val loss 4.103\n",
      "Epoch: 6 (Step 013360): Train loss 3.792, Val loss 4.099\n",
      "Epoch: 6 (Step 013370): Train loss 3.421, Val loss 4.045\n",
      "Epoch: 6 (Step 013380): Train loss 3.353, Val loss 4.041\n",
      "Epoch: 6 (Step 013390): Train loss 4.088, Val loss 4.063\n",
      "Epoch: 6 (Step 013400): Train loss 3.360, Val loss 4.027\n",
      "Epoch: 6 (Step 013410): Train loss 3.451, Val loss 4.042\n",
      "Epoch: 6 (Step 013420): Train loss 3.509, Val loss 4.047\n",
      "Epoch: 6 (Step 013430): Train loss 3.995, Val loss 4.073\n",
      "Epoch: 6 (Step 013440): Train loss 2.910, Val loss 4.081\n",
      "Epoch: 6 (Step 013450): Train loss 3.460, Val loss 4.071\n",
      "Epoch: 6 (Step 013460): Train loss 3.143, Val loss 4.065\n",
      "Epoch: 6 (Step 013470): Train loss 2.882, Val loss 3.986\n",
      "Epoch: 6 (Step 013480): Train loss 2.957, Val loss 3.964\n",
      "Epoch: 6 (Step 013490): Train loss 2.838, Val loss 4.002\n",
      "Epoch: 6 (Step 013500): Train loss 3.600, Val loss 4.021\n",
      "Epoch: 6 (Step 013510): Train loss 3.546, Val loss 4.036\n",
      "Epoch: 6 (Step 013520): Train loss 3.391, Val loss 4.054\n",
      "Epoch: 6 (Step 013530): Train loss 4.142, Val loss 4.037\n",
      "Epoch: 6 (Step 013540): Train loss 4.091, Val loss 4.004\n",
      "Epoch: 6 (Step 013550): Train loss 3.856, Val loss 3.971\n",
      "Epoch: 6 (Step 013560): Train loss 3.849, Val loss 4.020\n",
      "Epoch: 6 (Step 013570): Train loss 3.365, Val loss 4.036\n",
      "Epoch: 6 (Step 013580): Train loss 3.263, Val loss 4.021\n",
      "Epoch: 6 (Step 013590): Train loss 3.170, Val loss 4.015\n",
      "Epoch: 6 (Step 013600): Train loss 3.539, Val loss 4.011\n",
      "Epoch: 6 (Step 013610): Train loss 3.348, Val loss 4.013\n",
      "Epoch: 6 (Step 013620): Train loss 3.554, Val loss 4.026\n",
      "Epoch: 6 (Step 013630): Train loss 3.155, Val loss 4.016\n",
      "Epoch: 6 (Step 013640): Train loss 3.542, Val loss 3.984\n",
      "Epoch: 6 (Step 013650): Train loss 3.592, Val loss 3.974\n",
      "Epoch: 6 (Step 013660): Train loss 2.997, Val loss 3.973\n",
      "Epoch: 6 (Step 013670): Train loss 3.291, Val loss 3.958\n",
      "Epoch: 6 (Step 013680): Train loss 4.032, Val loss 3.943\n",
      "Epoch: 6 (Step 013690): Train loss 3.716, Val loss 3.920\n",
      "Epoch: 6 (Step 013700): Train loss 2.570, Val loss 3.943\n",
      "Epoch: 6 (Step 013710): Train loss 3.296, Val loss 3.983\n",
      "Epoch: 6 (Step 013720): Train loss 2.980, Val loss 4.042\n",
      "Epoch: 6 (Step 013730): Train loss 3.579, Val loss 4.018\n",
      "Epoch: 6 (Step 013740): Train loss 3.163, Val loss 3.976\n",
      "Epoch: 6 (Step 013750): Train loss 3.339, Val loss 3.972\n",
      "Epoch: 6 (Step 013760): Train loss 3.591, Val loss 4.010\n",
      "Epoch: 6 (Step 013770): Train loss 3.618, Val loss 4.026\n",
      "Epoch: 6 (Step 013780): Train loss 3.371, Val loss 3.950\n",
      "Epoch: 6 (Step 013790): Train loss 3.453, Val loss 3.986\n",
      "Epoch: 6 (Step 013800): Train loss 3.508, Val loss 4.010\n",
      "Epoch: 6 (Step 013810): Train loss 3.526, Val loss 3.997\n",
      "Epoch: 6 (Step 013820): Train loss 2.913, Val loss 3.991\n",
      "Epoch: 6 (Step 013830): Train loss 4.044, Val loss 4.019\n",
      "Epoch: 6 (Step 013840): Train loss 2.997, Val loss 3.995\n",
      "Epoch: 6 (Step 013850): Train loss 3.696, Val loss 4.012\n",
      "Epoch: 6 (Step 013860): Train loss 3.985, Val loss 3.980\n",
      "Epoch: 6 (Step 013870): Train loss 3.649, Val loss 3.931\n",
      "Epoch: 6 (Step 013880): Train loss 3.507, Val loss 3.964\n",
      "Epoch: 6 (Step 013890): Train loss 3.298, Val loss 3.978\n",
      "Epoch: 6 (Step 013900): Train loss 3.218, Val loss 3.960\n",
      "Epoch: 6 (Step 013910): Train loss 2.935, Val loss 3.981\n",
      "Epoch: 6 (Step 013920): Train loss 3.259, Val loss 4.028\n",
      "Epoch: 6 (Step 013930): Train loss 3.263, Val loss 4.038\n",
      "Epoch: 6 (Step 013940): Train loss 3.864, Val loss 4.071\n",
      "Epoch: 6 (Step 013950): Train loss 3.829, Val loss 4.065\n",
      "Epoch: 6 (Step 013960): Train loss 2.860, Val loss 4.052\n",
      "Epoch: 6 (Step 013970): Train loss 3.262, Val loss 4.022\n",
      "Epoch: 6 (Step 013980): Train loss 3.622, Val loss 4.039\n",
      "Epoch: 6 (Step 013990): Train loss 3.634, Val loss 4.035\n",
      "Epoch: 6 (Step 014000): Train loss 3.114, Val loss 4.083\n",
      "Epoch: 6 (Step 014010): Train loss 3.613, Val loss 4.085\n",
      "Epoch: 6 (Step 014020): Train loss 3.481, Val loss 4.079\n",
      "Epoch: 6 (Step 014030): Train loss 3.797, Val loss 4.043\n",
      "Epoch: 6 (Step 014040): Train loss 3.162, Val loss 4.021\n",
      "Epoch: 6 (Step 014050): Train loss 3.476, Val loss 4.086\n",
      "Epoch: 6 (Step 014060): Train loss 3.423, Val loss 4.097\n",
      "Epoch: 6 (Step 014070): Train loss 3.521, Val loss 4.072\n",
      "Epoch: 6 (Step 014080): Train loss 3.883, Val loss 4.084\n",
      "Epoch: 6 (Step 014090): Train loss 2.898, Val loss 4.071\n",
      "Epoch: 6 (Step 014100): Train loss 3.242, Val loss 4.041\n",
      "Epoch: 6 (Step 014110): Train loss 3.624, Val loss 3.981\n",
      "Epoch: 6 (Step 014120): Train loss 2.976, Val loss 3.937\n",
      "Epoch: 6 (Step 014130): Train loss 3.354, Val loss 3.942\n",
      "Epoch: 6 (Step 014140): Train loss 3.758, Val loss 3.950\n",
      "Epoch: 6 (Step 014150): Train loss 3.986, Val loss 3.983\n",
      "Epoch: 7 (Step 014160): Train loss 3.683, Val loss 3.999\n",
      "Epoch: 7 (Step 014170): Train loss 3.136, Val loss 4.026\n",
      "Epoch: 7 (Step 014180): Train loss 3.019, Val loss 4.030\n",
      "Epoch: 7 (Step 014190): Train loss 3.641, Val loss 4.066\n",
      "Epoch: 7 (Step 014200): Train loss 3.173, Val loss 4.056\n",
      "Epoch: 7 (Step 014210): Train loss 3.342, Val loss 4.076\n",
      "Epoch: 7 (Step 014220): Train loss 3.445, Val loss 4.103\n",
      "Epoch: 7 (Step 014230): Train loss 2.599, Val loss 4.066\n",
      "Epoch: 7 (Step 014240): Train loss 3.528, Val loss 4.093\n",
      "Epoch: 7 (Step 014250): Train loss 3.570, Val loss 4.110\n",
      "Epoch: 7 (Step 014260): Train loss 3.214, Val loss 4.120\n",
      "Epoch: 7 (Step 014270): Train loss 3.326, Val loss 4.065\n",
      "Epoch: 7 (Step 014280): Train loss 3.627, Val loss 4.073\n",
      "Epoch: 7 (Step 014290): Train loss 3.572, Val loss 4.092\n",
      "Epoch: 7 (Step 014300): Train loss 3.056, Val loss 4.115\n",
      "Epoch: 7 (Step 014310): Train loss 3.244, Val loss 4.077\n",
      "Epoch: 7 (Step 014320): Train loss 3.567, Val loss 4.066\n",
      "Epoch: 7 (Step 014330): Train loss 3.521, Val loss 4.040\n",
      "Epoch: 7 (Step 014340): Train loss 3.861, Val loss 4.040\n",
      "Epoch: 7 (Step 014350): Train loss 3.669, Val loss 4.076\n",
      "Epoch: 7 (Step 014360): Train loss 3.323, Val loss 4.054\n",
      "Epoch: 7 (Step 014370): Train loss 3.222, Val loss 4.073\n",
      "Epoch: 7 (Step 014380): Train loss 3.403, Val loss 4.072\n",
      "Epoch: 7 (Step 014390): Train loss 3.407, Val loss 4.058\n",
      "Epoch: 7 (Step 014400): Train loss 3.245, Val loss 4.061\n",
      "Epoch: 7 (Step 014410): Train loss 3.104, Val loss 4.103\n",
      "Epoch: 7 (Step 014420): Train loss 3.646, Val loss 4.032\n",
      "Epoch: 7 (Step 014430): Train loss 3.040, Val loss 3.958\n",
      "Epoch: 7 (Step 014440): Train loss 3.733, Val loss 3.946\n",
      "Epoch: 7 (Step 014450): Train loss 2.940, Val loss 4.010\n",
      "Epoch: 7 (Step 014460): Train loss 3.747, Val loss 4.063\n",
      "Epoch: 7 (Step 014470): Train loss 2.605, Val loss 4.079\n",
      "Epoch: 7 (Step 014480): Train loss 3.150, Val loss 4.113\n",
      "Epoch: 7 (Step 014490): Train loss 3.558, Val loss 4.101\n",
      "Epoch: 7 (Step 014500): Train loss 3.473, Val loss 4.104\n",
      "Epoch: 7 (Step 014510): Train loss 3.280, Val loss 4.106\n",
      "Epoch: 7 (Step 014520): Train loss 3.174, Val loss 4.110\n",
      "Epoch: 7 (Step 014530): Train loss 3.028, Val loss 4.127\n",
      "Epoch: 7 (Step 014540): Train loss 3.432, Val loss 4.163\n",
      "Epoch: 7 (Step 014550): Train loss 3.387, Val loss 4.151\n",
      "Epoch: 7 (Step 014560): Train loss 3.021, Val loss 4.139\n",
      "Epoch: 7 (Step 014570): Train loss 3.428, Val loss 4.144\n",
      "Epoch: 7 (Step 014580): Train loss 3.401, Val loss 4.155\n",
      "Epoch: 7 (Step 014590): Train loss 3.206, Val loss 4.201\n",
      "Epoch: 7 (Step 014600): Train loss 3.311, Val loss 4.176\n",
      "Epoch: 7 (Step 014610): Train loss 3.097, Val loss 4.199\n",
      "Epoch: 7 (Step 014620): Train loss 2.596, Val loss 4.167\n",
      "Epoch: 7 (Step 014630): Train loss 3.391, Val loss 4.178\n",
      "Epoch: 7 (Step 014640): Train loss 3.107, Val loss 4.216\n",
      "Epoch: 7 (Step 014650): Train loss 4.018, Val loss 4.231\n",
      "Epoch: 7 (Step 014660): Train loss 3.508, Val loss 4.257\n",
      "Epoch: 7 (Step 014670): Train loss 3.658, Val loss 4.239\n",
      "Epoch: 7 (Step 014680): Train loss 3.788, Val loss 4.230\n",
      "Epoch: 7 (Step 014690): Train loss 3.002, Val loss 4.200\n",
      "Epoch: 7 (Step 014700): Train loss 2.806, Val loss 4.144\n",
      "Epoch: 7 (Step 014710): Train loss 3.849, Val loss 4.083\n",
      "Epoch: 7 (Step 014720): Train loss 3.869, Val loss 4.184\n",
      "Epoch: 7 (Step 014730): Train loss 2.953, Val loss 4.231\n",
      "Epoch: 7 (Step 014740): Train loss 3.475, Val loss 4.201\n",
      "Epoch: 7 (Step 014750): Train loss 3.187, Val loss 4.210\n",
      "Epoch: 7 (Step 014760): Train loss 3.937, Val loss 4.215\n",
      "Epoch: 7 (Step 014770): Train loss 3.147, Val loss 4.117\n",
      "Epoch: 7 (Step 014780): Train loss 3.121, Val loss 4.092\n",
      "Epoch: 7 (Step 014790): Train loss 3.358, Val loss 4.099\n",
      "Epoch: 7 (Step 014800): Train loss 3.429, Val loss 4.075\n",
      "Epoch: 7 (Step 014810): Train loss 3.086, Val loss 4.063\n",
      "Epoch: 7 (Step 014820): Train loss 2.843, Val loss 4.102\n",
      "Epoch: 7 (Step 014830): Train loss 2.615, Val loss 4.091\n",
      "Epoch: 7 (Step 014840): Train loss 3.783, Val loss 4.060\n",
      "Epoch: 7 (Step 014850): Train loss 3.039, Val loss 4.033\n",
      "Epoch: 7 (Step 014860): Train loss 3.248, Val loss 4.102\n",
      "Epoch: 7 (Step 014870): Train loss 3.270, Val loss 4.092\n",
      "Epoch: 7 (Step 014880): Train loss 3.452, Val loss 4.052\n",
      "Epoch: 7 (Step 014890): Train loss 3.523, Val loss 4.038\n",
      "Epoch: 7 (Step 014900): Train loss 3.129, Val loss 4.026\n",
      "Epoch: 7 (Step 014910): Train loss 3.345, Val loss 4.048\n",
      "Epoch: 7 (Step 014920): Train loss 3.802, Val loss 4.065\n",
      "Epoch: 7 (Step 014930): Train loss 3.889, Val loss 4.107\n",
      "Epoch: 7 (Step 014940): Train loss 3.178, Val loss 4.107\n",
      "Epoch: 7 (Step 014950): Train loss 3.562, Val loss 4.098\n",
      "Epoch: 7 (Step 014960): Train loss 3.503, Val loss 4.078\n",
      "Epoch: 7 (Step 014970): Train loss 3.682, Val loss 4.079\n",
      "Epoch: 7 (Step 014980): Train loss 3.626, Val loss 4.134\n",
      "Epoch: 7 (Step 014990): Train loss 3.370, Val loss 4.085\n",
      "Epoch: 7 (Step 015000): Train loss 3.766, Val loss 4.092\n",
      "Epoch: 7 (Step 015010): Train loss 3.193, Val loss 4.112\n",
      "Epoch: 7 (Step 015020): Train loss 3.655, Val loss 4.106\n",
      "Epoch: 7 (Step 015030): Train loss 3.545, Val loss 4.104\n",
      "Epoch: 7 (Step 015040): Train loss 3.225, Val loss 4.077\n",
      "Epoch: 7 (Step 015050): Train loss 3.282, Val loss 4.088\n",
      "Epoch: 7 (Step 015060): Train loss 2.850, Val loss 4.104\n",
      "Epoch: 7 (Step 015070): Train loss 3.418, Val loss 4.124\n",
      "Epoch: 7 (Step 015080): Train loss 2.986, Val loss 4.083\n",
      "Epoch: 7 (Step 015090): Train loss 3.337, Val loss 4.095\n",
      "Epoch: 7 (Step 015100): Train loss 2.930, Val loss 4.064\n",
      "Epoch: 7 (Step 015110): Train loss 2.711, Val loss 4.093\n",
      "Epoch: 7 (Step 015120): Train loss 3.297, Val loss 4.072\n",
      "Epoch: 7 (Step 015130): Train loss 2.601, Val loss 4.041\n",
      "Epoch: 7 (Step 015140): Train loss 2.987, Val loss 4.044\n",
      "Epoch: 7 (Step 015150): Train loss 3.746, Val loss 3.956\n",
      "Epoch: 7 (Step 015160): Train loss 2.793, Val loss 3.925\n",
      "Epoch: 7 (Step 015170): Train loss 2.678, Val loss 3.927\n",
      "Epoch: 7 (Step 015180): Train loss 3.217, Val loss 3.973\n",
      "Epoch: 7 (Step 015190): Train loss 3.554, Val loss 3.990\n",
      "Epoch: 7 (Step 015200): Train loss 2.768, Val loss 4.000\n",
      "Epoch: 7 (Step 015210): Train loss 3.502, Val loss 4.015\n",
      "Epoch: 7 (Step 015220): Train loss 3.334, Val loss 4.015\n",
      "Epoch: 7 (Step 015230): Train loss 2.810, Val loss 3.980\n",
      "Epoch: 7 (Step 015240): Train loss 3.404, Val loss 3.983\n",
      "Epoch: 7 (Step 015250): Train loss 2.507, Val loss 4.041\n",
      "Epoch: 7 (Step 015260): Train loss 3.472, Val loss 4.006\n",
      "Epoch: 7 (Step 015270): Train loss 3.203, Val loss 3.975\n",
      "Epoch: 7 (Step 015280): Train loss 3.483, Val loss 4.022\n",
      "Epoch: 7 (Step 015290): Train loss 3.528, Val loss 4.048\n",
      "Epoch: 7 (Step 015300): Train loss 2.950, Val loss 4.074\n",
      "Epoch: 7 (Step 015310): Train loss 2.639, Val loss 4.096\n",
      "Epoch: 7 (Step 015320): Train loss 3.476, Val loss 4.112\n",
      "Epoch: 7 (Step 015330): Train loss 3.549, Val loss 4.038\n",
      "Epoch: 7 (Step 015340): Train loss 3.621, Val loss 4.031\n",
      "Epoch: 7 (Step 015350): Train loss 3.076, Val loss 4.089\n",
      "Epoch: 7 (Step 015360): Train loss 2.830, Val loss 4.050\n",
      "Epoch: 7 (Step 015370): Train loss 3.285, Val loss 4.040\n",
      "Epoch: 7 (Step 015380): Train loss 3.341, Val loss 4.041\n",
      "Epoch: 7 (Step 015390): Train loss 2.604, Val loss 4.066\n",
      "Epoch: 7 (Step 015400): Train loss 2.722, Val loss 4.048\n",
      "Epoch: 7 (Step 015410): Train loss 3.641, Val loss 4.082\n",
      "Epoch: 7 (Step 015420): Train loss 3.509, Val loss 4.105\n",
      "Epoch: 7 (Step 015430): Train loss 3.153, Val loss 4.089\n",
      "Epoch: 7 (Step 015440): Train loss 3.127, Val loss 4.039\n",
      "Epoch: 7 (Step 015450): Train loss 3.482, Val loss 3.995\n",
      "Epoch: 7 (Step 015460): Train loss 2.791, Val loss 4.013\n",
      "Epoch: 7 (Step 015470): Train loss 3.648, Val loss 3.992\n",
      "Epoch: 7 (Step 015480): Train loss 3.248, Val loss 4.026\n",
      "Epoch: 7 (Step 015490): Train loss 2.972, Val loss 4.080\n",
      "Epoch: 7 (Step 015500): Train loss 3.575, Val loss 4.040\n",
      "Epoch: 7 (Step 015510): Train loss 3.386, Val loss 4.003\n",
      "Epoch: 7 (Step 015520): Train loss 2.912, Val loss 3.957\n",
      "Epoch: 7 (Step 015530): Train loss 3.449, Val loss 3.995\n",
      "Epoch: 7 (Step 015540): Train loss 3.490, Val loss 4.035\n",
      "Epoch: 7 (Step 015550): Train loss 3.224, Val loss 4.032\n",
      "Epoch: 7 (Step 015560): Train loss 3.535, Val loss 4.034\n",
      "Epoch: 7 (Step 015570): Train loss 3.249, Val loss 4.051\n",
      "Epoch: 7 (Step 015580): Train loss 3.245, Val loss 4.101\n",
      "Epoch: 7 (Step 015590): Train loss 3.566, Val loss 4.088\n",
      "Epoch: 7 (Step 015600): Train loss 3.839, Val loss 4.064\n",
      "Epoch: 7 (Step 015610): Train loss 3.495, Val loss 4.059\n",
      "Epoch: 7 (Step 015620): Train loss 3.251, Val loss 4.034\n",
      "Epoch: 7 (Step 015630): Train loss 3.202, Val loss 4.000\n",
      "Epoch: 7 (Step 015640): Train loss 2.933, Val loss 4.020\n",
      "Epoch: 7 (Step 015650): Train loss 2.375, Val loss 3.996\n",
      "Epoch: 7 (Step 015660): Train loss 3.091, Val loss 4.030\n",
      "Epoch: 7 (Step 015670): Train loss 3.067, Val loss 4.090\n",
      "Epoch: 7 (Step 015680): Train loss 3.001, Val loss 4.064\n",
      "Epoch: 7 (Step 015690): Train loss 2.736, Val loss 4.064\n",
      "Epoch: 7 (Step 015700): Train loss 2.808, Val loss 4.017\n",
      "Epoch: 7 (Step 015710): Train loss 3.019, Val loss 3.979\n",
      "Epoch: 7 (Step 015720): Train loss 3.095, Val loss 3.943\n",
      "Epoch: 7 (Step 015730): Train loss 3.175, Val loss 3.963\n",
      "Epoch: 7 (Step 015740): Train loss 3.095, Val loss 3.982\n",
      "Epoch: 7 (Step 015750): Train loss 2.830, Val loss 3.978\n",
      "Epoch: 7 (Step 015760): Train loss 3.796, Val loss 4.030\n",
      "Epoch: 7 (Step 015770): Train loss 3.032, Val loss 4.094\n",
      "Epoch: 7 (Step 015780): Train loss 3.712, Val loss 4.083\n",
      "Epoch: 7 (Step 015790): Train loss 3.381, Val loss 4.101\n",
      "Epoch: 7 (Step 015800): Train loss 3.049, Val loss 4.064\n",
      "Epoch: 7 (Step 015810): Train loss 3.141, Val loss 4.046\n",
      "Epoch: 7 (Step 015820): Train loss 3.156, Val loss 4.023\n",
      "Epoch: 7 (Step 015830): Train loss 3.398, Val loss 4.042\n",
      "Epoch: 7 (Step 015840): Train loss 3.604, Val loss 4.049\n",
      "Epoch: 7 (Step 015850): Train loss 2.706, Val loss 4.041\n",
      "Epoch: 7 (Step 015860): Train loss 2.829, Val loss 4.043\n",
      "Epoch: 7 (Step 015870): Train loss 3.229, Val loss 4.038\n",
      "Epoch: 7 (Step 015880): Train loss 2.964, Val loss 4.031\n",
      "Epoch: 7 (Step 015890): Train loss 3.412, Val loss 4.012\n",
      "Epoch: 7 (Step 015900): Train loss 3.002, Val loss 4.024\n",
      "Epoch: 7 (Step 015910): Train loss 2.837, Val loss 4.043\n",
      "Epoch: 7 (Step 015920): Train loss 3.408, Val loss 4.014\n",
      "Epoch: 7 (Step 015930): Train loss 2.790, Val loss 4.030\n",
      "Epoch: 7 (Step 015940): Train loss 3.384, Val loss 4.067\n",
      "Epoch: 7 (Step 015950): Train loss 3.187, Val loss 4.127\n",
      "Epoch: 7 (Step 015960): Train loss 2.950, Val loss 4.149\n",
      "Epoch: 7 (Step 015970): Train loss 3.250, Val loss 4.116\n",
      "Epoch: 7 (Step 015980): Train loss 3.104, Val loss 4.098\n",
      "Epoch: 7 (Step 015990): Train loss 2.669, Val loss 4.085\n",
      "Epoch: 7 (Step 016000): Train loss 3.456, Val loss 4.108\n",
      "Epoch: 7 (Step 016010): Train loss 3.422, Val loss 4.102\n",
      "Epoch: 7 (Step 016020): Train loss 3.899, Val loss 4.112\n",
      "Epoch: 7 (Step 016030): Train loss 2.954, Val loss 4.156\n",
      "Epoch: 7 (Step 016040): Train loss 3.049, Val loss 4.135\n",
      "Epoch: 7 (Step 016050): Train loss 2.757, Val loss 4.135\n",
      "Epoch: 7 (Step 016060): Train loss 3.241, Val loss 4.095\n",
      "Epoch: 7 (Step 016070): Train loss 3.541, Val loss 4.149\n",
      "Epoch: 7 (Step 016080): Train loss 3.218, Val loss 4.104\n",
      "Epoch: 7 (Step 016090): Train loss 3.378, Val loss 4.128\n",
      "Epoch: 7 (Step 016100): Train loss 2.962, Val loss 4.137\n",
      "Epoch: 7 (Step 016110): Train loss 3.075, Val loss 4.124\n",
      "Epoch: 7 (Step 016120): Train loss 2.960, Val loss 4.101\n",
      "Epoch: 7 (Step 016130): Train loss 2.909, Val loss 4.083\n",
      "Epoch: 7 (Step 016140): Train loss 2.925, Val loss 4.093\n",
      "Epoch: 7 (Step 016150): Train loss 2.703, Val loss 4.132\n",
      "Epoch: 7 (Step 016160): Train loss 3.159, Val loss 4.144\n",
      "Epoch: 7 (Step 016170): Train loss 3.964, Val loss 4.135\n",
      "Epoch: 7 (Step 016180): Train loss 3.092, Val loss 4.147\n",
      "Epoch: 7 (Step 016190): Train loss 2.885, Val loss 4.111\n",
      "Epoch: 7 (Step 016200): Train loss 3.178, Val loss 4.135\n",
      "Epoch: 7 (Step 016210): Train loss 3.637, Val loss 4.133\n",
      "Epoch: 7 (Step 016220): Train loss 3.193, Val loss 4.164\n",
      "Epoch: 7 (Step 016230): Train loss 2.832, Val loss 4.143\n",
      "Epoch: 7 (Step 016240): Train loss 2.279, Val loss 4.127\n",
      "Epoch: 7 (Step 016250): Train loss 3.184, Val loss 4.109\n",
      "Epoch: 7 (Step 016260): Train loss 3.292, Val loss 4.067\n",
      "Epoch: 7 (Step 016270): Train loss 3.305, Val loss 4.076\n",
      "Epoch: 7 (Step 016280): Train loss 3.100, Val loss 4.131\n",
      "Epoch: 7 (Step 016290): Train loss 2.969, Val loss 4.145\n",
      "Epoch: 7 (Step 016300): Train loss 3.076, Val loss 4.150\n",
      "Epoch: 7 (Step 016310): Train loss 3.068, Val loss 4.126\n",
      "Epoch: 7 (Step 016320): Train loss 3.370, Val loss 4.142\n",
      "Epoch: 7 (Step 016330): Train loss 3.230, Val loss 4.171\n",
      "Epoch: 7 (Step 016340): Train loss 3.522, Val loss 4.147\n",
      "Epoch: 7 (Step 016350): Train loss 2.940, Val loss 4.146\n",
      "Epoch: 7 (Step 016360): Train loss 3.015, Val loss 4.181\n",
      "Epoch: 7 (Step 016370): Train loss 2.800, Val loss 4.187\n",
      "Epoch: 7 (Step 016380): Train loss 3.010, Val loss 4.159\n",
      "Epoch: 7 (Step 016390): Train loss 3.133, Val loss 4.151\n",
      "Epoch: 7 (Step 016400): Train loss 3.196, Val loss 4.097\n",
      "Epoch: 7 (Step 016410): Train loss 3.135, Val loss 4.103\n",
      "Epoch: 7 (Step 016420): Train loss 2.977, Val loss 4.085\n",
      "Epoch: 7 (Step 016430): Train loss 3.028, Val loss 4.027\n",
      "Epoch: 7 (Step 016440): Train loss 2.849, Val loss 4.067\n",
      "Epoch: 7 (Step 016450): Train loss 3.038, Val loss 4.044\n",
      "Epoch: 7 (Step 016460): Train loss 2.912, Val loss 4.036\n",
      "Epoch: 7 (Step 016470): Train loss 3.567, Val loss 4.049\n",
      "Epoch: 7 (Step 016480): Train loss 3.119, Val loss 4.008\n",
      "Epoch: 7 (Step 016490): Train loss 3.193, Val loss 4.034\n",
      "Epoch: 7 (Step 016500): Train loss 2.040, Val loss 4.062\n",
      "Epoch: 7 (Step 016510): Train loss 3.411, Val loss 4.068\n",
      "Epoch: 8 (Step 016520): Train loss 3.398, Val loss 4.082\n",
      "Epoch: 8 (Step 016530): Train loss 2.688, Val loss 4.069\n",
      "Epoch: 8 (Step 016540): Train loss 3.311, Val loss 4.088\n",
      "Epoch: 8 (Step 016550): Train loss 2.895, Val loss 4.126\n",
      "Epoch: 8 (Step 016560): Train loss 2.866, Val loss 4.136\n",
      "Epoch: 8 (Step 016570): Train loss 3.030, Val loss 4.138\n",
      "Epoch: 8 (Step 016580): Train loss 3.525, Val loss 4.109\n",
      "Epoch: 8 (Step 016590): Train loss 3.332, Val loss 4.096\n",
      "Epoch: 8 (Step 016600): Train loss 3.078, Val loss 4.075\n",
      "Epoch: 8 (Step 016610): Train loss 3.370, Val loss 4.104\n",
      "Epoch: 8 (Step 016620): Train loss 3.323, Val loss 4.104\n",
      "Epoch: 8 (Step 016630): Train loss 2.879, Val loss 4.141\n",
      "Epoch: 8 (Step 016640): Train loss 3.029, Val loss 4.225\n",
      "Epoch: 8 (Step 016650): Train loss 3.022, Val loss 4.215\n",
      "Epoch: 8 (Step 016660): Train loss 3.326, Val loss 4.196\n",
      "Epoch: 8 (Step 016670): Train loss 2.761, Val loss 4.178\n",
      "Epoch: 8 (Step 016680): Train loss 2.808, Val loss 4.175\n",
      "Epoch: 8 (Step 016690): Train loss 2.520, Val loss 4.168\n",
      "Epoch: 8 (Step 016700): Train loss 2.747, Val loss 4.161\n",
      "Epoch: 8 (Step 016710): Train loss 2.935, Val loss 4.154\n",
      "Epoch: 8 (Step 016720): Train loss 2.754, Val loss 4.135\n",
      "Epoch: 8 (Step 016730): Train loss 3.155, Val loss 4.137\n",
      "Epoch: 8 (Step 016740): Train loss 2.714, Val loss 4.142\n",
      "Epoch: 8 (Step 016750): Train loss 3.177, Val loss 4.147\n",
      "Epoch: 8 (Step 016760): Train loss 3.191, Val loss 4.126\n",
      "Epoch: 8 (Step 016770): Train loss 3.484, Val loss 4.107\n",
      "Epoch: 8 (Step 016780): Train loss 2.998, Val loss 4.125\n",
      "Epoch: 8 (Step 016790): Train loss 3.019, Val loss 4.138\n",
      "Epoch: 8 (Step 016800): Train loss 3.316, Val loss 4.171\n",
      "Epoch: 8 (Step 016810): Train loss 3.163, Val loss 4.186\n",
      "Epoch: 8 (Step 016820): Train loss 3.051, Val loss 4.184\n",
      "Epoch: 8 (Step 016830): Train loss 3.299, Val loss 4.220\n",
      "Epoch: 8 (Step 016840): Train loss 2.629, Val loss 4.216\n",
      "Epoch: 8 (Step 016850): Train loss 2.996, Val loss 4.186\n",
      "Epoch: 8 (Step 016860): Train loss 3.194, Val loss 4.170\n",
      "Epoch: 8 (Step 016870): Train loss 2.913, Val loss 4.123\n",
      "Epoch: 8 (Step 016880): Train loss 2.614, Val loss 4.092\n",
      "Epoch: 8 (Step 016890): Train loss 3.294, Val loss 4.126\n",
      "Epoch: 8 (Step 016900): Train loss 2.838, Val loss 4.144\n",
      "Epoch: 8 (Step 016910): Train loss 2.873, Val loss 4.132\n",
      "Epoch: 8 (Step 016920): Train loss 3.460, Val loss 4.090\n",
      "Epoch: 8 (Step 016930): Train loss 2.705, Val loss 4.101\n",
      "Epoch: 8 (Step 016940): Train loss 3.042, Val loss 4.114\n",
      "Epoch: 8 (Step 016950): Train loss 3.187, Val loss 4.100\n",
      "Epoch: 8 (Step 016960): Train loss 2.275, Val loss 4.120\n",
      "Epoch: 8 (Step 016970): Train loss 2.912, Val loss 4.195\n",
      "Epoch: 8 (Step 016980): Train loss 2.991, Val loss 4.164\n",
      "Epoch: 8 (Step 016990): Train loss 3.570, Val loss 4.151\n",
      "Epoch: 8 (Step 017000): Train loss 2.863, Val loss 4.112\n",
      "Epoch: 8 (Step 017010): Train loss 3.153, Val loss 4.086\n",
      "Epoch: 8 (Step 017020): Train loss 2.966, Val loss 4.133\n",
      "Epoch: 8 (Step 017030): Train loss 2.860, Val loss 4.188\n",
      "Epoch: 8 (Step 017040): Train loss 3.038, Val loss 4.194\n",
      "Epoch: 8 (Step 017050): Train loss 3.128, Val loss 4.175\n",
      "Epoch: 8 (Step 017060): Train loss 3.362, Val loss 4.191\n",
      "Epoch: 8 (Step 017070): Train loss 2.660, Val loss 4.185\n",
      "Epoch: 8 (Step 017080): Train loss 3.066, Val loss 4.187\n",
      "Epoch: 8 (Step 017090): Train loss 3.071, Val loss 4.199\n",
      "Epoch: 8 (Step 017100): Train loss 2.874, Val loss 4.229\n",
      "Epoch: 8 (Step 017110): Train loss 2.378, Val loss 4.263\n",
      "Epoch: 8 (Step 017120): Train loss 3.304, Val loss 4.236\n",
      "Epoch: 8 (Step 017130): Train loss 3.274, Val loss 4.200\n",
      "Epoch: 8 (Step 017140): Train loss 3.246, Val loss 4.203\n",
      "Epoch: 8 (Step 017150): Train loss 3.188, Val loss 4.165\n",
      "Epoch: 8 (Step 017160): Train loss 3.627, Val loss 4.160\n",
      "Epoch: 8 (Step 017170): Train loss 3.475, Val loss 4.244\n",
      "Epoch: 8 (Step 017180): Train loss 2.725, Val loss 4.191\n",
      "Epoch: 8 (Step 017190): Train loss 3.307, Val loss 4.210\n",
      "Epoch: 8 (Step 017200): Train loss 2.793, Val loss 4.187\n",
      "Epoch: 8 (Step 017210): Train loss 2.570, Val loss 4.177\n",
      "Epoch: 8 (Step 017220): Train loss 2.983, Val loss 4.197\n",
      "Epoch: 8 (Step 017230): Train loss 3.646, Val loss 4.203\n",
      "Epoch: 8 (Step 017240): Train loss 2.740, Val loss 4.210\n",
      "Epoch: 8 (Step 017250): Train loss 2.910, Val loss 4.213\n",
      "Epoch: 8 (Step 017260): Train loss 3.183, Val loss 4.198\n",
      "Epoch: 8 (Step 017270): Train loss 3.435, Val loss 4.212\n",
      "Epoch: 8 (Step 017280): Train loss 2.646, Val loss 4.181\n",
      "Epoch: 8 (Step 017290): Train loss 3.624, Val loss 4.190\n",
      "Epoch: 8 (Step 017300): Train loss 3.187, Val loss 4.196\n",
      "Epoch: 8 (Step 017310): Train loss 2.743, Val loss 4.223\n",
      "Epoch: 8 (Step 017320): Train loss 2.897, Val loss 4.203\n",
      "Epoch: 8 (Step 017330): Train loss 3.072, Val loss 4.231\n",
      "Epoch: 8 (Step 017340): Train loss 3.065, Val loss 4.275\n",
      "Epoch: 8 (Step 017350): Train loss 3.299, Val loss 4.222\n",
      "Epoch: 8 (Step 017360): Train loss 2.610, Val loss 4.240\n",
      "Epoch: 8 (Step 017370): Train loss 3.334, Val loss 4.291\n",
      "Epoch: 8 (Step 017380): Train loss 3.255, Val loss 4.254\n",
      "Epoch: 8 (Step 017390): Train loss 2.911, Val loss 4.246\n",
      "Epoch: 8 (Step 017400): Train loss 3.244, Val loss 4.192\n",
      "Epoch: 8 (Step 017410): Train loss 2.916, Val loss 4.176\n",
      "Epoch: 8 (Step 017420): Train loss 3.173, Val loss 4.204\n",
      "Epoch: 8 (Step 017430): Train loss 2.491, Val loss 4.230\n",
      "Epoch: 8 (Step 017440): Train loss 2.509, Val loss 4.253\n",
      "Epoch: 8 (Step 017450): Train loss 2.740, Val loss 4.275\n",
      "Epoch: 8 (Step 017460): Train loss 3.031, Val loss 4.307\n",
      "Epoch: 8 (Step 017470): Train loss 3.210, Val loss 4.291\n",
      "Epoch: 8 (Step 017480): Train loss 2.730, Val loss 4.299\n",
      "Epoch: 8 (Step 017490): Train loss 3.076, Val loss 4.224\n",
      "Epoch: 8 (Step 017500): Train loss 2.738, Val loss 4.170\n",
      "Epoch: 8 (Step 017510): Train loss 3.090, Val loss 4.243\n",
      "Epoch: 8 (Step 017520): Train loss 3.451, Val loss 4.235\n",
      "Epoch: 8 (Step 017530): Train loss 2.831, Val loss 4.191\n",
      "Epoch: 8 (Step 017540): Train loss 3.242, Val loss 4.135\n",
      "Epoch: 8 (Step 017550): Train loss 2.836, Val loss 4.143\n",
      "Epoch: 8 (Step 017560): Train loss 3.415, Val loss 4.135\n",
      "Epoch: 8 (Step 017570): Train loss 3.619, Val loss 4.083\n",
      "Epoch: 8 (Step 017580): Train loss 3.050, Val loss 4.087\n",
      "Epoch: 8 (Step 017590): Train loss 2.347, Val loss 4.116\n",
      "Epoch: 8 (Step 017600): Train loss 3.410, Val loss 4.087\n",
      "Epoch: 8 (Step 017610): Train loss 2.971, Val loss 4.109\n",
      "Epoch: 8 (Step 017620): Train loss 2.811, Val loss 4.117\n",
      "Epoch: 8 (Step 017630): Train loss 2.332, Val loss 4.102\n",
      "Epoch: 8 (Step 017640): Train loss 3.238, Val loss 4.129\n",
      "Epoch: 8 (Step 017650): Train loss 3.322, Val loss 4.123\n",
      "Epoch: 8 (Step 017660): Train loss 2.072, Val loss 4.086\n",
      "Epoch: 8 (Step 017670): Train loss 2.616, Val loss 4.038\n",
      "Epoch: 8 (Step 017680): Train loss 2.822, Val loss 3.989\n",
      "Epoch: 8 (Step 017690): Train loss 3.005, Val loss 4.022\n",
      "Epoch: 8 (Step 017700): Train loss 2.681, Val loss 4.015\n",
      "Epoch: 8 (Step 017710): Train loss 3.304, Val loss 4.057\n",
      "Epoch: 8 (Step 017720): Train loss 3.100, Val loss 4.072\n",
      "Epoch: 8 (Step 017730): Train loss 2.559, Val loss 4.127\n",
      "Epoch: 8 (Step 017740): Train loss 3.311, Val loss 4.067\n",
      "Epoch: 8 (Step 017750): Train loss 3.228, Val loss 4.046\n",
      "Epoch: 8 (Step 017760): Train loss 2.959, Val loss 4.012\n",
      "Epoch: 8 (Step 017770): Train loss 2.805, Val loss 4.038\n",
      "Epoch: 8 (Step 017780): Train loss 2.524, Val loss 4.063\n",
      "Epoch: 8 (Step 017790): Train loss 3.417, Val loss 4.089\n",
      "Epoch: 8 (Step 017800): Train loss 2.849, Val loss 4.093\n",
      "Epoch: 8 (Step 017810): Train loss 2.426, Val loss 4.139\n",
      "Epoch: 8 (Step 017820): Train loss 3.029, Val loss 4.076\n",
      "Epoch: 8 (Step 017830): Train loss 3.623, Val loss 4.116\n",
      "Epoch: 8 (Step 017840): Train loss 2.896, Val loss 4.117\n",
      "Epoch: 8 (Step 017850): Train loss 3.141, Val loss 4.146\n",
      "Epoch: 8 (Step 017860): Train loss 3.210, Val loss 4.206\n",
      "Epoch: 8 (Step 017870): Train loss 2.868, Val loss 4.188\n",
      "Epoch: 8 (Step 017880): Train loss 2.316, Val loss 4.126\n",
      "Epoch: 8 (Step 017890): Train loss 2.920, Val loss 4.140\n",
      "Epoch: 8 (Step 017900): Train loss 3.380, Val loss 4.099\n",
      "Epoch: 8 (Step 017910): Train loss 3.436, Val loss 4.063\n",
      "Epoch: 8 (Step 017920): Train loss 3.579, Val loss 4.068\n",
      "Epoch: 8 (Step 017930): Train loss 2.722, Val loss 4.109\n",
      "Epoch: 8 (Step 017940): Train loss 3.012, Val loss 4.115\n",
      "Epoch: 8 (Step 017950): Train loss 2.922, Val loss 4.113\n",
      "Epoch: 8 (Step 017960): Train loss 3.029, Val loss 4.109\n",
      "Epoch: 8 (Step 017970): Train loss 3.323, Val loss 4.100\n",
      "Epoch: 8 (Step 017980): Train loss 2.773, Val loss 4.106\n",
      "Epoch: 8 (Step 017990): Train loss 2.954, Val loss 4.103\n",
      "Epoch: 8 (Step 018000): Train loss 2.956, Val loss 4.099\n",
      "Epoch: 8 (Step 018010): Train loss 2.931, Val loss 4.109\n",
      "Epoch: 8 (Step 018020): Train loss 2.978, Val loss 4.111\n",
      "Epoch: 8 (Step 018030): Train loss 2.990, Val loss 4.100\n",
      "Epoch: 8 (Step 018040): Train loss 3.234, Val loss 4.116\n",
      "Epoch: 8 (Step 018050): Train loss 2.995, Val loss 4.146\n",
      "Epoch: 8 (Step 018060): Train loss 2.553, Val loss 4.164\n",
      "Epoch: 8 (Step 018070): Train loss 3.073, Val loss 4.182\n",
      "Epoch: 8 (Step 018080): Train loss 3.401, Val loss 4.080\n",
      "Epoch: 8 (Step 018090): Train loss 2.778, Val loss 4.056\n",
      "Epoch: 8 (Step 018100): Train loss 3.171, Val loss 4.088\n",
      "Epoch: 8 (Step 018110): Train loss 2.844, Val loss 4.121\n",
      "Epoch: 8 (Step 018120): Train loss 3.362, Val loss 4.072\n",
      "Epoch: 8 (Step 018130): Train loss 3.140, Val loss 4.096\n",
      "Epoch: 8 (Step 018140): Train loss 3.311, Val loss 4.106\n",
      "Epoch: 8 (Step 018150): Train loss 3.270, Val loss 4.102\n",
      "Epoch: 8 (Step 018160): Train loss 2.706, Val loss 4.151\n",
      "Epoch: 8 (Step 018170): Train loss 2.962, Val loss 4.160\n",
      "Epoch: 8 (Step 018180): Train loss 3.369, Val loss 4.072\n",
      "Epoch: 8 (Step 018190): Train loss 3.061, Val loss 4.079\n",
      "Epoch: 8 (Step 018200): Train loss 2.607, Val loss 4.086\n",
      "Epoch: 8 (Step 018210): Train loss 2.559, Val loss 4.127\n",
      "Epoch: 8 (Step 018220): Train loss 2.604, Val loss 4.113\n",
      "Epoch: 8 (Step 018230): Train loss 2.895, Val loss 4.067\n",
      "Epoch: 8 (Step 018240): Train loss 2.337, Val loss 4.103\n",
      "Epoch: 8 (Step 018250): Train loss 3.026, Val loss 4.105\n",
      "Epoch: 8 (Step 018260): Train loss 2.662, Val loss 4.166\n",
      "Epoch: 8 (Step 018270): Train loss 3.101, Val loss 4.195\n",
      "Epoch: 8 (Step 018280): Train loss 3.184, Val loss 4.146\n",
      "Epoch: 8 (Step 018290): Train loss 2.858, Val loss 4.124\n",
      "Epoch: 8 (Step 018300): Train loss 2.935, Val loss 4.083\n",
      "Epoch: 8 (Step 018310): Train loss 2.820, Val loss 4.119\n",
      "Epoch: 8 (Step 018320): Train loss 2.984, Val loss 4.104\n",
      "Epoch: 8 (Step 018330): Train loss 2.966, Val loss 4.098\n",
      "Epoch: 8 (Step 018340): Train loss 2.685, Val loss 4.066\n",
      "Epoch: 8 (Step 018350): Train loss 2.661, Val loss 4.058\n",
      "Epoch: 8 (Step 018360): Train loss 3.112, Val loss 4.040\n",
      "Epoch: 8 (Step 018370): Train loss 2.588, Val loss 4.067\n",
      "Epoch: 8 (Step 018380): Train loss 2.724, Val loss 4.077\n",
      "Epoch: 8 (Step 018390): Train loss 2.873, Val loss 4.067\n",
      "Epoch: 8 (Step 018400): Train loss 3.226, Val loss 4.084\n",
      "Epoch: 8 (Step 018410): Train loss 2.870, Val loss 4.104\n",
      "Epoch: 8 (Step 018420): Train loss 2.758, Val loss 4.119\n",
      "Epoch: 8 (Step 018430): Train loss 2.666, Val loss 4.114\n",
      "Epoch: 8 (Step 018440): Train loss 3.684, Val loss 4.090\n",
      "Epoch: 8 (Step 018450): Train loss 2.800, Val loss 4.110\n",
      "Epoch: 8 (Step 018460): Train loss 2.868, Val loss 4.167\n",
      "Epoch: 8 (Step 018470): Train loss 3.204, Val loss 4.109\n",
      "Epoch: 8 (Step 018480): Train loss 2.904, Val loss 4.097\n",
      "Epoch: 8 (Step 018490): Train loss 2.728, Val loss 4.083\n",
      "Epoch: 8 (Step 018500): Train loss 2.697, Val loss 4.096\n",
      "Epoch: 8 (Step 018510): Train loss 2.963, Val loss 4.104\n",
      "Epoch: 8 (Step 018520): Train loss 2.888, Val loss 4.103\n",
      "Epoch: 8 (Step 018530): Train loss 2.910, Val loss 4.087\n",
      "Epoch: 8 (Step 018540): Train loss 2.873, Val loss 4.051\n",
      "Epoch: 8 (Step 018550): Train loss 2.786, Val loss 4.062\n",
      "Epoch: 8 (Step 018560): Train loss 2.906, Val loss 4.050\n",
      "Epoch: 8 (Step 018570): Train loss 2.684, Val loss 4.050\n",
      "Epoch: 8 (Step 018580): Train loss 3.321, Val loss 4.055\n",
      "Epoch: 8 (Step 018590): Train loss 3.138, Val loss 4.051\n",
      "Epoch: 8 (Step 018600): Train loss 3.040, Val loss 4.071\n",
      "Epoch: 8 (Step 018610): Train loss 2.829, Val loss 4.061\n",
      "Epoch: 8 (Step 018620): Train loss 2.640, Val loss 4.032\n",
      "Epoch: 8 (Step 018630): Train loss 2.721, Val loss 4.067\n",
      "Epoch: 8 (Step 018640): Train loss 2.372, Val loss 4.088\n",
      "Epoch: 8 (Step 018650): Train loss 2.249, Val loss 4.098\n",
      "Epoch: 8 (Step 018660): Train loss 2.515, Val loss 4.134\n",
      "Epoch: 8 (Step 018670): Train loss 2.977, Val loss 4.106\n",
      "Epoch: 8 (Step 018680): Train loss 2.410, Val loss 4.083\n",
      "Epoch: 8 (Step 018690): Train loss 2.612, Val loss 4.056\n",
      "Epoch: 8 (Step 018700): Train loss 2.491, Val loss 4.034\n",
      "Epoch: 8 (Step 018710): Train loss 2.623, Val loss 4.019\n",
      "Epoch: 8 (Step 018720): Train loss 3.030, Val loss 4.078\n",
      "Epoch: 8 (Step 018730): Train loss 2.592, Val loss 4.067\n",
      "Epoch: 8 (Step 018740): Train loss 3.156, Val loss 4.092\n",
      "Epoch: 8 (Step 018750): Train loss 3.293, Val loss 4.101\n",
      "Epoch: 8 (Step 018760): Train loss 2.771, Val loss 4.186\n",
      "Epoch: 8 (Step 018770): Train loss 2.362, Val loss 4.158\n",
      "Epoch: 8 (Step 018780): Train loss 2.918, Val loss 4.128\n",
      "Epoch: 8 (Step 018790): Train loss 3.151, Val loss 4.119\n",
      "Epoch: 8 (Step 018800): Train loss 2.729, Val loss 4.126\n",
      "Epoch: 8 (Step 018810): Train loss 2.846, Val loss 4.095\n",
      "Epoch: 8 (Step 018820): Train loss 2.666, Val loss 4.086\n",
      "Epoch: 8 (Step 018830): Train loss 2.972, Val loss 4.120\n",
      "Epoch: 8 (Step 018840): Train loss 2.673, Val loss 4.135\n",
      "Epoch: 8 (Step 018850): Train loss 2.960, Val loss 4.096\n",
      "Epoch: 8 (Step 018860): Train loss 2.697, Val loss 4.063\n",
      "Epoch: 8 (Step 018870): Train loss 2.755, Val loss 4.045\n",
      "Epoch: 9 (Step 018880): Train loss 2.931, Val loss 4.049\n",
      "Epoch: 9 (Step 018890): Train loss 3.455, Val loss 4.126\n",
      "Epoch: 9 (Step 018900): Train loss 2.949, Val loss 4.130\n",
      "Epoch: 9 (Step 018910): Train loss 2.849, Val loss 4.109\n",
      "Epoch: 9 (Step 018920): Train loss 2.945, Val loss 4.065\n",
      "Epoch: 9 (Step 018930): Train loss 3.031, Val loss 4.010\n",
      "Epoch: 9 (Step 018940): Train loss 2.449, Val loss 4.028\n",
      "Epoch: 9 (Step 018950): Train loss 2.920, Val loss 4.073\n",
      "Epoch: 9 (Step 018960): Train loss 2.680, Val loss 4.095\n",
      "Epoch: 9 (Step 018970): Train loss 2.549, Val loss 4.124\n",
      "Epoch: 9 (Step 018980): Train loss 2.668, Val loss 4.171\n",
      "Epoch: 9 (Step 018990): Train loss 2.415, Val loss 4.110\n",
      "Epoch: 9 (Step 019000): Train loss 3.034, Val loss 4.096\n",
      "Epoch: 9 (Step 019010): Train loss 3.246, Val loss 4.087\n",
      "Epoch: 9 (Step 019020): Train loss 2.257, Val loss 4.091\n",
      "Epoch: 9 (Step 019030): Train loss 2.476, Val loss 4.043\n",
      "Epoch: 9 (Step 019040): Train loss 2.482, Val loss 4.147\n",
      "Epoch: 9 (Step 019050): Train loss 2.545, Val loss 4.159\n",
      "Epoch: 9 (Step 019060): Train loss 2.904, Val loss 4.167\n",
      "Epoch: 9 (Step 019070): Train loss 2.892, Val loss 4.161\n",
      "Epoch: 9 (Step 019080): Train loss 2.472, Val loss 4.154\n",
      "Epoch: 9 (Step 019090): Train loss 2.996, Val loss 4.175\n",
      "Epoch: 9 (Step 019100): Train loss 3.123, Val loss 4.138\n",
      "Epoch: 9 (Step 019110): Train loss 2.658, Val loss 4.126\n",
      "Epoch: 9 (Step 019120): Train loss 2.802, Val loss 4.093\n",
      "Epoch: 9 (Step 019130): Train loss 2.887, Val loss 4.156\n",
      "Epoch: 9 (Step 019140): Train loss 2.910, Val loss 4.211\n",
      "Epoch: 9 (Step 019150): Train loss 3.087, Val loss 4.256\n",
      "Epoch: 9 (Step 019160): Train loss 2.903, Val loss 4.198\n",
      "Epoch: 9 (Step 019170): Train loss 2.433, Val loss 4.186\n",
      "Epoch: 9 (Step 019180): Train loss 2.881, Val loss 4.185\n",
      "Epoch: 9 (Step 019190): Train loss 2.834, Val loss 4.183\n",
      "Epoch: 9 (Step 019200): Train loss 2.969, Val loss 4.205\n",
      "Epoch: 9 (Step 019210): Train loss 2.805, Val loss 4.245\n",
      "Epoch: 9 (Step 019220): Train loss 2.458, Val loss 4.236\n",
      "Epoch: 9 (Step 019230): Train loss 3.144, Val loss 4.169\n",
      "Epoch: 9 (Step 019240): Train loss 3.583, Val loss 4.143\n",
      "Epoch: 9 (Step 019250): Train loss 2.711, Val loss 4.177\n",
      "Epoch: 9 (Step 019260): Train loss 2.898, Val loss 4.164\n",
      "Epoch: 9 (Step 019270): Train loss 2.379, Val loss 4.179\n",
      "Epoch: 9 (Step 019280): Train loss 2.343, Val loss 4.218\n",
      "Epoch: 9 (Step 019290): Train loss 3.024, Val loss 4.254\n",
      "Epoch: 9 (Step 019300): Train loss 3.089, Val loss 4.254\n",
      "Epoch: 9 (Step 019310): Train loss 3.540, Val loss 4.214\n",
      "Epoch: 9 (Step 019320): Train loss 3.104, Val loss 4.212\n",
      "Epoch: 9 (Step 019330): Train loss 3.079, Val loss 4.221\n",
      "Epoch: 9 (Step 019340): Train loss 2.355, Val loss 4.262\n",
      "Epoch: 9 (Step 019350): Train loss 2.604, Val loss 4.234\n",
      "Epoch: 9 (Step 019360): Train loss 2.634, Val loss 4.262\n",
      "Epoch: 9 (Step 019370): Train loss 2.235, Val loss 4.258\n",
      "Epoch: 9 (Step 019380): Train loss 2.863, Val loss 4.255\n",
      "Epoch: 9 (Step 019390): Train loss 2.994, Val loss 4.237\n",
      "Epoch: 9 (Step 019400): Train loss 3.045, Val loss 4.285\n",
      "Epoch: 9 (Step 019410): Train loss 2.423, Val loss 4.245\n",
      "Epoch: 9 (Step 019420): Train loss 2.687, Val loss 4.227\n",
      "Epoch: 9 (Step 019430): Train loss 2.560, Val loss 4.284\n",
      "Epoch: 9 (Step 019440): Train loss 2.788, Val loss 4.243\n",
      "Epoch: 9 (Step 019450): Train loss 2.430, Val loss 4.243\n",
      "Epoch: 9 (Step 019460): Train loss 3.052, Val loss 4.208\n",
      "Epoch: 9 (Step 019470): Train loss 2.821, Val loss 4.183\n",
      "Epoch: 9 (Step 019480): Train loss 3.279, Val loss 4.246\n",
      "Epoch: 9 (Step 019490): Train loss 2.798, Val loss 4.189\n",
      "Epoch: 9 (Step 019500): Train loss 3.049, Val loss 4.194\n",
      "Epoch: 9 (Step 019510): Train loss 3.087, Val loss 4.233\n",
      "Epoch: 9 (Step 019520): Train loss 2.391, Val loss 4.256\n",
      "Epoch: 9 (Step 019530): Train loss 2.479, Val loss 4.289\n",
      "Epoch: 9 (Step 019540): Train loss 2.740, Val loss 4.297\n",
      "Epoch: 9 (Step 019550): Train loss 2.764, Val loss 4.285\n",
      "Epoch: 9 (Step 019560): Train loss 2.944, Val loss 4.292\n",
      "Epoch: 9 (Step 019570): Train loss 2.795, Val loss 4.290\n",
      "Epoch: 9 (Step 019580): Train loss 2.323, Val loss 4.292\n",
      "Epoch: 9 (Step 019590): Train loss 2.880, Val loss 4.267\n",
      "Epoch: 9 (Step 019600): Train loss 3.079, Val loss 4.249\n",
      "Epoch: 9 (Step 019610): Train loss 2.632, Val loss 4.236\n",
      "Epoch: 9 (Step 019620): Train loss 3.067, Val loss 4.242\n",
      "Epoch: 9 (Step 019630): Train loss 3.274, Val loss 4.186\n",
      "Epoch: 9 (Step 019640): Train loss 2.897, Val loss 4.138\n",
      "Epoch: 9 (Step 019650): Train loss 3.295, Val loss 4.183\n",
      "Epoch: 9 (Step 019660): Train loss 2.854, Val loss 4.255\n",
      "Epoch: 9 (Step 019670): Train loss 2.695, Val loss 4.272\n",
      "Epoch: 9 (Step 019680): Train loss 2.624, Val loss 4.315\n",
      "Epoch: 9 (Step 019690): Train loss 2.780, Val loss 4.353\n",
      "Epoch: 9 (Step 019700): Train loss 2.433, Val loss 4.275\n",
      "Epoch: 9 (Step 019710): Train loss 2.622, Val loss 4.250\n",
      "Epoch: 9 (Step 019720): Train loss 3.175, Val loss 4.259\n",
      "Epoch: 9 (Step 019730): Train loss 2.686, Val loss 4.265\n",
      "Epoch: 9 (Step 019740): Train loss 2.533, Val loss 4.246\n",
      "Epoch: 9 (Step 019750): Train loss 2.376, Val loss 4.290\n",
      "Epoch: 9 (Step 019760): Train loss 2.707, Val loss 4.285\n",
      "Epoch: 9 (Step 019770): Train loss 2.739, Val loss 4.281\n",
      "Epoch: 9 (Step 019780): Train loss 3.170, Val loss 4.278\n",
      "Epoch: 9 (Step 019790): Train loss 2.904, Val loss 4.296\n",
      "Epoch: 9 (Step 019800): Train loss 2.549, Val loss 4.285\n",
      "Epoch: 9 (Step 019810): Train loss 3.055, Val loss 4.313\n",
      "Epoch: 9 (Step 019820): Train loss 2.474, Val loss 4.349\n",
      "Epoch: 9 (Step 019830): Train loss 2.812, Val loss 4.276\n",
      "Epoch: 9 (Step 019840): Train loss 2.210, Val loss 4.260\n",
      "Epoch: 9 (Step 019850): Train loss 2.474, Val loss 4.279\n",
      "Epoch: 9 (Step 019860): Train loss 3.184, Val loss 4.284\n",
      "Epoch: 9 (Step 019870): Train loss 2.386, Val loss 4.278\n",
      "Epoch: 9 (Step 019880): Train loss 2.753, Val loss 4.272\n",
      "Epoch: 9 (Step 019890): Train loss 2.518, Val loss 4.274\n",
      "Epoch: 9 (Step 019900): Train loss 2.560, Val loss 4.336\n",
      "Epoch: 9 (Step 019910): Train loss 3.176, Val loss 4.329\n",
      "Epoch: 9 (Step 019920): Train loss 2.233, Val loss 4.296\n",
      "Epoch: 9 (Step 019930): Train loss 2.426, Val loss 4.272\n",
      "Epoch: 9 (Step 019940): Train loss 2.393, Val loss 4.257\n",
      "Epoch: 9 (Step 019950): Train loss 2.918, Val loss 4.214\n",
      "Epoch: 9 (Step 019960): Train loss 2.351, Val loss 4.222\n",
      "Epoch: 9 (Step 019970): Train loss 2.923, Val loss 4.314\n",
      "Epoch: 9 (Step 019980): Train loss 2.666, Val loss 4.328\n",
      "Epoch: 9 (Step 019990): Train loss 2.721, Val loss 4.339\n",
      "Epoch: 9 (Step 020000): Train loss 2.214, Val loss 4.335\n",
      "Epoch: 9 (Step 020010): Train loss 3.341, Val loss 4.344\n",
      "Epoch: 9 (Step 020020): Train loss 2.390, Val loss 4.354\n",
      "Epoch: 9 (Step 020030): Train loss 3.080, Val loss 4.331\n",
      "Epoch: 9 (Step 020040): Train loss 2.958, Val loss 4.286\n",
      "Epoch: 9 (Step 020050): Train loss 2.692, Val loss 4.293\n",
      "Epoch: 9 (Step 020060): Train loss 3.043, Val loss 4.297\n",
      "Epoch: 9 (Step 020070): Train loss 2.506, Val loss 4.312\n",
      "Epoch: 9 (Step 020080): Train loss 2.531, Val loss 4.299\n",
      "Epoch: 9 (Step 020090): Train loss 3.138, Val loss 4.325\n",
      "Epoch: 9 (Step 020100): Train loss 2.666, Val loss 4.300\n",
      "Epoch: 9 (Step 020110): Train loss 3.242, Val loss 4.303\n",
      "Epoch: 9 (Step 020120): Train loss 2.953, Val loss 4.287\n",
      "Epoch: 9 (Step 020130): Train loss 3.052, Val loss 4.274\n",
      "Epoch: 9 (Step 020140): Train loss 2.655, Val loss 4.269\n",
      "Epoch: 9 (Step 020150): Train loss 2.808, Val loss 4.292\n",
      "Epoch: 9 (Step 020160): Train loss 3.016, Val loss 4.282\n",
      "Epoch: 9 (Step 020170): Train loss 2.756, Val loss 4.294\n",
      "Epoch: 9 (Step 020180): Train loss 2.362, Val loss 4.278\n",
      "Epoch: 9 (Step 020190): Train loss 2.810, Val loss 4.305\n",
      "Epoch: 9 (Step 020200): Train loss 2.081, Val loss 4.346\n",
      "Epoch: 9 (Step 020210): Train loss 2.827, Val loss 4.353\n",
      "Epoch: 9 (Step 020220): Train loss 2.319, Val loss 4.355\n",
      "Epoch: 9 (Step 020230): Train loss 2.791, Val loss 4.337\n",
      "Epoch: 9 (Step 020240): Train loss 2.543, Val loss 4.303\n",
      "Epoch: 9 (Step 020250): Train loss 2.534, Val loss 4.309\n",
      "Epoch: 9 (Step 020260): Train loss 2.968, Val loss 4.344\n",
      "Epoch: 9 (Step 020270): Train loss 2.865, Val loss 4.326\n",
      "Epoch: 9 (Step 020280): Train loss 2.449, Val loss 4.308\n",
      "Epoch: 9 (Step 020290): Train loss 3.082, Val loss 4.326\n",
      "Epoch: 9 (Step 020300): Train loss 2.920, Val loss 4.326\n",
      "Epoch: 9 (Step 020310): Train loss 2.888, Val loss 4.333\n",
      "Epoch: 9 (Step 020320): Train loss 3.085, Val loss 4.347\n",
      "Epoch: 9 (Step 020330): Train loss 2.212, Val loss 4.367\n",
      "Epoch: 9 (Step 020340): Train loss 2.234, Val loss 4.318\n",
      "Epoch: 9 (Step 020350): Train loss 2.591, Val loss 4.299\n",
      "Epoch: 9 (Step 020360): Train loss 2.629, Val loss 4.266\n",
      "Epoch: 9 (Step 020370): Train loss 2.408, Val loss 4.230\n",
      "Epoch: 9 (Step 020380): Train loss 2.512, Val loss 4.267\n",
      "Epoch: 9 (Step 020390): Train loss 2.761, Val loss 4.255\n",
      "Epoch: 9 (Step 020400): Train loss 2.551, Val loss 4.236\n",
      "Epoch: 9 (Step 020410): Train loss 2.567, Val loss 4.245\n",
      "Epoch: 9 (Step 020420): Train loss 2.449, Val loss 4.298\n",
      "Epoch: 9 (Step 020430): Train loss 2.861, Val loss 4.305\n",
      "Epoch: 9 (Step 020440): Train loss 2.571, Val loss 4.360\n",
      "Epoch: 9 (Step 020450): Train loss 2.697, Val loss 4.291\n",
      "Epoch: 9 (Step 020460): Train loss 2.024, Val loss 4.266\n",
      "Epoch: 9 (Step 020470): Train loss 2.592, Val loss 4.274\n",
      "Epoch: 9 (Step 020480): Train loss 2.485, Val loss 4.291\n",
      "Epoch: 9 (Step 020490): Train loss 2.829, Val loss 4.302\n",
      "Epoch: 9 (Step 020500): Train loss 2.963, Val loss 4.259\n",
      "Epoch: 9 (Step 020510): Train loss 3.346, Val loss 4.233\n",
      "Epoch: 9 (Step 020520): Train loss 2.866, Val loss 4.198\n",
      "Epoch: 9 (Step 020530): Train loss 2.357, Val loss 4.154\n",
      "Epoch: 9 (Step 020540): Train loss 2.284, Val loss 4.112\n",
      "Epoch: 9 (Step 020550): Train loss 2.641, Val loss 4.093\n",
      "Epoch: 9 (Step 020560): Train loss 2.881, Val loss 4.087\n",
      "Epoch: 9 (Step 020570): Train loss 2.433, Val loss 4.115\n",
      "Epoch: 9 (Step 020580): Train loss 2.167, Val loss 4.104\n",
      "Epoch: 9 (Step 020590): Train loss 2.503, Val loss 4.074\n",
      "Epoch: 9 (Step 020600): Train loss 2.261, Val loss 4.060\n",
      "Epoch: 9 (Step 020610): Train loss 2.559, Val loss 4.054\n",
      "Epoch: 9 (Step 020620): Train loss 2.401, Val loss 4.090\n",
      "Epoch: 9 (Step 020630): Train loss 2.892, Val loss 4.100\n",
      "Epoch: 9 (Step 020640): Train loss 2.536, Val loss 4.104\n",
      "Epoch: 9 (Step 020650): Train loss 2.783, Val loss 4.085\n",
      "Epoch: 9 (Step 020660): Train loss 2.716, Val loss 4.096\n",
      "Epoch: 9 (Step 020670): Train loss 2.253, Val loss 4.085\n",
      "Epoch: 9 (Step 020680): Train loss 2.778, Val loss 4.050\n",
      "Epoch: 9 (Step 020690): Train loss 2.482, Val loss 4.105\n",
      "Epoch: 9 (Step 020700): Train loss 2.405, Val loss 4.085\n",
      "Epoch: 9 (Step 020710): Train loss 2.175, Val loss 4.059\n",
      "Epoch: 9 (Step 020720): Train loss 2.467, Val loss 4.030\n",
      "Epoch: 9 (Step 020730): Train loss 2.134, Val loss 4.027\n",
      "Epoch: 9 (Step 020740): Train loss 2.678, Val loss 4.042\n",
      "Epoch: 9 (Step 020750): Train loss 2.586, Val loss 4.079\n",
      "Epoch: 9 (Step 020760): Train loss 2.991, Val loss 4.070\n",
      "Epoch: 9 (Step 020770): Train loss 2.639, Val loss 4.093\n",
      "Epoch: 9 (Step 020780): Train loss 2.722, Val loss 4.088\n",
      "Epoch: 9 (Step 020790): Train loss 2.729, Val loss 4.047\n",
      "Epoch: 9 (Step 020800): Train loss 3.014, Val loss 4.071\n",
      "Epoch: 9 (Step 020810): Train loss 2.539, Val loss 4.014\n",
      "Epoch: 9 (Step 020820): Train loss 2.607, Val loss 4.045\n",
      "Epoch: 9 (Step 020830): Train loss 2.406, Val loss 4.057\n",
      "Epoch: 9 (Step 020840): Train loss 2.785, Val loss 4.056\n",
      "Epoch: 9 (Step 020850): Train loss 1.868, Val loss 4.100\n",
      "Epoch: 9 (Step 020860): Train loss 2.399, Val loss 4.135\n",
      "Epoch: 9 (Step 020870): Train loss 2.712, Val loss 4.068\n",
      "Epoch: 9 (Step 020880): Train loss 2.318, Val loss 4.096\n",
      "Epoch: 9 (Step 020890): Train loss 2.698, Val loss 4.094\n",
      "Epoch: 9 (Step 020900): Train loss 2.706, Val loss 4.078\n",
      "Epoch: 9 (Step 020910): Train loss 2.144, Val loss 4.069\n",
      "Epoch: 9 (Step 020920): Train loss 2.387, Val loss 4.100\n",
      "Epoch: 9 (Step 020930): Train loss 2.517, Val loss 4.111\n",
      "Epoch: 9 (Step 020940): Train loss 2.708, Val loss 4.098\n",
      "Epoch: 9 (Step 020950): Train loss 2.535, Val loss 4.068\n",
      "Epoch: 9 (Step 020960): Train loss 2.566, Val loss 4.083\n",
      "Epoch: 9 (Step 020970): Train loss 2.386, Val loss 4.099\n",
      "Epoch: 9 (Step 020980): Train loss 2.623, Val loss 4.159\n",
      "Epoch: 9 (Step 020990): Train loss 2.502, Val loss 4.114\n",
      "Epoch: 9 (Step 021000): Train loss 2.665, Val loss 4.088\n",
      "Epoch: 9 (Step 021010): Train loss 2.525, Val loss 4.071\n",
      "Epoch: 9 (Step 021020): Train loss 2.882, Val loss 4.052\n",
      "Epoch: 9 (Step 021030): Train loss 2.229, Val loss 4.053\n",
      "Epoch: 9 (Step 021040): Train loss 2.752, Val loss 4.034\n",
      "Epoch: 9 (Step 021050): Train loss 2.659, Val loss 4.070\n",
      "Epoch: 9 (Step 021060): Train loss 2.943, Val loss 4.067\n",
      "Epoch: 9 (Step 021070): Train loss 2.316, Val loss 4.100\n",
      "Epoch: 9 (Step 021080): Train loss 2.510, Val loss 4.144\n",
      "Epoch: 9 (Step 021090): Train loss 2.578, Val loss 4.204\n",
      "Epoch: 9 (Step 021100): Train loss 2.483, Val loss 4.176\n",
      "Epoch: 9 (Step 021110): Train loss 2.719, Val loss 4.132\n",
      "Epoch: 9 (Step 021120): Train loss 2.607, Val loss 4.136\n",
      "Epoch: 9 (Step 021130): Train loss 2.438, Val loss 4.099\n",
      "Epoch: 9 (Step 021140): Train loss 2.887, Val loss 4.101\n",
      "Epoch: 9 (Step 021150): Train loss 2.323, Val loss 4.114\n",
      "Epoch: 9 (Step 021160): Train loss 2.187, Val loss 4.142\n",
      "Epoch: 9 (Step 021170): Train loss 2.426, Val loss 4.164\n",
      "Epoch: 9 (Step 021180): Train loss 2.717, Val loss 4.132\n",
      "Epoch: 9 (Step 021190): Train loss 2.589, Val loss 4.119\n",
      "Epoch: 9 (Step 021200): Train loss 2.056, Val loss 4.102\n",
      "Epoch: 9 (Step 021210): Train loss 2.610, Val loss 4.108\n",
      "Epoch: 9 (Step 021220): Train loss 2.515, Val loss 4.143\n",
      "Epoch: 9 (Step 021230): Train loss 2.894, Val loss 4.097\n",
      "Epoch: 10 (Step 021240): Train loss 2.842, Val loss 4.139\n",
      "Epoch: 10 (Step 021250): Train loss 2.392, Val loss 4.185\n",
      "Epoch: 10 (Step 021260): Train loss 3.297, Val loss 4.204\n",
      "Epoch: 10 (Step 021270): Train loss 2.472, Val loss 4.235\n",
      "Epoch: 10 (Step 021280): Train loss 2.625, Val loss 4.208\n",
      "Epoch: 10 (Step 021290): Train loss 2.647, Val loss 4.209\n",
      "Epoch: 10 (Step 021300): Train loss 2.665, Val loss 4.264\n",
      "Epoch: 10 (Step 021310): Train loss 2.182, Val loss 4.257\n",
      "Epoch: 10 (Step 021320): Train loss 2.542, Val loss 4.207\n",
      "Epoch: 10 (Step 021330): Train loss 2.483, Val loss 4.207\n",
      "Epoch: 10 (Step 021340): Train loss 2.026, Val loss 4.239\n",
      "Epoch: 10 (Step 021350): Train loss 2.270, Val loss 4.208\n",
      "Epoch: 10 (Step 021360): Train loss 2.316, Val loss 4.232\n",
      "Epoch: 10 (Step 021370): Train loss 2.312, Val loss 4.232\n",
      "Epoch: 10 (Step 021380): Train loss 2.856, Val loss 4.261\n",
      "Epoch: 10 (Step 021390): Train loss 2.350, Val loss 4.263\n",
      "Epoch: 10 (Step 021400): Train loss 2.696, Val loss 4.227\n",
      "Epoch: 10 (Step 021410): Train loss 2.322, Val loss 4.221\n",
      "Epoch: 10 (Step 021420): Train loss 2.145, Val loss 4.218\n",
      "Epoch: 10 (Step 021430): Train loss 2.893, Val loss 4.232\n",
      "Epoch: 10 (Step 021440): Train loss 2.279, Val loss 4.257\n",
      "Epoch: 10 (Step 021450): Train loss 2.182, Val loss 4.224\n",
      "Epoch: 10 (Step 021460): Train loss 2.401, Val loss 4.236\n",
      "Epoch: 10 (Step 021470): Train loss 2.237, Val loss 4.214\n",
      "Epoch: 10 (Step 021480): Train loss 2.278, Val loss 4.221\n",
      "Epoch: 10 (Step 021490): Train loss 2.882, Val loss 4.258\n",
      "Epoch: 10 (Step 021500): Train loss 2.538, Val loss 4.281\n",
      "Epoch: 10 (Step 021510): Train loss 1.717, Val loss 4.255\n",
      "Epoch: 10 (Step 021520): Train loss 2.740, Val loss 4.285\n",
      "Epoch: 10 (Step 021530): Train loss 2.403, Val loss 4.255\n",
      "Epoch: 10 (Step 021540): Train loss 2.300, Val loss 4.254\n",
      "Epoch: 10 (Step 021550): Train loss 2.201, Val loss 4.286\n",
      "Epoch: 10 (Step 021560): Train loss 2.311, Val loss 4.332\n",
      "Epoch: 10 (Step 021570): Train loss 2.494, Val loss 4.320\n",
      "Epoch: 10 (Step 021580): Train loss 2.674, Val loss 4.314\n",
      "Epoch: 10 (Step 021590): Train loss 2.448, Val loss 4.294\n",
      "Epoch: 10 (Step 021600): Train loss 2.392, Val loss 4.294\n",
      "Epoch: 10 (Step 021610): Train loss 2.467, Val loss 4.254\n",
      "Epoch: 10 (Step 021620): Train loss 2.615, Val loss 4.171\n",
      "Epoch: 10 (Step 021630): Train loss 2.250, Val loss 4.216\n",
      "Epoch: 10 (Step 021640): Train loss 2.253, Val loss 4.276\n",
      "Epoch: 10 (Step 021650): Train loss 2.547, Val loss 4.342\n",
      "Epoch: 10 (Step 021660): Train loss 2.199, Val loss 4.376\n",
      "Epoch: 10 (Step 021670): Train loss 2.013, Val loss 4.380\n",
      "Epoch: 10 (Step 021680): Train loss 1.718, Val loss 4.380\n",
      "Epoch: 10 (Step 021690): Train loss 2.216, Val loss 4.387\n",
      "Epoch: 10 (Step 021700): Train loss 2.299, Val loss 4.386\n",
      "Epoch: 10 (Step 021710): Train loss 1.772, Val loss 4.317\n",
      "Epoch: 10 (Step 021720): Train loss 2.417, Val loss 4.394\n",
      "Epoch: 10 (Step 021730): Train loss 2.373, Val loss 4.400\n",
      "Epoch: 10 (Step 021740): Train loss 3.114, Val loss 4.390\n",
      "Epoch: 10 (Step 021750): Train loss 2.508, Val loss 4.339\n",
      "Epoch: 10 (Step 021760): Train loss 2.593, Val loss 4.353\n",
      "Epoch: 10 (Step 021770): Train loss 2.375, Val loss 4.362\n",
      "Epoch: 10 (Step 021780): Train loss 2.970, Val loss 4.378\n",
      "Epoch: 10 (Step 021790): Train loss 2.458, Val loss 4.349\n",
      "Epoch: 10 (Step 021800): Train loss 2.584, Val loss 4.335\n",
      "Epoch: 10 (Step 021810): Train loss 2.301, Val loss 4.331\n",
      "Epoch: 10 (Step 021820): Train loss 2.013, Val loss 4.374\n",
      "Epoch: 10 (Step 021830): Train loss 2.235, Val loss 4.413\n",
      "Epoch: 10 (Step 021840): Train loss 2.226, Val loss 4.325\n",
      "Epoch: 10 (Step 021850): Train loss 2.629, Val loss 4.301\n",
      "Epoch: 10 (Step 021860): Train loss 2.910, Val loss 4.320\n",
      "Epoch: 10 (Step 021870): Train loss 2.752, Val loss 4.268\n",
      "Epoch: 10 (Step 021880): Train loss 2.687, Val loss 4.237\n",
      "Epoch: 10 (Step 021890): Train loss 1.862, Val loss 4.225\n",
      "Epoch: 10 (Step 021900): Train loss 1.999, Val loss 4.251\n",
      "Epoch: 10 (Step 021910): Train loss 2.057, Val loss 4.246\n",
      "Epoch: 10 (Step 021920): Train loss 2.063, Val loss 4.238\n",
      "Epoch: 10 (Step 021930): Train loss 2.764, Val loss 4.249\n",
      "Epoch: 10 (Step 021940): Train loss 3.112, Val loss 4.265\n",
      "Epoch: 10 (Step 021950): Train loss 2.201, Val loss 4.282\n",
      "Epoch: 10 (Step 021960): Train loss 2.919, Val loss 4.252\n",
      "Epoch: 10 (Step 021970): Train loss 2.261, Val loss 4.275\n",
      "Epoch: 10 (Step 021980): Train loss 2.515, Val loss 4.306\n",
      "Epoch: 10 (Step 021990): Train loss 2.627, Val loss 4.267\n",
      "Epoch: 10 (Step 022000): Train loss 2.553, Val loss 4.314\n",
      "Epoch: 10 (Step 022010): Train loss 2.377, Val loss 4.344\n",
      "Epoch: 10 (Step 022020): Train loss 2.652, Val loss 4.324\n",
      "Epoch: 10 (Step 022030): Train loss 2.180, Val loss 4.365\n",
      "Epoch: 10 (Step 022040): Train loss 2.262, Val loss 4.320\n",
      "Epoch: 10 (Step 022050): Train loss 1.633, Val loss 4.368\n",
      "Epoch: 10 (Step 022060): Train loss 2.655, Val loss 4.394\n",
      "Epoch: 10 (Step 022070): Train loss 2.077, Val loss 4.359\n",
      "Epoch: 10 (Step 022080): Train loss 2.470, Val loss 4.319\n",
      "Epoch: 10 (Step 022090): Train loss 2.505, Val loss 4.351\n",
      "Epoch: 10 (Step 022100): Train loss 2.388, Val loss 4.328\n",
      "Epoch: 10 (Step 022110): Train loss 2.664, Val loss 4.355\n",
      "Epoch: 10 (Step 022120): Train loss 2.410, Val loss 4.391\n",
      "Epoch: 10 (Step 022130): Train loss 2.465, Val loss 4.383\n",
      "Epoch: 10 (Step 022140): Train loss 2.650, Val loss 4.388\n",
      "Epoch: 10 (Step 022150): Train loss 2.419, Val loss 4.344\n",
      "Epoch: 10 (Step 022160): Train loss 2.479, Val loss 4.319\n",
      "Epoch: 10 (Step 022170): Train loss 2.517, Val loss 4.353\n",
      "Epoch: 10 (Step 022180): Train loss 2.538, Val loss 4.346\n",
      "Epoch: 10 (Step 022190): Train loss 2.373, Val loss 4.287\n",
      "Epoch: 10 (Step 022200): Train loss 2.920, Val loss 4.255\n",
      "Epoch: 10 (Step 022210): Train loss 2.459, Val loss 4.306\n",
      "Epoch: 10 (Step 022220): Train loss 2.162, Val loss 4.326\n",
      "Epoch: 10 (Step 022230): Train loss 2.649, Val loss 4.316\n",
      "Epoch: 10 (Step 022240): Train loss 2.316, Val loss 4.281\n",
      "Epoch: 10 (Step 022250): Train loss 2.583, Val loss 4.238\n",
      "Epoch: 10 (Step 022260): Train loss 2.078, Val loss 4.262\n",
      "Epoch: 10 (Step 022270): Train loss 2.838, Val loss 4.299\n",
      "Epoch: 10 (Step 022280): Train loss 2.710, Val loss 4.328\n",
      "Epoch: 10 (Step 022290): Train loss 2.604, Val loss 4.357\n",
      "Epoch: 10 (Step 022300): Train loss 2.511, Val loss 4.325\n",
      "Epoch: 10 (Step 022310): Train loss 2.743, Val loss 4.349\n",
      "Epoch: 10 (Step 022320): Train loss 2.001, Val loss 4.368\n",
      "Epoch: 10 (Step 022330): Train loss 2.515, Val loss 4.321\n",
      "Epoch: 10 (Step 022340): Train loss 2.527, Val loss 4.320\n",
      "Epoch: 10 (Step 022350): Train loss 2.183, Val loss 4.298\n",
      "Epoch: 10 (Step 022360): Train loss 2.263, Val loss 4.344\n",
      "Epoch: 10 (Step 022370): Train loss 2.092, Val loss 4.394\n",
      "Epoch: 10 (Step 022380): Train loss 2.058, Val loss 4.392\n",
      "Epoch: 10 (Step 022390): Train loss 2.696, Val loss 4.347\n",
      "Epoch: 10 (Step 022400): Train loss 1.983, Val loss 4.325\n",
      "Epoch: 10 (Step 022410): Train loss 1.921, Val loss 4.368\n",
      "Epoch: 10 (Step 022420): Train loss 2.208, Val loss 4.367\n",
      "Epoch: 10 (Step 022430): Train loss 2.301, Val loss 4.329\n",
      "Epoch: 10 (Step 022440): Train loss 2.189, Val loss 4.333\n",
      "Epoch: 10 (Step 022450): Train loss 2.177, Val loss 4.364\n",
      "Epoch: 10 (Step 022460): Train loss 2.107, Val loss 4.337\n",
      "Epoch: 10 (Step 022470): Train loss 3.172, Val loss 4.364\n",
      "Epoch: 10 (Step 022480): Train loss 2.085, Val loss 4.403\n",
      "Epoch: 10 (Step 022490): Train loss 2.780, Val loss 4.401\n",
      "Epoch: 10 (Step 022500): Train loss 2.203, Val loss 4.413\n",
      "Epoch: 10 (Step 022510): Train loss 2.365, Val loss 4.350\n",
      "Epoch: 10 (Step 022520): Train loss 2.052, Val loss 4.282\n",
      "Epoch: 10 (Step 022530): Train loss 2.618, Val loss 4.342\n",
      "Epoch: 10 (Step 022540): Train loss 2.022, Val loss 4.347\n",
      "Epoch: 10 (Step 022550): Train loss 2.220, Val loss 4.356\n",
      "Epoch: 10 (Step 022560): Train loss 2.254, Val loss 4.355\n",
      "Epoch: 10 (Step 022570): Train loss 1.999, Val loss 4.329\n",
      "Epoch: 10 (Step 022580): Train loss 2.216, Val loss 4.268\n",
      "Epoch: 10 (Step 022590): Train loss 1.991, Val loss 4.321\n",
      "Epoch: 10 (Step 022600): Train loss 2.808, Val loss 4.289\n",
      "Epoch: 10 (Step 022610): Train loss 2.252, Val loss 4.261\n",
      "Epoch: 10 (Step 022620): Train loss 2.690, Val loss 4.324\n",
      "Epoch: 10 (Step 022630): Train loss 1.759, Val loss 4.262\n",
      "Epoch: 10 (Step 022640): Train loss 2.604, Val loss 4.218\n",
      "Epoch: 10 (Step 022650): Train loss 3.047, Val loss 4.244\n",
      "Epoch: 10 (Step 022660): Train loss 2.440, Val loss 4.242\n",
      "Epoch: 10 (Step 022670): Train loss 2.202, Val loss 4.271\n",
      "Epoch: 10 (Step 022680): Train loss 2.647, Val loss 4.249\n",
      "Epoch: 10 (Step 022690): Train loss 2.100, Val loss 4.215\n",
      "Epoch: 10 (Step 022700): Train loss 3.078, Val loss 4.236\n",
      "Epoch: 10 (Step 022710): Train loss 2.203, Val loss 4.218\n",
      "Epoch: 10 (Step 022720): Train loss 2.080, Val loss 4.244\n",
      "Epoch: 10 (Step 022730): Train loss 2.193, Val loss 4.257\n",
      "Epoch: 10 (Step 022740): Train loss 2.332, Val loss 4.279\n",
      "Epoch: 10 (Step 022750): Train loss 1.806, Val loss 4.262\n",
      "Epoch: 10 (Step 022760): Train loss 2.188, Val loss 4.269\n",
      "Epoch: 10 (Step 022770): Train loss 1.795, Val loss 4.280\n",
      "Epoch: 10 (Step 022780): Train loss 2.214, Val loss 4.341\n",
      "Epoch: 10 (Step 022790): Train loss 1.989, Val loss 4.344\n",
      "Epoch: 10 (Step 022800): Train loss 2.134, Val loss 4.336\n",
      "Epoch: 10 (Step 022810): Train loss 2.265, Val loss 4.356\n",
      "Epoch: 10 (Step 022820): Train loss 2.402, Val loss 4.328\n",
      "Epoch: 10 (Step 022830): Train loss 1.980, Val loss 4.254\n",
      "Epoch: 10 (Step 022840): Train loss 1.957, Val loss 4.316\n",
      "Epoch: 10 (Step 022850): Train loss 2.189, Val loss 4.247\n",
      "Epoch: 10 (Step 022860): Train loss 2.216, Val loss 4.248\n",
      "Epoch: 10 (Step 022870): Train loss 2.503, Val loss 4.263\n",
      "Epoch: 10 (Step 022880): Train loss 2.709, Val loss 4.252\n",
      "Epoch: 10 (Step 022890): Train loss 2.818, Val loss 4.262\n",
      "Epoch: 10 (Step 022900): Train loss 2.681, Val loss 4.230\n",
      "Epoch: 10 (Step 022910): Train loss 2.756, Val loss 4.244\n",
      "Epoch: 10 (Step 022920): Train loss 2.013, Val loss 4.230\n",
      "Epoch: 10 (Step 022930): Train loss 1.985, Val loss 4.242\n",
      "Epoch: 10 (Step 022940): Train loss 2.241, Val loss 4.299\n",
      "Epoch: 10 (Step 022950): Train loss 2.684, Val loss 4.271\n",
      "Epoch: 10 (Step 022960): Train loss 2.953, Val loss 4.262\n",
      "Epoch: 10 (Step 022970): Train loss 2.499, Val loss 4.226\n",
      "Epoch: 10 (Step 022980): Train loss 2.426, Val loss 4.244\n",
      "Epoch: 10 (Step 022990): Train loss 2.408, Val loss 4.233\n",
      "Epoch: 10 (Step 023000): Train loss 2.124, Val loss 4.287\n",
      "Epoch: 10 (Step 023010): Train loss 2.275, Val loss 4.290\n",
      "Epoch: 10 (Step 023020): Train loss 2.084, Val loss 4.263\n",
      "Epoch: 10 (Step 023030): Train loss 2.122, Val loss 4.211\n",
      "Epoch: 10 (Step 023040): Train loss 2.137, Val loss 4.259\n",
      "Epoch: 10 (Step 023050): Train loss 3.163, Val loss 4.299\n",
      "Epoch: 10 (Step 023060): Train loss 2.256, Val loss 4.248\n",
      "Epoch: 10 (Step 023070): Train loss 2.231, Val loss 4.241\n",
      "Epoch: 10 (Step 023080): Train loss 2.524, Val loss 4.315\n",
      "Epoch: 10 (Step 023090): Train loss 2.487, Val loss 4.337\n",
      "Epoch: 10 (Step 023100): Train loss 2.352, Val loss 4.311\n",
      "Epoch: 10 (Step 023110): Train loss 1.848, Val loss 4.277\n",
      "Epoch: 10 (Step 023120): Train loss 2.010, Val loss 4.220\n",
      "Epoch: 10 (Step 023130): Train loss 2.368, Val loss 4.266\n",
      "Epoch: 10 (Step 023140): Train loss 2.094, Val loss 4.294\n",
      "Epoch: 10 (Step 023150): Train loss 2.125, Val loss 4.270\n",
      "Epoch: 10 (Step 023160): Train loss 1.839, Val loss 4.247\n",
      "Epoch: 10 (Step 023170): Train loss 2.742, Val loss 4.265\n",
      "Epoch: 10 (Step 023180): Train loss 2.319, Val loss 4.274\n",
      "Epoch: 10 (Step 023190): Train loss 2.222, Val loss 4.231\n",
      "Epoch: 10 (Step 023200): Train loss 2.833, Val loss 4.311\n",
      "Epoch: 10 (Step 023210): Train loss 2.609, Val loss 4.319\n",
      "Epoch: 10 (Step 023220): Train loss 2.804, Val loss 4.279\n",
      "Epoch: 10 (Step 023230): Train loss 2.487, Val loss 4.230\n",
      "Epoch: 10 (Step 023240): Train loss 2.465, Val loss 4.233\n",
      "Epoch: 10 (Step 023250): Train loss 2.520, Val loss 4.244\n",
      "Epoch: 10 (Step 023260): Train loss 2.045, Val loss 4.257\n",
      "Epoch: 10 (Step 023270): Train loss 2.150, Val loss 4.236\n",
      "Epoch: 10 (Step 023280): Train loss 2.212, Val loss 4.261\n",
      "Epoch: 10 (Step 023290): Train loss 2.160, Val loss 4.273\n",
      "Epoch: 10 (Step 023300): Train loss 1.844, Val loss 4.270\n",
      "Epoch: 10 (Step 023310): Train loss 2.856, Val loss 4.241\n",
      "Epoch: 10 (Step 023320): Train loss 2.334, Val loss 4.261\n",
      "Epoch: 10 (Step 023330): Train loss 2.437, Val loss 4.269\n",
      "Epoch: 10 (Step 023340): Train loss 1.958, Val loss 4.314\n",
      "Epoch: 10 (Step 023350): Train loss 2.174, Val loss 4.286\n",
      "Epoch: 10 (Step 023360): Train loss 2.148, Val loss 4.278\n",
      "Epoch: 10 (Step 023370): Train loss 2.261, Val loss 4.263\n",
      "Epoch: 10 (Step 023380): Train loss 2.581, Val loss 4.252\n",
      "Epoch: 10 (Step 023390): Train loss 1.979, Val loss 4.240\n",
      "Epoch: 10 (Step 023400): Train loss 2.099, Val loss 4.265\n",
      "Epoch: 10 (Step 023410): Train loss 2.249, Val loss 4.295\n",
      "Epoch: 10 (Step 023420): Train loss 2.199, Val loss 4.313\n",
      "Epoch: 10 (Step 023430): Train loss 2.126, Val loss 4.300\n",
      "Epoch: 10 (Step 023440): Train loss 2.186, Val loss 4.244\n",
      "Epoch: 10 (Step 023450): Train loss 2.449, Val loss 4.264\n",
      "Epoch: 10 (Step 023460): Train loss 2.235, Val loss 4.252\n",
      "Epoch: 10 (Step 023470): Train loss 2.562, Val loss 4.212\n",
      "Epoch: 10 (Step 023480): Train loss 1.937, Val loss 4.193\n",
      "Epoch: 10 (Step 023490): Train loss 2.305, Val loss 4.198\n",
      "Epoch: 10 (Step 023500): Train loss 2.009, Val loss 4.217\n",
      "Epoch: 10 (Step 023510): Train loss 2.282, Val loss 4.225\n",
      "Epoch: 10 (Step 023520): Train loss 2.270, Val loss 4.184\n",
      "Epoch: 10 (Step 023530): Train loss 1.933, Val loss 4.202\n",
      "Epoch: 10 (Step 023540): Train loss 2.382, Val loss 4.219\n",
      "Epoch: 10 (Step 023550): Train loss 1.991, Val loss 4.182\n",
      "Epoch: 10 (Step 023560): Train loss 2.341, Val loss 4.192\n",
      "Epoch: 10 (Step 023570): Train loss 1.918, Val loss 4.208\n",
      "Epoch: 10 (Step 023580): Train loss 2.579, Val loss 4.201\n",
      "Epoch: 11 (Step 023590): Train loss 2.580, Val loss 4.208\n",
      "Epoch: 11 (Step 023600): Train loss 2.231, Val loss 4.186\n",
      "Epoch: 11 (Step 023610): Train loss 2.615, Val loss 4.151\n",
      "Epoch: 11 (Step 023620): Train loss 2.178, Val loss 4.188\n",
      "Epoch: 11 (Step 023630): Train loss 2.123, Val loss 4.238\n",
      "Epoch: 11 (Step 023640): Train loss 1.925, Val loss 4.271\n",
      "Epoch: 11 (Step 023650): Train loss 2.172, Val loss 4.271\n",
      "Epoch: 11 (Step 023660): Train loss 2.281, Val loss 4.263\n",
      "Epoch: 11 (Step 023670): Train loss 2.175, Val loss 4.306\n",
      "Epoch: 11 (Step 023680): Train loss 2.288, Val loss 4.294\n",
      "Epoch: 11 (Step 023690): Train loss 2.275, Val loss 4.292\n",
      "Epoch: 11 (Step 023700): Train loss 2.469, Val loss 4.333\n",
      "Epoch: 11 (Step 023710): Train loss 2.258, Val loss 4.420\n",
      "Epoch: 11 (Step 023720): Train loss 2.253, Val loss 4.404\n",
      "Epoch: 11 (Step 023730): Train loss 2.049, Val loss 4.404\n",
      "Epoch: 11 (Step 023740): Train loss 2.081, Val loss 4.392\n",
      "Epoch: 11 (Step 023750): Train loss 2.148, Val loss 4.395\n",
      "Epoch: 11 (Step 023760): Train loss 2.051, Val loss 4.368\n",
      "Epoch: 11 (Step 023770): Train loss 1.870, Val loss 4.394\n",
      "Epoch: 11 (Step 023780): Train loss 2.485, Val loss 4.387\n",
      "Epoch: 11 (Step 023790): Train loss 2.125, Val loss 4.336\n",
      "Epoch: 11 (Step 023800): Train loss 2.003, Val loss 4.286\n",
      "Epoch: 11 (Step 023810): Train loss 1.651, Val loss 4.291\n",
      "Epoch: 11 (Step 023820): Train loss 2.103, Val loss 4.313\n",
      "Epoch: 11 (Step 023830): Train loss 1.926, Val loss 4.290\n",
      "Epoch: 11 (Step 023840): Train loss 1.919, Val loss 4.297\n",
      "Epoch: 11 (Step 023850): Train loss 2.481, Val loss 4.305\n",
      "Epoch: 11 (Step 023860): Train loss 2.201, Val loss 4.321\n",
      "Epoch: 11 (Step 023870): Train loss 2.004, Val loss 4.348\n",
      "Epoch: 11 (Step 023880): Train loss 2.323, Val loss 4.301\n",
      "Epoch: 11 (Step 023890): Train loss 2.593, Val loss 4.362\n",
      "Epoch: 11 (Step 023900): Train loss 2.034, Val loss 4.318\n",
      "Epoch: 11 (Step 023910): Train loss 2.261, Val loss 4.320\n",
      "Epoch: 11 (Step 023920): Train loss 2.262, Val loss 4.337\n",
      "Epoch: 11 (Step 023930): Train loss 2.130, Val loss 4.300\n",
      "Epoch: 11 (Step 023940): Train loss 1.989, Val loss 4.244\n",
      "Epoch: 11 (Step 023950): Train loss 2.179, Val loss 4.303\n",
      "Epoch: 11 (Step 023960): Train loss 2.488, Val loss 4.333\n",
      "Epoch: 11 (Step 023970): Train loss 1.999, Val loss 4.326\n",
      "Epoch: 11 (Step 023980): Train loss 2.430, Val loss 4.355\n",
      "Epoch: 11 (Step 023990): Train loss 2.594, Val loss 4.385\n",
      "Epoch: 11 (Step 024000): Train loss 1.790, Val loss 4.410\n",
      "Epoch: 11 (Step 024010): Train loss 2.393, Val loss 4.432\n",
      "Epoch: 11 (Step 024020): Train loss 2.209, Val loss 4.388\n",
      "Epoch: 11 (Step 024030): Train loss 1.654, Val loss 4.353\n",
      "Epoch: 11 (Step 024040): Train loss 1.734, Val loss 4.365\n",
      "Epoch: 11 (Step 024050): Train loss 1.681, Val loss 4.422\n",
      "Epoch: 11 (Step 024060): Train loss 2.227, Val loss 4.409\n",
      "Epoch: 11 (Step 024070): Train loss 2.182, Val loss 4.415\n",
      "Epoch: 11 (Step 024080): Train loss 2.060, Val loss 4.378\n",
      "Epoch: 11 (Step 024090): Train loss 1.882, Val loss 4.364\n",
      "Epoch: 11 (Step 024100): Train loss 1.771, Val loss 4.367\n",
      "Epoch: 11 (Step 024110): Train loss 2.113, Val loss 4.359\n",
      "Epoch: 11 (Step 024120): Train loss 2.068, Val loss 4.380\n",
      "Epoch: 11 (Step 024130): Train loss 2.449, Val loss 4.422\n",
      "Epoch: 11 (Step 024140): Train loss 2.242, Val loss 4.423\n",
      "Epoch: 11 (Step 024150): Train loss 2.739, Val loss 4.391\n",
      "Epoch: 11 (Step 024160): Train loss 2.356, Val loss 4.370\n",
      "Epoch: 11 (Step 024170): Train loss 2.528, Val loss 4.362\n",
      "Epoch: 11 (Step 024180): Train loss 2.361, Val loss 4.459\n",
      "Epoch: 11 (Step 024190): Train loss 1.810, Val loss 4.515\n",
      "Epoch: 11 (Step 024200): Train loss 2.049, Val loss 4.444\n",
      "Epoch: 11 (Step 024210): Train loss 2.356, Val loss 4.472\n",
      "Epoch: 11 (Step 024220): Train loss 2.079, Val loss 4.441\n",
      "Epoch: 11 (Step 024230): Train loss 2.383, Val loss 4.439\n",
      "Epoch: 11 (Step 024240): Train loss 1.981, Val loss 4.439\n",
      "Epoch: 11 (Step 024250): Train loss 2.424, Val loss 4.442\n",
      "Epoch: 11 (Step 024260): Train loss 2.499, Val loss 4.471\n",
      "Epoch: 11 (Step 024270): Train loss 2.280, Val loss 4.429\n",
      "Epoch: 11 (Step 024280): Train loss 2.047, Val loss 4.357\n",
      "Epoch: 11 (Step 024290): Train loss 2.366, Val loss 4.445\n",
      "Epoch: 11 (Step 024300): Train loss 1.922, Val loss 4.433\n",
      "Epoch: 11 (Step 024310): Train loss 1.798, Val loss 4.475\n",
      "Epoch: 11 (Step 024320): Train loss 2.304, Val loss 4.520\n",
      "Epoch: 11 (Step 024330): Train loss 2.493, Val loss 4.495\n",
      "Epoch: 11 (Step 024340): Train loss 1.872, Val loss 4.497\n",
      "Epoch: 11 (Step 024350): Train loss 2.265, Val loss 4.431\n",
      "Epoch: 11 (Step 024360): Train loss 1.802, Val loss 4.439\n",
      "Epoch: 11 (Step 024370): Train loss 2.801, Val loss 4.496\n",
      "Epoch: 11 (Step 024380): Train loss 2.582, Val loss 4.529\n",
      "Epoch: 11 (Step 024390): Train loss 2.128, Val loss 4.515\n",
      "Epoch: 11 (Step 024400): Train loss 1.689, Val loss 4.465\n",
      "Epoch: 11 (Step 024410): Train loss 2.331, Val loss 4.459\n",
      "Epoch: 11 (Step 024420): Train loss 1.724, Val loss 4.429\n",
      "Epoch: 11 (Step 024430): Train loss 1.587, Val loss 4.537\n",
      "Epoch: 11 (Step 024440): Train loss 1.849, Val loss 4.561\n",
      "Epoch: 11 (Step 024450): Train loss 2.246, Val loss 4.566\n",
      "Epoch: 11 (Step 024460): Train loss 2.438, Val loss 4.510\n",
      "Epoch: 11 (Step 024470): Train loss 2.137, Val loss 4.498\n",
      "Epoch: 11 (Step 024480): Train loss 2.041, Val loss 4.511\n",
      "Epoch: 11 (Step 024490): Train loss 1.682, Val loss 4.497\n",
      "Epoch: 11 (Step 024500): Train loss 1.894, Val loss 4.520\n",
      "Epoch: 11 (Step 024510): Train loss 1.924, Val loss 4.500\n",
      "Epoch: 11 (Step 024520): Train loss 2.010, Val loss 4.483\n",
      "Epoch: 11 (Step 024530): Train loss 1.758, Val loss 4.444\n",
      "Epoch: 11 (Step 024540): Train loss 1.850, Val loss 4.406\n",
      "Epoch: 11 (Step 024550): Train loss 2.432, Val loss 4.393\n",
      "Epoch: 11 (Step 024560): Train loss 1.809, Val loss 4.424\n",
      "Epoch: 11 (Step 024570): Train loss 2.580, Val loss 4.429\n",
      "Epoch: 11 (Step 024580): Train loss 2.491, Val loss 4.467\n",
      "Epoch: 11 (Step 024590): Train loss 2.521, Val loss 4.471\n",
      "Epoch: 11 (Step 024600): Train loss 2.305, Val loss 4.452\n",
      "Epoch: 11 (Step 024610): Train loss 1.990, Val loss 4.508\n",
      "Epoch: 11 (Step 024620): Train loss 2.072, Val loss 4.526\n",
      "Epoch: 11 (Step 024630): Train loss 2.118, Val loss 4.475\n",
      "Epoch: 11 (Step 024640): Train loss 2.282, Val loss 4.483\n",
      "Epoch: 11 (Step 024650): Train loss 1.803, Val loss 4.477\n",
      "Epoch: 11 (Step 024660): Train loss 2.768, Val loss 4.481\n",
      "Epoch: 11 (Step 024670): Train loss 1.746, Val loss 4.453\n",
      "Epoch: 11 (Step 024680): Train loss 1.960, Val loss 4.436\n",
      "Epoch: 11 (Step 024690): Train loss 1.935, Val loss 4.490\n",
      "Epoch: 11 (Step 024700): Train loss 2.393, Val loss 4.455\n",
      "Epoch: 11 (Step 024710): Train loss 2.119, Val loss 4.432\n",
      "Epoch: 11 (Step 024720): Train loss 2.595, Val loss 4.436\n",
      "Epoch: 11 (Step 024730): Train loss 1.922, Val loss 4.399\n",
      "Epoch: 11 (Step 024740): Train loss 1.842, Val loss 4.444\n",
      "Epoch: 11 (Step 024750): Train loss 1.642, Val loss 4.466\n",
      "Epoch: 11 (Step 024760): Train loss 1.293, Val loss 4.448\n",
      "Epoch: 11 (Step 024770): Train loss 1.658, Val loss 4.417\n",
      "Epoch: 11 (Step 024780): Train loss 2.627, Val loss 4.412\n",
      "Epoch: 11 (Step 024790): Train loss 1.971, Val loss 4.444\n",
      "Epoch: 11 (Step 024800): Train loss 1.449, Val loss 4.469\n",
      "Epoch: 11 (Step 024810): Train loss 1.879, Val loss 4.419\n",
      "Epoch: 11 (Step 024820): Train loss 1.951, Val loss 4.420\n",
      "Epoch: 11 (Step 024830): Train loss 2.697, Val loss 4.438\n",
      "Epoch: 11 (Step 024840): Train loss 1.846, Val loss 4.416\n",
      "Epoch: 11 (Step 024850): Train loss 1.824, Val loss 4.419\n",
      "Epoch: 11 (Step 024860): Train loss 2.522, Val loss 4.451\n",
      "Epoch: 11 (Step 024870): Train loss 2.071, Val loss 4.408\n",
      "Epoch: 11 (Step 024880): Train loss 1.869, Val loss 4.412\n",
      "Epoch: 11 (Step 024890): Train loss 2.019, Val loss 4.398\n",
      "Epoch: 11 (Step 024900): Train loss 2.217, Val loss 4.403\n",
      "Epoch: 11 (Step 024910): Train loss 2.247, Val loss 4.422\n",
      "Epoch: 11 (Step 024920): Train loss 1.994, Val loss 4.434\n",
      "Epoch: 11 (Step 024930): Train loss 1.886, Val loss 4.366\n",
      "Epoch: 11 (Step 024940): Train loss 2.799, Val loss 4.405\n",
      "Epoch: 11 (Step 024950): Train loss 2.171, Val loss 4.382\n",
      "Epoch: 11 (Step 024960): Train loss 2.121, Val loss 4.329\n",
      "Epoch: 11 (Step 024970): Train loss 1.700, Val loss 4.398\n",
      "Epoch: 11 (Step 024980): Train loss 2.640, Val loss 4.395\n",
      "Epoch: 11 (Step 024990): Train loss 2.338, Val loss 4.438\n",
      "Epoch: 11 (Step 025000): Train loss 2.038, Val loss 4.399\n",
      "Epoch: 11 (Step 025010): Train loss 1.803, Val loss 4.396\n",
      "Epoch: 11 (Step 025020): Train loss 2.077, Val loss 4.422\n",
      "Epoch: 11 (Step 025030): Train loss 1.715, Val loss 4.473\n",
      "Epoch: 11 (Step 025040): Train loss 1.712, Val loss 4.489\n",
      "Epoch: 11 (Step 025050): Train loss 2.129, Val loss 4.427\n",
      "Epoch: 11 (Step 025060): Train loss 2.510, Val loss 4.462\n",
      "Epoch: 11 (Step 025070): Train loss 2.166, Val loss 4.460\n",
      "Epoch: 11 (Step 025080): Train loss 1.766, Val loss 4.464\n",
      "Epoch: 11 (Step 025090): Train loss 1.853, Val loss 4.421\n",
      "Epoch: 11 (Step 025100): Train loss 2.743, Val loss 4.378\n",
      "Epoch: 11 (Step 025110): Train loss 2.117, Val loss 4.386\n",
      "Epoch: 11 (Step 025120): Train loss 1.777, Val loss 4.402\n",
      "Epoch: 11 (Step 025130): Train loss 1.884, Val loss 4.409\n",
      "Epoch: 11 (Step 025140): Train loss 2.527, Val loss 4.382\n",
      "Epoch: 11 (Step 025150): Train loss 2.058, Val loss 4.337\n",
      "Epoch: 11 (Step 025160): Train loss 1.940, Val loss 4.411\n",
      "Epoch: 11 (Step 025170): Train loss 2.096, Val loss 4.422\n",
      "Epoch: 11 (Step 025180): Train loss 2.478, Val loss 4.448\n",
      "Epoch: 11 (Step 025190): Train loss 2.019, Val loss 4.485\n",
      "Epoch: 11 (Step 025200): Train loss 2.037, Val loss 4.466\n",
      "Epoch: 11 (Step 025210): Train loss 1.743, Val loss 4.512\n",
      "Epoch: 11 (Step 025220): Train loss 1.619, Val loss 4.518\n",
      "Epoch: 11 (Step 025230): Train loss 2.195, Val loss 4.526\n",
      "Epoch: 11 (Step 025240): Train loss 1.969, Val loss 4.496\n",
      "Epoch: 11 (Step 025250): Train loss 2.356, Val loss 4.471\n",
      "Epoch: 11 (Step 025260): Train loss 1.917, Val loss 4.510\n",
      "Epoch: 11 (Step 025270): Train loss 2.036, Val loss 4.489\n",
      "Epoch: 11 (Step 025280): Train loss 2.100, Val loss 4.442\n",
      "Epoch: 11 (Step 025290): Train loss 2.227, Val loss 4.486\n",
      "Epoch: 11 (Step 025300): Train loss 2.238, Val loss 4.456\n",
      "Epoch: 11 (Step 025310): Train loss 1.873, Val loss 4.460\n",
      "Epoch: 11 (Step 025320): Train loss 1.987, Val loss 4.405\n",
      "Epoch: 11 (Step 025330): Train loss 2.009, Val loss 4.432\n",
      "Epoch: 11 (Step 025340): Train loss 1.623, Val loss 4.571\n",
      "Epoch: 11 (Step 025350): Train loss 2.180, Val loss 4.577\n",
      "Epoch: 11 (Step 025360): Train loss 2.257, Val loss 4.560\n",
      "Epoch: 11 (Step 025370): Train loss 2.170, Val loss 4.514\n",
      "Epoch: 11 (Step 025380): Train loss 2.296, Val loss 4.487\n",
      "Epoch: 11 (Step 025390): Train loss 2.431, Val loss 4.424\n",
      "Epoch: 11 (Step 025400): Train loss 2.507, Val loss 4.475\n",
      "Epoch: 11 (Step 025410): Train loss 2.002, Val loss 4.494\n",
      "Epoch: 11 (Step 025420): Train loss 2.442, Val loss 4.504\n",
      "Epoch: 11 (Step 025430): Train loss 2.303, Val loss 4.555\n",
      "Epoch: 11 (Step 025440): Train loss 2.127, Val loss 4.513\n",
      "Epoch: 11 (Step 025450): Train loss 1.963, Val loss 4.489\n",
      "Epoch: 11 (Step 025460): Train loss 1.777, Val loss 4.525\n",
      "Epoch: 11 (Step 025470): Train loss 2.343, Val loss 4.553\n",
      "Epoch: 11 (Step 025480): Train loss 1.885, Val loss 4.540\n",
      "Epoch: 11 (Step 025490): Train loss 2.368, Val loss 4.540\n",
      "Epoch: 11 (Step 025500): Train loss 1.830, Val loss 4.502\n",
      "Epoch: 11 (Step 025510): Train loss 1.945, Val loss 4.489\n",
      "Epoch: 11 (Step 025520): Train loss 2.202, Val loss 4.471\n",
      "Epoch: 11 (Step 025530): Train loss 2.123, Val loss 4.465\n",
      "Epoch: 11 (Step 025540): Train loss 1.792, Val loss 4.477\n",
      "Epoch: 11 (Step 025550): Train loss 2.109, Val loss 4.492\n",
      "Epoch: 11 (Step 025560): Train loss 2.040, Val loss 4.502\n",
      "Epoch: 11 (Step 025570): Train loss 1.950, Val loss 4.512\n",
      "Epoch: 11 (Step 025580): Train loss 1.785, Val loss 4.508\n",
      "Epoch: 11 (Step 025590): Train loss 2.183, Val loss 4.539\n",
      "Epoch: 11 (Step 025600): Train loss 1.865, Val loss 4.597\n",
      "Epoch: 11 (Step 025610): Train loss 2.336, Val loss 4.522\n",
      "Epoch: 11 (Step 025620): Train loss 2.039, Val loss 4.444\n",
      "Epoch: 11 (Step 025630): Train loss 2.257, Val loss 4.445\n",
      "Epoch: 11 (Step 025640): Train loss 2.063, Val loss 4.383\n",
      "Epoch: 11 (Step 025650): Train loss 2.324, Val loss 4.443\n",
      "Epoch: 11 (Step 025660): Train loss 1.946, Val loss 4.452\n",
      "Epoch: 11 (Step 025670): Train loss 1.821, Val loss 4.483\n",
      "Epoch: 11 (Step 025680): Train loss 2.085, Val loss 4.495\n",
      "Epoch: 11 (Step 025690): Train loss 1.812, Val loss 4.416\n",
      "Epoch: 11 (Step 025700): Train loss 1.725, Val loss 4.439\n",
      "Epoch: 11 (Step 025710): Train loss 2.209, Val loss 4.444\n",
      "Epoch: 11 (Step 025720): Train loss 1.692, Val loss 4.389\n",
      "Epoch: 11 (Step 025730): Train loss 2.081, Val loss 4.368\n",
      "Epoch: 11 (Step 025740): Train loss 2.713, Val loss 4.374\n",
      "Epoch: 11 (Step 025750): Train loss 1.890, Val loss 4.385\n",
      "Epoch: 11 (Step 025760): Train loss 2.319, Val loss 4.392\n",
      "Epoch: 11 (Step 025770): Train loss 2.040, Val loss 4.409\n",
      "Epoch: 11 (Step 025780): Train loss 1.950, Val loss 4.391\n",
      "Epoch: 11 (Step 025790): Train loss 2.069, Val loss 4.368\n",
      "Epoch: 11 (Step 025800): Train loss 2.066, Val loss 4.324\n",
      "Epoch: 11 (Step 025810): Train loss 2.009, Val loss 4.334\n",
      "Epoch: 11 (Step 025820): Train loss 2.134, Val loss 4.334\n",
      "Epoch: 11 (Step 025830): Train loss 2.460, Val loss 4.318\n",
      "Epoch: 11 (Step 025840): Train loss 1.883, Val loss 4.298\n",
      "Epoch: 11 (Step 025850): Train loss 1.722, Val loss 4.304\n",
      "Epoch: 11 (Step 025860): Train loss 1.764, Val loss 4.354\n",
      "Epoch: 11 (Step 025870): Train loss 2.071, Val loss 4.427\n",
      "Epoch: 11 (Step 025880): Train loss 1.915, Val loss 4.364\n",
      "Epoch: 11 (Step 025890): Train loss 2.115, Val loss 4.396\n",
      "Epoch: 11 (Step 025900): Train loss 1.825, Val loss 4.415\n",
      "Epoch: 11 (Step 025910): Train loss 1.821, Val loss 4.391\n",
      "Epoch: 11 (Step 025920): Train loss 1.677, Val loss 4.391\n",
      "Epoch: 11 (Step 025930): Train loss 1.759, Val loss 4.409\n",
      "Epoch: 11 (Step 025940): Train loss 1.736, Val loss 4.464\n",
      "Epoch: 12 (Step 025950): Train loss 1.920, Val loss 4.439\n",
      "Epoch: 12 (Step 025960): Train loss 2.212, Val loss 4.430\n",
      "Epoch: 12 (Step 025970): Train loss 2.178, Val loss 4.415\n",
      "Epoch: 12 (Step 025980): Train loss 1.981, Val loss 4.459\n",
      "Epoch: 12 (Step 025990): Train loss 1.924, Val loss 4.467\n",
      "Epoch: 12 (Step 026000): Train loss 1.988, Val loss 4.473\n",
      "Epoch: 12 (Step 026010): Train loss 1.392, Val loss 4.505\n",
      "Epoch: 12 (Step 026020): Train loss 1.674, Val loss 4.509\n",
      "Epoch: 12 (Step 026030): Train loss 1.777, Val loss 4.462\n",
      "Epoch: 12 (Step 026040): Train loss 2.199, Val loss 4.460\n",
      "Epoch: 12 (Step 026050): Train loss 1.805, Val loss 4.449\n",
      "Epoch: 12 (Step 026060): Train loss 1.708, Val loss 4.451\n",
      "Epoch: 12 (Step 026070): Train loss 1.695, Val loss 4.479\n",
      "Epoch: 12 (Step 026080): Train loss 1.439, Val loss 4.510\n",
      "Epoch: 12 (Step 026090): Train loss 1.853, Val loss 4.578\n",
      "Epoch: 12 (Step 026100): Train loss 2.048, Val loss 4.592\n",
      "Epoch: 12 (Step 026110): Train loss 2.327, Val loss 4.550\n",
      "Epoch: 12 (Step 026120): Train loss 1.809, Val loss 4.529\n",
      "Epoch: 12 (Step 026130): Train loss 2.031, Val loss 4.545\n",
      "Epoch: 12 (Step 026140): Train loss 2.122, Val loss 4.562\n",
      "Epoch: 12 (Step 026150): Train loss 1.184, Val loss 4.581\n",
      "Epoch: 12 (Step 026160): Train loss 2.072, Val loss 4.556\n",
      "Epoch: 12 (Step 026170): Train loss 1.767, Val loss 4.522\n",
      "Epoch: 12 (Step 026180): Train loss 2.140, Val loss 4.515\n",
      "Epoch: 12 (Step 026190): Train loss 2.004, Val loss 4.563\n",
      "Epoch: 12 (Step 026200): Train loss 1.560, Val loss 4.569\n",
      "Epoch: 12 (Step 026210): Train loss 1.778, Val loss 4.590\n",
      "Epoch: 12 (Step 026220): Train loss 1.842, Val loss 4.556\n",
      "Epoch: 12 (Step 026230): Train loss 1.737, Val loss 4.517\n",
      "Epoch: 12 (Step 026240): Train loss 1.568, Val loss 4.570\n",
      "Epoch: 12 (Step 026250): Train loss 1.793, Val loss 4.608\n",
      "Epoch: 12 (Step 026260): Train loss 1.591, Val loss 4.664\n",
      "Epoch: 12 (Step 026270): Train loss 1.926, Val loss 4.657\n",
      "Epoch: 12 (Step 026280): Train loss 1.930, Val loss 4.650\n",
      "Epoch: 12 (Step 026290): Train loss 1.757, Val loss 4.590\n",
      "Epoch: 12 (Step 026300): Train loss 2.244, Val loss 4.629\n",
      "Epoch: 12 (Step 026310): Train loss 1.959, Val loss 4.627\n",
      "Epoch: 12 (Step 026320): Train loss 1.698, Val loss 4.625\n",
      "Epoch: 12 (Step 026330): Train loss 1.613, Val loss 4.685\n",
      "Epoch: 12 (Step 026340): Train loss 1.442, Val loss 4.640\n",
      "Epoch: 12 (Step 026350): Train loss 1.976, Val loss 4.621\n",
      "Epoch: 12 (Step 026360): Train loss 1.537, Val loss 4.653\n",
      "Epoch: 12 (Step 026370): Train loss 2.074, Val loss 4.705\n",
      "Epoch: 12 (Step 026380): Train loss 2.067, Val loss 4.740\n",
      "Epoch: 12 (Step 026390): Train loss 1.962, Val loss 4.774\n",
      "Epoch: 12 (Step 026400): Train loss 1.716, Val loss 4.806\n",
      "Epoch: 12 (Step 026410): Train loss 2.052, Val loss 4.790\n",
      "Epoch: 12 (Step 026420): Train loss 1.366, Val loss 4.772\n",
      "Epoch: 12 (Step 026430): Train loss 1.670, Val loss 4.702\n",
      "Epoch: 12 (Step 026440): Train loss 1.747, Val loss 4.670\n",
      "Epoch: 12 (Step 026450): Train loss 1.571, Val loss 4.685\n",
      "Epoch: 12 (Step 026460): Train loss 1.301, Val loss 4.679\n",
      "Epoch: 12 (Step 026470): Train loss 2.036, Val loss 4.657\n",
      "Epoch: 12 (Step 026480): Train loss 1.785, Val loss 4.648\n",
      "Epoch: 12 (Step 026490): Train loss 1.786, Val loss 4.670\n",
      "Epoch: 12 (Step 026500): Train loss 2.328, Val loss 4.688\n",
      "Epoch: 12 (Step 026510): Train loss 1.654, Val loss 4.660\n",
      "Epoch: 12 (Step 026520): Train loss 1.648, Val loss 4.643\n",
      "Epoch: 12 (Step 026530): Train loss 2.523, Val loss 4.657\n",
      "Epoch: 12 (Step 026540): Train loss 1.954, Val loss 4.656\n",
      "Epoch: 12 (Step 026550): Train loss 2.169, Val loss 4.682\n",
      "Epoch: 12 (Step 026560): Train loss 1.783, Val loss 4.681\n",
      "Epoch: 12 (Step 026570): Train loss 1.489, Val loss 4.640\n",
      "Epoch: 12 (Step 026580): Train loss 1.984, Val loss 4.677\n",
      "Epoch: 12 (Step 026590): Train loss 1.610, Val loss 4.691\n",
      "Epoch: 12 (Step 026600): Train loss 1.761, Val loss 4.670\n",
      "Epoch: 12 (Step 026610): Train loss 1.801, Val loss 4.699\n",
      "Epoch: 12 (Step 026620): Train loss 2.283, Val loss 4.698\n",
      "Epoch: 12 (Step 026630): Train loss 1.852, Val loss 4.730\n",
      "Epoch: 12 (Step 026640): Train loss 1.992, Val loss 4.673\n",
      "Epoch: 12 (Step 026650): Train loss 1.831, Val loss 4.657\n",
      "Epoch: 12 (Step 026660): Train loss 1.831, Val loss 4.641\n",
      "Epoch: 12 (Step 026670): Train loss 2.210, Val loss 4.663\n",
      "Epoch: 12 (Step 026680): Train loss 1.381, Val loss 4.640\n",
      "Epoch: 12 (Step 026690): Train loss 1.207, Val loss 4.655\n",
      "Epoch: 12 (Step 026700): Train loss 1.405, Val loss 4.663\n",
      "Epoch: 12 (Step 026710): Train loss 1.764, Val loss 4.652\n",
      "Epoch: 12 (Step 026720): Train loss 1.650, Val loss 4.677\n",
      "Epoch: 12 (Step 026730): Train loss 1.698, Val loss 4.692\n",
      "Epoch: 12 (Step 026740): Train loss 1.614, Val loss 4.644\n",
      "Epoch: 12 (Step 026750): Train loss 1.977, Val loss 4.675\n",
      "Epoch: 12 (Step 026760): Train loss 2.433, Val loss 4.655\n",
      "Epoch: 12 (Step 026770): Train loss 1.403, Val loss 4.640\n",
      "Epoch: 12 (Step 026780): Train loss 1.228, Val loss 4.635\n",
      "Epoch: 12 (Step 026790): Train loss 2.000, Val loss 4.636\n",
      "Epoch: 12 (Step 026800): Train loss 1.982, Val loss 4.653\n",
      "Epoch: 12 (Step 026810): Train loss 2.002, Val loss 4.641\n",
      "Epoch: 12 (Step 026820): Train loss 2.054, Val loss 4.655\n",
      "Epoch: 12 (Step 026830): Train loss 1.624, Val loss 4.652\n",
      "Epoch: 12 (Step 026840): Train loss 2.102, Val loss 4.593\n",
      "Epoch: 12 (Step 026850): Train loss 2.127, Val loss 4.598\n",
      "Epoch: 12 (Step 026860): Train loss 1.696, Val loss 4.592\n",
      "Epoch: 12 (Step 026870): Train loss 2.037, Val loss 4.553\n",
      "Epoch: 12 (Step 026880): Train loss 1.674, Val loss 4.562\n",
      "Epoch: 12 (Step 026890): Train loss 1.840, Val loss 4.538\n",
      "Epoch: 12 (Step 026900): Train loss 1.890, Val loss 4.496\n",
      "Epoch: 12 (Step 026910): Train loss 1.552, Val loss 4.549\n",
      "Epoch: 12 (Step 026920): Train loss 1.596, Val loss 4.541\n",
      "Epoch: 12 (Step 026930): Train loss 1.989, Val loss 4.522\n",
      "Epoch: 12 (Step 026940): Train loss 1.497, Val loss 4.572\n",
      "Epoch: 12 (Step 026950): Train loss 2.115, Val loss 4.579\n",
      "Epoch: 12 (Step 026960): Train loss 2.458, Val loss 4.613\n",
      "Epoch: 12 (Step 026970): Train loss 1.842, Val loss 4.561\n",
      "Epoch: 12 (Step 026980): Train loss 1.722, Val loss 4.536\n",
      "Epoch: 12 (Step 026990): Train loss 1.594, Val loss 4.497\n",
      "Epoch: 12 (Step 027000): Train loss 2.266, Val loss 4.556\n",
      "Epoch: 12 (Step 027010): Train loss 1.954, Val loss 4.542\n",
      "Epoch: 12 (Step 027020): Train loss 1.862, Val loss 4.489\n",
      "Epoch: 12 (Step 027030): Train loss 2.019, Val loss 4.495\n",
      "Epoch: 12 (Step 027040): Train loss 1.802, Val loss 4.472\n",
      "Epoch: 12 (Step 027050): Train loss 2.189, Val loss 4.408\n",
      "Epoch: 12 (Step 027060): Train loss 1.513, Val loss 4.393\n",
      "Epoch: 12 (Step 027070): Train loss 1.877, Val loss 4.438\n",
      "Epoch: 12 (Step 027080): Train loss 2.264, Val loss 4.490\n",
      "Epoch: 12 (Step 027090): Train loss 2.203, Val loss 4.531\n",
      "Epoch: 12 (Step 027100): Train loss 1.423, Val loss 4.585\n",
      "Epoch: 12 (Step 027110): Train loss 1.824, Val loss 4.581\n",
      "Epoch: 12 (Step 027120): Train loss 1.563, Val loss 4.567\n",
      "Epoch: 12 (Step 027130): Train loss 1.859, Val loss 4.575\n",
      "Epoch: 12 (Step 027140): Train loss 2.084, Val loss 4.564\n",
      "Epoch: 12 (Step 027150): Train loss 1.739, Val loss 4.540\n",
      "Epoch: 12 (Step 027160): Train loss 1.645, Val loss 4.572\n",
      "Epoch: 12 (Step 027170): Train loss 2.067, Val loss 4.576\n",
      "Epoch: 12 (Step 027180): Train loss 1.905, Val loss 4.552\n",
      "Epoch: 12 (Step 027190): Train loss 1.482, Val loss 4.551\n",
      "Epoch: 12 (Step 027200): Train loss 2.392, Val loss 4.577\n",
      "Epoch: 12 (Step 027210): Train loss 1.979, Val loss 4.574\n",
      "Epoch: 12 (Step 027220): Train loss 2.188, Val loss 4.598\n",
      "Epoch: 12 (Step 027230): Train loss 1.582, Val loss 4.588\n",
      "Epoch: 12 (Step 027240): Train loss 1.962, Val loss 4.565\n",
      "Epoch: 12 (Step 027250): Train loss 1.648, Val loss 4.580\n",
      "Epoch: 12 (Step 027260): Train loss 2.459, Val loss 4.585\n",
      "Epoch: 12 (Step 027270): Train loss 1.855, Val loss 4.599\n",
      "Epoch: 12 (Step 027280): Train loss 1.438, Val loss 4.619\n",
      "Epoch: 12 (Step 027290): Train loss 1.901, Val loss 4.630\n",
      "Epoch: 12 (Step 027300): Train loss 1.534, Val loss 4.641\n",
      "Epoch: 12 (Step 027310): Train loss 1.556, Val loss 4.612\n",
      "Epoch: 12 (Step 027320): Train loss 2.057, Val loss 4.598\n",
      "Epoch: 12 (Step 027330): Train loss 2.071, Val loss 4.649\n",
      "Epoch: 12 (Step 027340): Train loss 1.791, Val loss 4.594\n",
      "Epoch: 12 (Step 027350): Train loss 1.512, Val loss 4.597\n",
      "Epoch: 12 (Step 027360): Train loss 1.636, Val loss 4.621\n",
      "Epoch: 12 (Step 027370): Train loss 1.761, Val loss 4.551\n",
      "Epoch: 12 (Step 027380): Train loss 1.961, Val loss 4.565\n",
      "Epoch: 12 (Step 027390): Train loss 1.488, Val loss 4.638\n",
      "Epoch: 12 (Step 027400): Train loss 1.320, Val loss 4.637\n",
      "Epoch: 12 (Step 027410): Train loss 1.280, Val loss 4.615\n",
      "Epoch: 12 (Step 027420): Train loss 1.802, Val loss 4.619\n",
      "Epoch: 12 (Step 027430): Train loss 1.774, Val loss 4.603\n",
      "Epoch: 12 (Step 027440): Train loss 1.356, Val loss 4.591\n",
      "Epoch: 12 (Step 027450): Train loss 1.512, Val loss 4.632\n",
      "Epoch: 12 (Step 027460): Train loss 2.038, Val loss 4.651\n",
      "Epoch: 12 (Step 027470): Train loss 1.815, Val loss 4.597\n",
      "Epoch: 12 (Step 027480): Train loss 1.528, Val loss 4.516\n",
      "Epoch: 12 (Step 027490): Train loss 1.868, Val loss 4.549\n",
      "Epoch: 12 (Step 027500): Train loss 1.471, Val loss 4.535\n",
      "Epoch: 12 (Step 027510): Train loss 1.332, Val loss 4.554\n",
      "Epoch: 12 (Step 027520): Train loss 2.287, Val loss 4.547\n",
      "Epoch: 12 (Step 027530): Train loss 1.942, Val loss 4.544\n",
      "Epoch: 12 (Step 027540): Train loss 1.697, Val loss 4.548\n",
      "Epoch: 12 (Step 027550): Train loss 1.545, Val loss 4.536\n",
      "Epoch: 12 (Step 027560): Train loss 1.857, Val loss 4.516\n",
      "Epoch: 12 (Step 027570): Train loss 1.960, Val loss 4.547\n",
      "Epoch: 12 (Step 027580): Train loss 1.662, Val loss 4.522\n",
      "Epoch: 12 (Step 027590): Train loss 1.378, Val loss 4.562\n",
      "Epoch: 12 (Step 027600): Train loss 1.313, Val loss 4.578\n",
      "Epoch: 12 (Step 027610): Train loss 1.526, Val loss 4.610\n",
      "Epoch: 12 (Step 027620): Train loss 1.793, Val loss 4.606\n",
      "Epoch: 12 (Step 027630): Train loss 1.961, Val loss 4.624\n",
      "Epoch: 12 (Step 027640): Train loss 1.526, Val loss 4.623\n",
      "Epoch: 12 (Step 027650): Train loss 1.678, Val loss 4.570\n",
      "Epoch: 12 (Step 027660): Train loss 1.853, Val loss 4.587\n",
      "Epoch: 12 (Step 027670): Train loss 1.731, Val loss 4.596\n",
      "Epoch: 12 (Step 027680): Train loss 1.522, Val loss 4.609\n",
      "Epoch: 12 (Step 027690): Train loss 1.647, Val loss 4.553\n",
      "Epoch: 12 (Step 027700): Train loss 1.668, Val loss 4.526\n",
      "Epoch: 12 (Step 027710): Train loss 2.362, Val loss 4.565\n",
      "Epoch: 12 (Step 027720): Train loss 1.350, Val loss 4.589\n",
      "Epoch: 12 (Step 027730): Train loss 1.801, Val loss 4.621\n",
      "Epoch: 12 (Step 027740): Train loss 1.756, Val loss 4.575\n",
      "Epoch: 12 (Step 027750): Train loss 1.508, Val loss 4.556\n",
      "Epoch: 12 (Step 027760): Train loss 1.390, Val loss 4.542\n",
      "Epoch: 12 (Step 027770): Train loss 1.391, Val loss 4.502\n",
      "Epoch: 12 (Step 027780): Train loss 1.389, Val loss 4.580\n",
      "Epoch: 12 (Step 027790): Train loss 1.580, Val loss 4.582\n",
      "Epoch: 12 (Step 027800): Train loss 1.764, Val loss 4.572\n",
      "Epoch: 12 (Step 027810): Train loss 1.727, Val loss 4.574\n",
      "Epoch: 12 (Step 027820): Train loss 2.219, Val loss 4.548\n",
      "Epoch: 12 (Step 027830): Train loss 1.697, Val loss 4.482\n",
      "Epoch: 12 (Step 027840): Train loss 1.850, Val loss 4.499\n",
      "Epoch: 12 (Step 027850): Train loss 1.770, Val loss 4.491\n",
      "Epoch: 12 (Step 027860): Train loss 2.181, Val loss 4.552\n",
      "Epoch: 12 (Step 027870): Train loss 1.599, Val loss 4.584\n",
      "Epoch: 12 (Step 027880): Train loss 1.526, Val loss 4.603\n",
      "Epoch: 12 (Step 027890): Train loss 1.685, Val loss 4.662\n",
      "Epoch: 12 (Step 027900): Train loss 1.953, Val loss 4.613\n",
      "Epoch: 12 (Step 027910): Train loss 1.396, Val loss 4.618\n",
      "Epoch: 12 (Step 027920): Train loss 1.949, Val loss 4.599\n",
      "Epoch: 12 (Step 027930): Train loss 1.525, Val loss 4.595\n",
      "Epoch: 12 (Step 027940): Train loss 2.130, Val loss 4.572\n",
      "Epoch: 12 (Step 027950): Train loss 1.505, Val loss 4.521\n",
      "Epoch: 12 (Step 027960): Train loss 1.975, Val loss 4.547\n",
      "Epoch: 12 (Step 027970): Train loss 1.557, Val loss 4.491\n",
      "Epoch: 12 (Step 027980): Train loss 1.843, Val loss 4.547\n",
      "Epoch: 12 (Step 027990): Train loss 1.682, Val loss 4.524\n",
      "Epoch: 12 (Step 028000): Train loss 1.565, Val loss 4.532\n",
      "Epoch: 12 (Step 028010): Train loss 1.788, Val loss 4.504\n",
      "Epoch: 12 (Step 028020): Train loss 1.577, Val loss 4.488\n",
      "Epoch: 12 (Step 028030): Train loss 1.373, Val loss 4.501\n",
      "Epoch: 12 (Step 028040): Train loss 1.268, Val loss 4.476\n",
      "Epoch: 12 (Step 028050): Train loss 1.516, Val loss 4.512\n",
      "Epoch: 12 (Step 028060): Train loss 1.369, Val loss 4.509\n",
      "Epoch: 12 (Step 028070): Train loss 1.551, Val loss 4.475\n",
      "Epoch: 12 (Step 028080): Train loss 1.750, Val loss 4.533\n",
      "Epoch: 12 (Step 028090): Train loss 1.832, Val loss 4.589\n",
      "Epoch: 12 (Step 028100): Train loss 1.508, Val loss 4.537\n",
      "Epoch: 12 (Step 028110): Train loss 1.661, Val loss 4.557\n",
      "Epoch: 12 (Step 028120): Train loss 1.481, Val loss 4.544\n",
      "Epoch: 12 (Step 028130): Train loss 2.130, Val loss 4.534\n",
      "Epoch: 12 (Step 028140): Train loss 1.567, Val loss 4.521\n",
      "Epoch: 12 (Step 028150): Train loss 1.497, Val loss 4.538\n",
      "Epoch: 12 (Step 028160): Train loss 1.385, Val loss 4.478\n",
      "Epoch: 12 (Step 028170): Train loss 2.194, Val loss 4.481\n",
      "Epoch: 12 (Step 028180): Train loss 1.628, Val loss 4.431\n",
      "Epoch: 12 (Step 028190): Train loss 1.636, Val loss 4.469\n",
      "Epoch: 12 (Step 028200): Train loss 1.273, Val loss 4.463\n",
      "Epoch: 12 (Step 028210): Train loss 1.400, Val loss 4.548\n",
      "Epoch: 12 (Step 028220): Train loss 1.812, Val loss 4.581\n",
      "Epoch: 12 (Step 028230): Train loss 2.006, Val loss 4.550\n",
      "Epoch: 12 (Step 028240): Train loss 1.682, Val loss 4.527\n",
      "Epoch: 12 (Step 028250): Train loss 1.663, Val loss 4.442\n",
      "Epoch: 12 (Step 028260): Train loss 1.758, Val loss 4.480\n",
      "Epoch: 12 (Step 028270): Train loss 1.319, Val loss 4.510\n",
      "Epoch: 12 (Step 028280): Train loss 1.965, Val loss 4.507\n",
      "Epoch: 12 (Step 028290): Train loss 1.855, Val loss 4.512\n",
      "Epoch: 12 (Step 028300): Train loss 1.674, Val loss 4.527\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "train_losses, val_losses = [], []\n",
    "global_step = -1\n",
    "num_epochs = 500\n",
    "eval_freq = 10\n",
    "eval_iter = 2\n",
    "optimizer = AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "# Main training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "\n",
    "    for input_batch, target_batch in train_loader:\n",
    "        optimizer.zero_grad()  # Reset loss gradients from previous batch iteration\n",
    "        loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "        loss.backward()  # Calculate loss gradients\n",
    "        optimizer.step()  # Update model weights using loss gradients\n",
    "        global_step += 1\n",
    "        \n",
    "        # evaluation each eval_freq\n",
    "        if global_step % eval_freq == 0:\n",
    "            train_loss, val_loss = evaluate_model(\n",
    "                model, train_loader, val_loader, device, eval_iter)\n",
    "            train_losses.append(train_loss)\n",
    "            val_losses.append(val_loss)\n",
    "            print(f\"Epoch: {epoch+1} (Step {global_step:06d}): \"\n",
    "                  f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "\n",
    "    early_stopping(val_loss, model)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a73de11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-31T19:17:37.974382Z",
     "iopub.status.busy": "2025-05-31T19:17:37.973559Z",
     "iopub.status.idle": "2025-05-31T19:17:37.980682Z",
     "shell.execute_reply": "2025-05-31T19:17:37.980219Z"
    },
    "papermill": {
     "duration": 0.112473,
     "end_time": "2025-05-31T19:17:37.981681",
     "exception": false,
     "start_time": "2025-05-31T19:17:37.869208",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "early_stopping.load_best_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c9660c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-31T19:17:38.238503Z",
     "iopub.status.busy": "2025-05-31T19:17:38.237980Z",
     "iopub.status.idle": "2025-05-31T19:17:41.365024Z",
     "shell.execute_reply": "2025-05-31T19:17:41.364213Z"
    },
    "papermill": {
     "duration": 3.282062,
     "end_time": "2025-05-31T19:17:41.366543",
     "exception": false,
     "start_time": "2025-05-31T19:17:38.084481",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save and load model\n",
    "save_path = \"/kaggle/working/model.pth\"\n",
    "torch.save(model.state_dict(), save_path)\n",
    "loaded_model = GPTModel(GPT_CONFIG_124M)\n",
    "loaded_model.load_state_dict(torch.load(save_path, weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c185bf66",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-31T19:17:41.577967Z",
     "iopub.status.busy": "2025-05-31T19:17:41.577413Z",
     "iopub.status.idle": "2025-05-31T19:17:42.666840Z",
     "shell.execute_reply": "2025-05-31T19:17:42.666235Z"
    },
    "papermill": {
     "duration": 1.194821,
     "end_time": "2025-05-31T19:17:42.668235",
     "exception": false,
     "start_time": "2025-05-31T19:17:41.473414",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "idx = text_to_token_ids('I went to the', tokenizer)\n",
    "o = loaded_model.generate(idx, 20, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3a40904c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-31T19:17:42.882810Z",
     "iopub.status.busy": "2025-05-31T19:17:42.882529Z",
     "iopub.status.idle": "2025-05-31T19:17:42.887728Z",
     "shell.execute_reply": "2025-05-31T19:17:42.887161Z"
    },
    "papermill": {
     "duration": 0.11117,
     "end_time": "2025-05-31T19:17:42.888758",
     "exception": false,
     "start_time": "2025-05-31T19:17:42.777588",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I went to the palace of the king of York,\\nAnd so I shall soon to my brother.\\n\\nKING'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids_to_text(o, tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7561664,
     "sourceId": 12018911,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7561767,
     "sourceId": 12019058,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1598.844304,
   "end_time": "2025-05-31T19:17:45.714136",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-31T18:51:06.869832",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
